# Function Flow Reference Guide

**Educational Documentation: Tracing Results Back to Source Code**

This document maps every table, figure, and result in the Kenya GBIF Bias Assessment manuscript to the specific code that generated it. This is designed for educational purposes, allowing students and researchers to understand exactly how raw data becomes final results.

---

## How to Use This Guide

1. **Find your output**: Look for the table or figure number from the manuscript
2. **Trace the pipeline**: Follow the data flow from raw data ‚Üí processing ‚Üí analysis ‚Üí visualization
3. **Explore the code**: Use the file paths and line numbers to examine the source code
4. **Understand the methods**: Read the function descriptions to see what transformations were applied

---

## Data Pipeline Overview

```
Raw GBIF Data
    ‚Üì
01_data_download.R ‚Üí kenya_gbif_raw.rds
    ‚Üì
01b_data_quality_assessment.R ‚Üí kenya_gbif_flagged.rds
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           ‚îÇ            ‚îÇ               ‚îÇ                 ‚îÇ
02_spatial  03_temporal  04_taxonomic    05_statistical
_bias.R     _bias.R      _bias.R         _models.R
‚îÇ           ‚îÇ            ‚îÇ               ‚îÇ
‚Üì           ‚Üì            ‚Üì               ‚Üì
Spatial     Temporal     Taxonomic       Model
Outputs     Outputs      Outputs         Outputs
    ‚îÇ           ‚îÇ            ‚îÇ               ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
        kenya_gbif_bias_assessment.Rmd
                      ‚Üì
              Final HTML Report
```

---

## Table Reference

### Table 1: Summary Statistics

**Location**: `docs/kenya_gbif_bias_assessment.Rmd:410-457`

**Data Sources**:
- `data/outputs/summary_statistics.rds` (generated by `01b_data_quality_assessment.R`)
- `data/processed/kenya_gbif_flagged.rds` (generated by `01b_data_quality_assessment.R`)

**Function Flow**:
```r
# Step 1: Load quality-filtered data
kenya_flagged <- readRDS("data/processed/kenya_gbif_flagged.rds")
summary_stats <- readRDS("data/outputs/summary_statistics.rds")

# Step 2: Calculate metrics
# - Record counts after filtering (from quality assessment)
# - Taxonomic diversity (n_distinct for species, genera, families, etc.)
# - Temporal range (min/max of year column)
# - Spatial uncertainty (median of coordinateUncertaintyInMeters)

# Step 3: Format as data.frame
summary_table <- data.frame(Metric = ..., Value = ...)

# Step 4: Display as interactive DT table
create_interactive_table(summary_table, ...)
```

**Key Functions**:
- `readRDS()`: Load processed data
- `nrow()`: Count records
- `n_distinct()`: Count unique taxonomic units
- `create_interactive_table()`: Custom function for interactive tables (defined in Rmd setup)

**Educational Notes**:
- This table combines multiple data sources processed by different scripts
- The "Analyzed Records" differ from "Original Records" due to quality filtering
- All downstream analyses use this filtered dataset as input

---

### Table 5: Sensitivity Analysis

**Location**: `docs/kenya_gbif_bias_assessment.Rmd:969-1011`

**Data Source**:
- `data/processed/kenya_gbif_flagged.rds`

**Function Flow**:
```r
# Step 1: Apply three filtering strategies
minimal_data <- kenya_flagged %>%
  filter(!flag_missing_coords, !flag_missing_species)

moderate_data <- kenya_flagged %>%
  filter(!flag_missing_coords, !flag_missing_species,
         !flag_invalid_date, !flag_any_coord_issue)

strict_data <- kenya_flagged %>%
  filter(<all quality flags must pass>)

# Step 2: Calculate biodiversity metrics for each
sensitivity_metrics <- data.frame(
  Strategy = c("Minimal", "Moderate", "Strict"),
  Records = c(nrow(minimal), nrow(moderate), nrow(strict)),
  Species = c(n_distinct(minimal$species), ...),
  Families = c(n_distinct(minimal$family), ...),
  ...
)

# Step 3: Display as interactive table with highlighting
datatable(sensitivity_metrics, ...) %>%
  formatStyle('Strategy',
    backgroundColor = styleEqual('Moderate (Primary)', '#E8F4F8'))
```

**Key Functions**:
- `filter()`: Apply quality criteria
- `n_distinct()`: Count unique taxonomic units
- `datatable()`: Create interactive DT table
- `formatStyle()`: Conditional formatting to highlight primary strategy

**Educational Notes**:
- Demonstrates impact of filtering stringency on sample size and diversity
- Shows that different research questions require different quality standards
- Moderate filtering balances data quality with sample size retention

---

### Table 6: Spatial Autocorrelation

**Location**: `docs/kenya_gbif_bias_assessment.Rmd:1127-1155`

**Data Source**:
- `data/outputs/moran_results.rds` (from `02_spatial_bias.R`)

**Source Code**: `scripts/02_spatial_bias.R:220-275`

**Function Flow**:
```r
# Step 1: Create spatial neighbors (in 02_spatial_bias.R)
library(spdep)
neighbors <- poly2nb(spatial_grid, queen = TRUE)
weights <- nb2listw(neighbors, style = "W", zero.policy = TRUE)

# Step 2: Calculate Moran's I
moran_test <- moran.test(
  spatial_grid$n_records,
  weights,
  zero.policy = TRUE
)

# Step 3: Extract results
moran_results <- list(
  morans_i = moran_test$estimate["Moran I statistic"],
  expectation = moran_test$estimate["Expectation"],
  variance = moran_test$estimate["Variance"],
  p_value = moran_test$p.value,
  interpretation = "Positive spatial autocorrelation"
)

# Step 4: Display in manuscript
moran_table <- data.frame(Statistic = ..., Value = ...)
kable(moran_table, ...)
```

**Key Functions**:
- `poly2nb()`: Create spatial neighbors from polygons
- `nb2listw()`: Convert neighbors to spatial weights
- `moran.test()`: Global Moran's I test for spatial autocorrelation

**Educational Notes**:
- Moran's I ranges from -1 (perfect dispersion) to +1 (perfect clustering)
- Positive values indicate sampling is clustered (not random)
- P-value tests if pattern differs from random expectation

---

### Table 7: Environmental Bias

**Location**: `docs/kenya_gbif_bias_assessment.Rmd:1183-1197`

**Data Source**:
- `data/outputs/environmental_bias.rds` (from `02_spatial_bias.R`)

**Source Code**: `scripts/02_spatial_bias.R:320-420`

**Function Flow**:
```r
# Step 1: Extract environmental values at occurrence points
sampled_env <- terra::extract(
  env_stack,  # Raster stack: elevation, temperature, precipitation
  kenya_coords,
  ID = FALSE
)

# Step 2: Extract environmental values for all of Kenya
available_env <- terra::spatSample(
  env_stack,
  size = 10000,  # Random sample of available space
  method = "random",
  as.df = TRUE
)

# Step 3: Kolmogorov-Smirnov test for each variable
env_bias <- data.frame()
for(var in c("elevation", "temperature", "precipitation")) {
  ks_test <- ks.test(sampled_env[[var]], available_env[[var]])

  env_bias <- rbind(env_bias, data.frame(
    variable = var,
    D_statistic = ks_test$statistic,
    p_value = ks_test$p.value,
    significant = ks_test$p.value < 0.001
  ))
}

# Step 4: Display results
kable(env_bias, ...)
```

**Key Functions**:
- `terra::extract()`: Extract raster values at point locations
- `terra::spatSample()`: Random sample from raster extent
- `ks.test()`: Two-sample Kolmogorov-Smirnov test

**Educational Notes**:
- Compares sampled space vs. available space
- D-statistic measures maximum difference between distributions
- Significant p-values indicate sampling doesn't represent full environmental range

---

### Table 8: Temporal Trends

**Location**: `docs/kenya_gbif_bias_assessment.Rmd:1209-1220`

**Data Source**:
- `data/outputs/trend_tests.rds` (from `03_temporal_bias.R`)

**Source Code**: `scripts/03_temporal_bias.R:85-150`

**Function Flow**:
```r
# Step 1: Aggregate by year (in 03_temporal_bias.R)
temporal_stats <- kenya_clean %>%
  filter(!is.na(year)) %>%
  group_by(year) %>%
  summarize(
    n_records = n(),
    n_species = n_distinct(species)
  )

# Step 2: Mann-Kendall trend test
library(Kendall)
mk_records <- MannKendall(temporal_stats$n_records)
mk_species <- MannKendall(temporal_stats$n_species)

# Step 3: Format results
trend_tests <- data.frame(
  variable = c("n_records", "n_species"),
  tau = c(mk_records$tau, mk_species$tau),
  p_value = c(mk_records$sl, mk_species$sl),
  trend = c("Increasing", "Increasing")
)

# Step 4: Display in manuscript
kable(trend_table, ...)
```

**Key Functions**:
- `group_by() + summarize()`: Temporal aggregation
- `MannKendall()`: Non-parametric trend test (doesn't assume normality)

**Educational Notes**:
- Mann-Kendall is robust to outliers and non-normal distributions
- Kendall's œÑ ranges from -1 (strong negative trend) to +1 (strong positive trend)
- Positive trends indicate increasing data accumulation over time

---

## Figure Reference

### Figure 1: Spatial Sampling Effort Map (Static)

**Location**: `docs/kenya_gbif_bias_assessment.Rmd:1055-1057`

**Data Source**:
- `figures/01_sampling_effort_map.png` (generated by `02_spatial_bias.R`)

**Source Code**: `scripts/02_spatial_bias.R:50-180`

**Function Flow**:
```r
# Step 1: Create hexagonal grid over Kenya
kenya_boundary <- st_read("data/boundaries/kenya.shp")
grid <- st_make_grid(
  kenya_boundary,
  cellsize = 0.1,  # ~10 km resolution
  square = FALSE,  # hexagonal cells
  what = "polygons"
)

# Step 2: Count records per grid cell
spatial_grid <- grid %>%
  st_join(kenya_coords, .) %>%
  group_by(grid_id) %>%
  summarize(
    n_records = n(),
    n_species = n_distinct(species)
  )

# Step 3: Visualize with ggplot2
ggplot(spatial_grid) +
  geom_sf(aes(fill = log10(n_records + 1))) +
  scale_fill_viridis_c() +
  theme_minimal()

# Step 4: Save figure
ggsave("figures/01_sampling_effort_map.png", width = 10, height = 8)
```

**Key Functions**:
- `st_make_grid()`: Create spatial grid
- `st_join()`: Spatial join of points to grid cells
- `geom_sf()`: Plot spatial features with ggplot2
- `scale_fill_viridis_c()`: Color scale for continuous data

**Educational Notes**:
- Hexagonal grids provide equal-area cells and better neighbor relationships
- Log transformation used because of extreme right skew in record counts
- ~10 km resolution balances detail with computational efficiency

---

### Figure 1 (Interactive): Leaflet Sampling Map

**Location**: `docs/kenya_gbif_bias_assessment.Rmd:1059-1121`

**Data Source**:
- `data/outputs/spatial_grid_effort.rds` (from `02_spatial_bias.R`)

**Function Flow**:
```r
# Step 1: Load spatial grid (already created by 02_spatial_bias.R)
spatial_grid <- readRDS("data/outputs/spatial_grid_effort.rds")

# Step 2: Transform to WGS84 (required by leaflet)
spatial_grid_wgs84 <- st_transform(spatial_grid, 4326)

# Step 3: Create color palette
pal <- colorNumeric(
  palette = "viridis",
  domain = log10(spatial_grid_wgs84$n_records + 1)
)

# Step 4: Create interactive map
leaflet(spatial_grid_wgs84) %>%
  addProviderTiles("OpenStreetMap") %>%
  addPolygons(
    fillColor = ~pal(log10(n_records + 1)),
    popup = ~paste0("Records: ", n_records, "<br>Species: ", n_species),
    highlightOptions = highlightOptions(...)
  ) %>%
  addLegend(...)
```

**Key Functions**:
- `leaflet()`: Initialize interactive map
- `addProviderTiles()`: Add basemap (OpenStreetMap)
- `addPolygons()`: Add spatial features with interactivity
- `colorNumeric()`: Create color scale for continuous variables

**Educational Notes**:
- Leaflet maps are fully interactive: zoom, pan, click for details
- Popups show exact values on click
- Students can explore spatial patterns hands-on
- Basemap provides geographic context

---

### Figure 5: Temporal Trends (Static)

**Location**: `docs/kenya_gbif_bias_assessment.Rmd:1222-1224`

**Data Source**:
- `figures/05_temporal_trends.png` (generated by `03_temporal_bias.R`)

**Source Code**: `scripts/03_temporal_bias.R:190-250`

**Function Flow**:
```r
# Step 1: Aggregate temporal data (already done)
temporal_stats <- # ... (see Table 8 above)

# Step 2: Create dual-axis plot
p <- ggplot(temporal_stats, aes(x = year)) +
  geom_line(aes(y = n_records), color = "#2E86AB", size = 1) +
  geom_line(aes(y = n_species * 10), color = "#A23B72", size = 1) +
  scale_y_continuous(
    name = "Number of Records",
    sec.axis = sec_axis(~./10, name = "Number of Species")
  ) +
  theme_minimal()

# Step 3: Save figure
ggsave("figures/05_temporal_trends.png", p, width = 10, height = 6)
```

**Key Functions**:
- `ggplot() + geom_line()`: Line plot over time
- `sec_axis()`: Create secondary y-axis with scaling
- `scale_y_continuous()`: Configure axis labels and transformations

**Educational Notes**:
- Dual-axis plot allows comparing trends with different scales
- Species scaled by 10x to fit on same plot
- Shows both volume (records) and diversity (species) over time

---

### Figure 5 (Interactive): Plotly Temporal Trends

**Location**: `docs/kenya_gbif_bias_assessment.Rmd:1226-1332`

**Data Source**:
- `data/outputs/temporal_statistics.rds` (from `03_temporal_bias.R`)

**Function Flow**:
```r
# Step 1: Load temporal data
temporal_data <- readRDS("data/outputs/temporal_statistics.rds")

# Step 2: Create interactive plotly chart with dual axes
fig <- plot_ly(temporal_data)

# Step 3: Add records trace
fig <- fig %>%
  add_trace(
    x = ~year,
    y = ~n_records,
    name = "Records",
    type = 'scatter',
    mode = 'lines+markers',
    hovertemplate = '<b>Year:</b> %{x}<br><b>Records:</b> %{y:,}<br>'
  )

# Step 4: Add species trace on secondary axis
fig <- fig %>%
  add_trace(
    x = ~year,
    y = ~n_species,
    name = "Species",
    yaxis = 'y2',  # Use secondary y-axis
    hovertemplate = '<b>Year:</b> %{x}<br><b>Species:</b> %{y}<br>'
  )

# Step 5: Configure layout
fig <- fig %>%
  layout(
    yaxis = list(title = "Number of Records"),
    yaxis2 = list(
      title = "Number of Species",
      overlaying = 'y',
      side = 'right'
    ),
    hovermode = 'x unified'
  )
```

**Key Functions**:
- `plot_ly()`: Initialize plotly chart
- `add_trace()`: Add data series
- `layout()`: Configure axes, labels, and interactivity
- `hovertemplate`: Customize hover tooltips

**Educational Notes**:
- Plotly charts support zoom, pan, and interactive hover
- Dual y-axes without scaling (each has its own range)
- `hovermode = 'x unified'` shows all traces at once when hovering
- Students can zoom into specific time periods

---

## Analysis Script Reference

### 01_data_download.R (354 lines)

**Purpose**: Download and initial cleaning of GBIF data

**Key Outputs**:
- `data/raw/kenya_gbif_raw.rds`: Raw GBIF download
- `data/processed/kenya_gbif_clean.rds`: Coordinate-cleaned data

**Key Functions**:
- `occ_download()`: GBIF download API
- `clean_coordinates()`: CoordinateCleaner package
- `cc_val()`, `cc_equ()`, `cc_zero()`: Coordinate validity tests

**Function Flow**:
```r
# 1. Download from GBIF
kenya_raw <- occ_download(country = "KE")

# 2. Basic filtering
kenya_filtered <- kenya_raw %>%
  filter(!is.na(decimalLatitude), !is.na(decimalLongitude))

# 3. Coordinate cleaning
kenya_clean <- clean_coordinates(
  kenya_filtered,
  lon = "decimalLongitude",
  lat = "decimalLatitude",
  tests = c("capitals", "centroids", "equal", "zeros", "urban", "outliers")
)

# 4. Save
saveRDS(kenya_clean, "data/processed/kenya_gbif_clean.rds")
```

---

### 01b_data_quality_assessment.R (584 lines)

**Purpose**: Comprehensive quality flagging and assessment

**Key Outputs**:
- `data/processed/kenya_gbif_flagged.rds`: Data with quality flags
- `data/outputs/data_quality_assessment.rds`: Quality metrics
- `data/outputs/coordinate_issues_summary.csv`: CoordinateCleaner results

**Quality Flags Added** (14 boolean columns):
- `flag_missing_coords`: No lat/lon
- `flag_high_uncertainty`: Coordinate uncertainty > 10 km
- `flag_inappropriate_basis`: Fossil or living specimen
- `flag_missing_species`: No species-level ID
- `flag_invalid_date`: Date before 1950 or in future
- `flag_duplicate`: Exact duplicate record
- `flag_coord_capitals`: Near capital cities
- `flag_coord_centroids`: Near centroids
- `flag_coord_equal`: Lat = Lon
- `flag_coord_gbif`: At GBIF headquarters
- `flag_coord_zeros`: At (0, 0)
- `flag_coord_urban`: In urban areas
- `flag_coord_outliers`: Statistical outlier
- `flag_any_coord_issue`: Any coordinate flag

**Function Flow**:
```r
# 1. Load clean data
kenya <- readRDS("data/processed/kenya_gbif_clean.rds")

# 2. Add quality flags (one per issue type)
kenya_flagged <- kenya %>%
  mutate(
    flag_missing_coords = is.na(decimalLatitude) | is.na(decimalLongitude),
    flag_high_uncertainty = coordinateUncertaintyInMeters > 10000,
    flag_missing_species = is.na(species),
    # ... etc
  )

# 3. Calculate retention under different strategies
quality_assessment <- list(
  n_original = nrow(kenya_flagged),
  clean_counts = list(
    minimal_clean = sum(!kenya_flagged$flag_missing_coords &
                       !kenya_flagged$flag_missing_species),
    moderate_clean = sum(<moderate criteria>),
    strict_clean = sum(<all flags pass>)
  ),
  # ... metrics
)

# 4. Save
saveRDS(kenya_flagged, "data/processed/kenya_gbif_flagged.rds")
saveRDS(quality_assessment, "data/outputs/data_quality_assessment.rds")
```

**Educational Notes**:
- **Flagging vs. Filtering**: Records are flagged but NOT removed
- Each analysis applies appropriate filters for its needs
- Preserves data for analyses where "issues" are the signal (e.g., urban bias)

---

### 02_spatial_bias.R (646 lines)

**Purpose**: Assess spatial patterns and biases

**Key Outputs**:
- `data/outputs/spatial_grid_effort.rds`: Hexagonal grid with metrics
- `data/outputs/moran_results.rds`: Spatial autocorrelation
- `data/outputs/environmental_bias.rds`: KS tests for env bias
- `figures/01_sampling_effort_map.png`: Spatial distribution map

**Key Functions**:
- `st_make_grid()`: Create hexagonal grid
- `moran.test()`: Global Moran's I
- `terra::extract()`: Extract environmental values
- `ks.test()`: Kolmogorov-Smirnov test

**Function Flow**:
```r
# 1. Filter flagged data (spatial-specific criteria)
spatial_data <- kenya_flagged %>%
  filter(
    !flag_missing_coords,
    !flag_missing_species,
    !flag_coord_equal,
    !flag_coord_gbif,
    !flag_coord_zeros
  )
  # NOTE: Retains urban/capital flags (measuring this bias!)

# 2. Create spatial grid
grid <- st_make_grid(kenya_boundary, cellsize = 0.1, square = FALSE)

# 3. Calculate metrics per cell
spatial_grid <- grid %>%
  st_join(spatial_data) %>%
  group_by(grid_id) %>%
  summarize(
    n_records = n(),
    n_species = n_distinct(species),
    n_families = n_distinct(family)
  )

# 4. Spatial autocorrelation
neighbors <- poly2nb(spatial_grid)
weights <- nb2listw(neighbors)
moran <- moran.test(spatial_grid$n_records, weights)

# 5. Environmental bias
sampled_env <- terra::extract(env_stack, spatial_data[, c("lon", "lat")])
available_env <- terra::spatSample(env_stack, size = 10000)
env_bias <- ks.test(sampled_env$elevation, available_env$elevation)

# 6. Save outputs
saveRDS(spatial_grid, "data/outputs/spatial_grid_effort.rds")
saveRDS(moran, "data/outputs/moran_results.rds")
```

**Educational Notes**:
- Different filtering than quality assessment (keeps urban bias!)
- Hexagons provide better spatial properties than squares
- Environmental bias uses 2-sample KS test (sampled vs available)

---

### 03_temporal_bias.R (472 lines)

**Purpose**: Assess temporal patterns and trends

**Key Outputs**:
- `data/outputs/temporal_statistics.rds`: Year-by-year summary
- `data/outputs/trend_tests.rds`: Mann-Kendall tests
- `data/outputs/species_accumulation.rds`: Cumulative species over time
- `figures/05_temporal_trends.png`: Temporal trends plot

**Key Functions**:
- `group_by(year) + summarize()`: Temporal aggregation
- `MannKendall()`: Non-parametric trend test
- `iNEXT()`: Species accumulation curves

**Function Flow**:
```r
# 1. Filter flagged data (temporal-specific criteria)
temporal_data <- kenya_flagged %>%
  filter(
    !flag_missing_species,
    !flag_invalid_date
  )
  # NOTE: Coordinates NOT required for temporal analysis!

# 2. Aggregate by year
temporal_stats <- temporal_data %>%
  filter(!is.na(year)) %>%
  group_by(year) %>%
  summarize(
    n_records = n(),
    n_species = n_distinct(species),
    n_families = n_distinct(family)
  )

# 3. Trend tests
library(Kendall)
mk_records <- MannKendall(temporal_stats$n_records)
mk_species <- MannKendall(temporal_stats$n_species)

trend_tests <- data.frame(
  variable = c("n_records", "n_species"),
  tau = c(mk_records$tau, mk_species$tau),
  p_value = c(mk_records$sl, mk_species$sl)
)

# 4. Species accumulation
library(iNEXT)
species_by_year <- temporal_data %>%
  group_by(year, species) %>%
  summarize(n = n()) %>%
  pivot_wider(names_from = species, values_from = n, values_fill = 0)

accumulation <- iNEXT(species_by_year, q = 0, datatype = "incidence_raw")

# 5. Save
saveRDS(temporal_stats, "data/outputs/temporal_statistics.rds")
saveRDS(trend_tests, "data/outputs/trend_tests.rds")
```

**Educational Notes**:
- Temporal analysis doesn't need coordinates (only dates and species)
- Mann-Kendall is robust to non-normality and outliers
- iNEXT uses rarefaction to account for unequal sampling

---

### 04_taxonomic_bias.R (632 lines)

**Purpose**: Assess taxonomic coverage and inequality

**Key Outputs**:
- `data/outputs/class_summary.rds`: Class-level statistics
- `data/outputs/gini_coefficients.rds`: Inequality metrics
- `data/outputs/taxonomic_diversity.rds`: Shannon, Simpson indices
- `figures/10_taxonomic_treemap.png`: Hierarchical treemap
- `figures/11_lorenz_curve.png`: Inequality visualization

**Key Functions**:
- `group_by() + summarize()`: Taxonomic aggregation
- `calculate_gini()`: Gini coefficient for inequality
- `diversity()`: Shannon and Simpson indices (vegan package)
- `geom_treemap()`: Hierarchical visualization

**Function Flow**:
```r
# 1. Filter flagged data (taxonomic-specific criteria)
taxonomic_data <- kenya_flagged %>%
  filter(!flag_missing_species)
  # NOTE: Coordinates and dates NOT required!

# 2. Hierarchical summaries
class_summary <- taxonomic_data %>%
  group_by(kingdom, phylum, class) %>%
  summarize(
    n_records = n(),
    n_species = n_distinct(species),
    n_families = n_distinct(family)
  )

# 3. Inequality metrics
record_distribution <- class_summary$n_records
gini_coef <- calculate_gini(record_distribution)

# Custom Gini function:
calculate_gini <- function(x) {
  x <- sort(x[x > 0])
  n <- length(x)
  index <- 1:n
  2 * sum((index - (n + 1)/2) * x) / (n * sum(x))
}

# 4. Diversity indices
library(vegan)
species_counts <- table(taxonomic_data$species)
shannon <- diversity(species_counts, index = "shannon")
simpson <- diversity(species_counts, index = "simpson")

# 5. Treemap visualization
library(treemapify)
ggplot(class_summary, aes(area = n_records, fill = n_species,
                          label = class, subgroup = phylum)) +
  geom_treemap() +
  geom_treemap_subgroup_border() +
  geom_treemap_text(color = "white", place = "centre")

# 6. Save
saveRDS(class_summary, "data/outputs/class_summary.rds")
saveRDS(gini_coef, "data/outputs/gini_coefficients.rds")
```

**Educational Notes**:
- Gini coefficient ranges from 0 (perfect equality) to 1 (perfect inequality)
- Shannon index accounts for both richness and evenness
- Treemaps show hierarchical structure (kingdom ‚Üí phylum ‚Üí class)
- Area = records, Color = species richness

---

### 05_statistical_models.R (885 lines)

**Purpose**: Model predictors of sampling effort

**Key Outputs**:
- `data/outputs/glm_model.rds`: Negative Binomial GLM
- `data/outputs/gam_model.rds`: Generalized Additive Model
- `data/outputs/model_predictions.rds`: Predicted vs. observed
- `figures/23_coefficient_plot.png`: Effect sizes
- `figures/22_sampling_bias_map.png`: Residuals map

**Key Functions**:
- `glm.nb()`: Negative Binomial GLM (MASS package)
- `gam()`: Generalized Additive Model (mgcv package)
- `AIC()`: Model selection
- `predict()`: Generate predictions

**Function Flow**:
```r
# 1. Filter data (strict for modeling)
model_data <- kenya_flagged %>%
  filter(
    !flag_missing_coords,
    !flag_missing_species,
    !flag_any_coord_issue,
    !flag_high_uncertainty,
    !flag_coord_outliers  # Critical: remove outliers!
  )

# 2. Extract environmental predictors
env_data <- terra::extract(
  env_stack,  # elevation, temp, precip
  model_data[, c("lon", "lat")]
)

# 3. Calculate distance to cities
cities <- st_read("data/cities.shp")
model_data$dist_city <- st_distance(model_data, cities, by_element = TRUE)

# 4. Fit Negative Binomial GLM
library(MASS)
glm_model <- glm.nb(
  n_records ~ elevation + temperature + precipitation +
              dist_city + lat + lon,
  data = model_data
)

# 5. Fit GAM with smoothers
library(mgcv)
gam_model <- gam(
  n_records ~ s(elevation) + s(temperature) + s(precipitation) +
              s(dist_city) + s(lat, lon),  # 2D spatial smoother
  family = nb(),
  data = model_data
)

# 6. Model selection
AIC(glm_model, gam_model)

# 7. Predictions and residuals
predictions <- predict(gam_model, type = "response")
residuals <- model_data$n_records - predictions

# 8. Save
saveRDS(gam_model, "data/outputs/gam_model.rds")
saveRDS(predictions, "data/outputs/model_predictions.rds")
```

**Educational Notes**:
- Negative Binomial handles overdispersed count data
- GAMs allow non-linear relationships via smooth functions
- Residuals show where model under/over-predicts
- Positive residuals = areas sampled more than predicted

---

## Utility Functions (R/utils.R)

### calculate_grid_metrics()

**Location**: `R/utils.R:39-85`

**Purpose**: Calculate summary metrics for spatial grid cells

**Usage**:
```r
grid_metrics <- calculate_grid_metrics(
  data = kenya_clean,
  grid = spatial_grid,
  coord_cols = c("decimalLongitude", "decimalLatitude")
)
```

**Function Flow**:
```r
calculate_grid_metrics <- function(data, grid, coord_cols = c("lon", "lat")) {
  # 1. Convert data to spatial points
  points_sf <- st_as_sf(
    data,
    coords = coord_cols,
    crs = st_crs(grid)
  )

  # 2. Spatial join points to grid
  joined <- st_join(points_sf, grid)

  # 3. Calculate metrics per cell
  metrics <- joined %>%
    st_drop_geometry() %>%
    group_by(grid_id) %>%
    summarize(
      n_records = n(),
      n_species = n_distinct(species),
      n_families = n_distinct(family),
      n_orders = n_distinct(order),
      median_uncertainty = median(coordinateUncertaintyInMeters, na.rm = TRUE)
    )

  # 4. Join back to grid geometry
  grid_with_metrics <- grid %>%
    left_join(metrics, by = "grid_id") %>%
    mutate(across(where(is.numeric), ~replace_na(., 0)))

  return(grid_with_metrics)
}
```

**Educational Notes**:
- Spatial join is core operation for grid-based analysis
- `st_join()` finds which grid cell each point falls in
- `group_by(grid_id)` aggregates all points in same cell
- Missing values set to 0 for cells with no records

---

### calculate_gini()

**Location**: `R/utils.R:120-142`

**Purpose**: Calculate Gini coefficient for inequality

**Usage**:
```r
gini_coef <- calculate_gini(class_summary$n_records)
```

**Mathematical Formula**:
```
Gini = (2 * Œ£(i * x_i)) / (n * Œ£(x_i)) - (n + 1) / n

Where:
- x_i = sorted values (ascending)
- i = rank (1 to n)
- n = number of observations
```

**Function Flow**:
```r
calculate_gini <- function(x, na.rm = TRUE) {
  # 1. Remove missing values
  if (na.rm) x <- x[!is.na(x)]

  # 2. Remove zeros
  x <- x[x > 0]

  # 3. Sort ascending
  x <- sort(x)

  # 4. Calculate ranks
  n <- length(x)
  index <- 1:n

  # 5. Apply Gini formula
  gini <- 2 * sum((index - (n + 1)/2) * x) / (n * sum(x))

  return(gini)
}
```

**Educational Notes**:
- Gini = 0: Perfect equality (all classes have equal records)
- Gini = 1: Perfect inequality (one class has all records)
- Higher Gini = more biased toward few dominant groups
- Same metric used for income inequality in economics

---

### create_interactive_table()

**Location**: `docs/kenya_gbif_bias_assessment.Rmd:113-145`

**Purpose**: Create DT interactive table with source annotation

**Usage**:
```r
create_interactive_table(
  data = summary_table,
  caption = "Table 1: Summary Statistics",
  pageLength = 10,
  source_info = list(
    file = "docs/kenya_gbif_bias_assessment.Rmd",
    lines = "407-432",
    function = "data.frame() + create_interactive_table()",
    description = "Summary statistics from quality filtering"
  )
)
```

**Features**:
- Sortable columns (click headers)
- Search/filter (top row)
- Pagination (show 10/50/100 rows)
- Export buttons (CSV, Excel)
- Source code annotation

**Educational Notes**:
- DT package builds on JavaScript DataTables library
- `extensions = 'Buttons'` enables export functionality
- `filter = 'top'` adds search boxes to each column
- Function flow annotation shows code lineage

---

## Interactive Features for Education

### Why Interactive?

1. **Active Learning**: Students explore data hands-on rather than passively viewing
2. **Pattern Discovery**: Zooming and filtering reveals patterns not obvious in static views
3. **Reproducibility**: Source code annotations link outputs to methods
4. **Engagement**: Interactive features increase curiosity and retention

### Interactive Table Features (DT)

**Capabilities**:
- Sort by any column (ascending/descending)
- Filter by text, number ranges, categories
- Paginate through large datasets
- Export selections to CSV/Excel
- Responsive design (works on mobile)

**Educational Use Cases**:
- Find specific species in taxonomic tables
- Sort by record count to identify dominant taxa
- Filter to specific regions or time periods
- Export subsets for further analysis

**Example Code**:
```r
datatable(
  data,
  options = list(
    pageLength = 10,        # Rows per page
    searching = TRUE,       # Enable search
    ordering = TRUE,        # Enable sorting
    dom = 'Bfrtip',        # Layout: Buttons, filter, table, info, pagination
    buttons = c('copy', 'csv', 'excel')  # Export buttons
  ),
  filter = 'top'           # Filter boxes at top of columns
)
```

---

### Interactive Map Features (Leaflet)

**Capabilities**:
- Zoom in/out (mouse wheel or +/- buttons)
- Pan by dragging
- Click cells for detailed popup
- Hover to highlight
- Layer toggles (if multiple layers)
- Basemap selection (OpenStreetMap, satellite, etc.)

**Educational Use Cases**:
- Explore geographic patterns at multiple scales
- Identify sampling gaps in specific regions
- Compare urban vs. rural sampling intensity
- Understand spatial autocorrelation visually

**Example Code**:
```r
leaflet(spatial_data) %>%
  addProviderTiles("OpenStreetMap") %>%
  addPolygons(
    fillColor = ~colorPalette(value),
    fillOpacity = 0.7,
    popup = ~paste("Value:", value),  # Click for details
    highlightOptions = highlightOptions(  # Hover effect
      weight = 3,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    )
  ) %>%
  addLegend(...)
```

---

### Interactive Chart Features (Plotly)

**Capabilities**:
- Zoom (box select or scroll)
- Pan (drag while zoomed)
- Hover for exact values
- Toggle traces on/off (click legend)
- Download as PNG
- Autoscale axes
- Unified hover mode (all traces at once)

**Educational Use Cases**:
- Zoom into specific time periods in temporal trends
- Compare multiple variables simultaneously
- Identify exact values at points of interest
- Export figures for presentations

**Example Code**:
```r
plot_ly(data, x = ~year, y = ~n_records) %>%
  add_trace(
    type = 'scatter',
    mode = 'lines+markers',
    hovertemplate = '<b>Year:</b> %{x}<br><b>Records:</b> %{y:,}<br>'
  ) %>%
  layout(
    hovermode = 'x unified',  # Show all traces on hover
    xaxis = list(title = "Year"),
    yaxis = list(title = "Records")
  ) %>%
  config(
    displayModeBar = TRUE,
    modeBarButtonsToRemove = c('lasso2d', 'select2d')
  )
```

---

### Function Flow Annotations

**Purpose**: Show exactly which code generated each output

**Features**:
- Source file and line numbers
- Function names
- Description of transformation
- Educational context

**Example**:
```r
create_function_flow(
  source_file = "scripts/02_spatial_bias.R",
  line_range = "50-125",
  function = "calculate_grid_metrics() + leaflet()",
  description = "Hexagonal grid created with st_make_grid(), metrics calculated by joining occurrence data to grid cells"
)
```

**Renders as**:
```
üìä Function Flow:
Hexagonal grid created with st_make_grid(), metrics calculated by joining occurrence data to grid cells
Source: scripts/02_spatial_bias.R lines 50-125
Function: calculate_grid_metrics() + leaflet()
Educational note: This output was generated by the code above. You can trace back through the analysis pipeline to understand how raw data becomes final results.
```

---

### Collapsible Code Blocks

**Purpose**: Show source code without cluttering the document

**Features**:
- Collapsed by default (click to expand)
- Syntax highlighting
- Scrollable for long code
- Educational annotations

**Example**:
```r
create_code_details("
# Temporal aggregation performed in 03_temporal_bias.R
temporal_stats <- kenya_clean %>%
  filter(!is.na(year)) %>%
  group_by(year) %>%
  summarize(
    n_records = n(),
    n_species = n_distinct(species),
    .groups = 'drop'
  )
",
  title = "View temporal aggregation code"
)
```

**Renders as**:
```
üîç View temporal aggregation code (click to expand)
[Collapsed by default - click summary to reveal code]
```

---

## Learning Pathways

### For Beginners: Understanding the Pipeline

1. **Start with Table 1** (Summary Statistics)
   - Read the function flow annotation
   - Click the collapsible code block
   - Trace back to `01b_data_quality_assessment.R`
   - Understand: Raw data ‚Üí Quality filtering ‚Üí Summary

2. **Explore the Interactive Map**
   - Zoom into Nairobi (high sampling)
   - Zoom into northern Kenya (low sampling)
   - Click cells to see exact counts
   - Question: Why is sampling so uneven?

3. **Play with Temporal Trends**
   - Zoom into specific decades
   - Hover to see exact values
   - Compare records vs. species trends
   - Question: What caused the spike in recent years?

### For Intermediate Users: Reproducing Results

1. **Choose an output** (e.g., Spatial Autocorrelation Table)
2. **Read the function flow** to find source file
3. **Open the source file** (`scripts/02_spatial_bias.R`)
4. **Follow the code** step-by-step:
   - What inputs are loaded?
   - What transformations are applied?
   - What tests are run?
   - What outputs are saved?
5. **Run the script** yourself to reproduce the result
6. **Modify parameters** (e.g., change grid resolution from 10km to 20km)
7. **Compare** your modified results to the original

### For Advanced Users: Extending the Analysis

1. **Identify a limitation** (e.g., environmental bias only tests 3 variables)
2. **Locate relevant code** using function flow annotations
3. **Add new variables** (e.g., distance to roads, protected areas)
4. **Run modified analysis**
5. **Create new interactive visualizations** using the helper functions
6. **Document your extension** with function flow annotations

---

## Glossary of Key Terms

**Flagging vs. Filtering**:
- **Flagging**: Marking records with potential issues using boolean columns (TRUE/FALSE)
- **Filtering**: Removing records based on flag criteria
- **Advantage**: Flags preserve data for analyses where "issues" are the signal

**Spatial Autocorrelation**:
- Measure of whether nearby locations have similar values
- **Moran's I**: Ranges from -1 (dispersion) to +1 (clustering)
- **Educational example**: High Moran's I for sampling effort = records are clustered, not random

**Environmental Bias**:
- Systematic deviation of sampled locations from available environmental space
- **Kolmogorov-Smirnov test**: Compares two distributions
- **Educational example**: If D = 0.5 for elevation, sampled elevations differ greatly from all elevations in Kenya

**Gini Coefficient**:
- Measure of inequality in a distribution
- Ranges from 0 (perfect equality) to 1 (perfect inequality)
- **Educational example**: Gini = 0.8 for taxonomic classes = few classes have most records

**Mann-Kendall Test**:
- Non-parametric test for monotonic trends in time series
- Doesn't assume normality or linear relationships
- **Kendall's œÑ**: Ranges from -1 (decreasing) to +1 (increasing)

**Hexagonal Grid**:
- Spatial tessellation using hexagons instead of squares
- **Advantages**: Equal area, better neighbor relationships (6 neighbors vs. 4 or 8)
- **Educational example**: 10 km grid = ~100 km¬≤ hexagons covering Kenya

---

## Further Reading

### R Packages Used

1. **tidyverse**: Data manipulation and visualization
   - `dplyr`: Data transformation (`filter`, `mutate`, `summarize`)
   - `ggplot2`: Static visualizations
   - `purrr`: Functional programming

2. **sf**: Spatial data handling
   - Modern replacement for `sp` package
   - Works with tidyverse syntax
   - `st_make_grid()`, `st_join()`, `st_transform()`

3. **DT**: Interactive tables
   - Based on JavaScript DataTables
   - `datatable()`, `formatStyle()`, `formatCurrency()`

4. **plotly**: Interactive charts
   - Converts ggplot2 to interactive: `ggplotly()`
   - Native plotly: `plot_ly()`, `add_trace()`, `layout()`

5. **leaflet**: Interactive maps
   - Based on JavaScript Leaflet library
   - `leaflet()`, `addPolygons()`, `addMarkers()`

6. **terra**: Raster data and spatial operations
   - Replacement for `raster` package (faster)
   - `extract()`, `spatSample()`, `crop()`

7. **spdep**: Spatial statistics
   - `poly2nb()`: Create neighbors
   - `moran.test()`: Spatial autocorrelation

8. **vegan**: Ecological diversity indices
   - `diversity()`: Shannon, Simpson indices
   - `specaccum()`: Species accumulation

### Methodological References

1. **Data Quality**: Zizka et al. 2019. CoordinateCleaner: Standardized cleaning of occurrence records from biological collection databases. *Methods in Ecology and Evolution* 10(5):744-751.

2. **Spatial Bias**: Hortal et al. 2008. Historical bias in biodiversity inventories affects the observed environmental niche of the species. *Oikos* 117(6):847-858.

3. **Temporal Bias**: Troudet et al. 2017. Taxonomic bias in biodiversity data and societal preferences. *Scientific Reports* 7:9132.

4. **Environmental Bias**: Reddy & D√°valos 2003. Geographical sampling bias and its implications for conservation priorities in Africa. *Journal of Biogeography* 30(11):1719-1727.

---

## Contact & Contributions

This function flow documentation is designed to be a living document. If you:
- Find errors or omissions
- Have suggestions for improvement
- Want to add new interactive features
- Need clarification on any analysis

Please open an issue or submit a pull request on the project repository.

**Educational Mission**: This project demonstrates best practices in reproducible biodiversity data science, with explicit code lineage and interactive exploration capabilities suitable for teaching and learning.

---

**Document Version**: 1.0
**Last Updated**: 2025-11-23
**Maintainer**: Kwiz Computing Technologies
