---
title: "Comprehensive Data Quality Assessment and Bias Analysis of GBIF Biodiversity Data in Kenya"
author:
  - name: "Research Team"
    affiliation: "Biodiversity Research Institute"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    theme: flatly
    highlight: tango
    code_folding: hide
    fig_width: 10
    fig_height: 8
  pdf_document:
    toc: true
    number_sections: true
    fig_caption: yes
  word_document:
    toc: true
    fig_caption: yes
bibliography: references.bib
csl: ecology.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  cache = FALSE
)

# Load required packages
suppressPackageStartupMessages({
  library(tidyverse)
  library(sf)
  library(here)
  library(knitr)
  library(kableExtra)
  library(patchwork)
  library(viridis)
  library(scales)
})

# Source utility functions
source(here("R", "utils.R"))

# Set paths
data_processed <- here("data", "processed")
data_outputs <- here("data", "outputs")
figures_dir <- here("figures")

# Load all data needed for abstract and inline R code
quality_assessment <- readRDS(file.path(data_outputs, "data_quality_assessment.rds"))
quality_tracking <- quality_assessment$tracking
coord_issues <- quality_assessment$coord_issues
quality_metrics <- quality_assessment$metrics

# Load flagged data to get actual filtering numbers
kenya_flagged <- readRDS(file.path(data_processed, "kenya_gbif_flagged.rds"))

spatial_summary <- readRDS(file.path(data_outputs, "spatial_bias_summary.rds"))
moran_results <- spatial_summary$moran_results

temporal_summary <- readRDS(file.path(data_outputs, "temporal_bias_summary.rds"))
temporal_stats <- temporal_summary$temporal_stats

taxonomic_summary <- readRDS(file.path(data_outputs, "taxonomic_bias_summary.rds"))
diversity_metrics <- taxonomic_summary$diversity_metrics
gini_records <- diversity_metrics$value[diversity_metrics$metric == "Gini_records"]
```

# Abstract

**Background:** The Global Biodiversity Information Facility (GBIF) represents the largest repository of species occurrence data globally. However, these data are known to suffer from both quality issues and systematic biases that can affect biodiversity assessments and conservation decisions. Understanding data quality and biases is critical for Kenya, a biodiversity hotspot with rich fauna and flora.

**Objectives:** We conducted a comprehensive assessment of data quality issues and systematic biases in GBIF occurrence data for Kenya, quantifying all quality filters applied during data cleaning and analyzing spatial, temporal, and taxonomic biases using reproducible R-based workflows.

**Methods:** We downloaded `r format(quality_assessment$n_original, big.mark=",")` occurrence records from GBIF for Kenya and implemented a systematic quality **flagging** framework that identified and documented seven major quality issues: missing coordinates, high coordinate uncertainty (>10 km), inappropriate basis of record, missing species identification, invalid dates, duplicates, and coordinate quality flags. Critically, our approach **flags** quality issues rather than automatically removing records, allowing each downstream analysis to apply filters appropriate to its specific requirements. We assessed spatial bias using grid-based analyses, environmental space coverage, and spatial autocorrelation. Temporal bias was evaluated through trend analyses and completeness metrics. Taxonomic bias was quantified using diversity indices and species accumulation curves. We used generalized linear models (GLMs) and generalized additive models (GAMs) to identify environmental and geographic predictors of sampling effort.

**Results:** Data quality assessment identified issues in `r format(sum(rowSums(kenya_flagged[,grepl("^flag_", names(kenya_flagged))]) > 0), big.mark=",")` records (`r round(100 * sum(rowSums(kenya_flagged[,grepl("^flag_", names(kenya_flagged))]) > 0) / nrow(kenya_flagged), 1)`%), with coordinate-related problems being most common: `r format(quality_metrics$coordinate_quality$total_coord_flags, big.mark=",")` records (`r round(quality_metrics$coordinate_quality$percent_coord_flags, 1)`%) flagged by CoordinateCleaner tests. Different analyses applied different quality filters based on their requirements: spatial bias analysis required valid coordinates and species identification but retained records near cities (the bias being measured); temporal analysis required valid dates but not coordinates; taxonomic analysis required only species identification. Under a moderate filtering strategy (coordinates, species, dates, no coordinate errors), `r format(quality_assessment$clean_counts$moderate_clean, big.mark=",")` records (`r round(100 * quality_assessment$clean_counts$moderate_clean / quality_assessment$n_original, 1)`%) were retained. Our bias analyses revealed significant spatial, temporal, and taxonomic biases. Sampling effort was concentrated near major cities and roads, with strong positive spatial autocorrelation (Moran's I = `r round(moran_results$morans_i, 3)`, p < 0.001). Temporal analysis showed increasing trends in data collection, with `r round(temporal_stats$cv_records, 2)` coefficient of variation in annual records. Taxonomic coverage was highly uneven, with a Gini coefficient of `r round(gini_records, 3)` for records across classes.

**Conclusions:** GBIF data for Kenya exhibit substantial quality issues and multiple forms of systematic bias that must be considered in biodiversity assessments. Our flagging approach allows different analyses to apply appropriate quality filters rather than using one-size-fits-all cleaning. We provide detailed quantification of all data quality flags, document which filters each analysis applies and why, provide recommendations for targeted sampling to address identified gaps, and present a reproducible framework applicable to other regions. This flexible, transparent approach to quality assessment and bias analysis provides a template for modern biodiversity data science.

**Keywords:** GBIF, biodiversity data, data quality, sampling bias, Kenya, spatial bias, temporal bias, taxonomic bias, occAssess, reproducible research

# Introduction

## The Growing Importance of Biodiversity Data

Biodiversity data from the Global Biodiversity Information Facility (GBIF) represent one of the largest openly accessible repositories of species occurrence records, with over 2 billion records globally [@gbif2023]. These data are increasingly used for critical applications including conservation planning, climate change research, species distribution modeling, biodiversity assessment, and policy decisions. However, the utility of these data for scientific research and conservation management depends fundamentally on both their quality and their representativeness [@meyer2016].

## The Dual Challenge: Quality and Bias

GBIF data face two distinct but interconnected challenges:

**Data Quality Issues** include technical errors and inconsistencies such as missing or erroneous coordinates, taxonomic misidentifications, temporal inconsistencies, and duplicate records [@zizka2019]. These issues arise from:
- Historical data digitization from museum specimens with incomplete metadata
- Georeferencing errors when converting locality descriptions to coordinates
- Taxonomic revisions that create outdated names in historical records
- Data entry mistakes during transcription
- Multiple institutions sharing the same observations, creating duplicates

**Systematic Biases** stem from non-random sampling efforts that create uneven coverage across space, time, and taxonomic groups [@beck2014; @hortal2015]. Unlike quality issues (which are errors), biases reflect real patterns in where, when, and what researchers choose to sample:
- **Spatial bias**: Sampling concentrated near roads, cities, and accessible areas
- **Temporal bias**: Increased sampling in recent decades due to digital technologies
- **Taxonomic bias**: Focus on charismatic species, economically important groups, or taxa with active hobbyist communities

Both quality issues and systematic biases can substantially affect downstream analyses if not properly addressed. Quality issues introduce noise and errors, while biases create misleading patterns that can be mistaken for biological signals. For example, high species richness near cities might reflect sampling effort rather than true biodiversity patterns.

## The Kenya Context

Kenya, as a biodiversity hotspot with rich fauna and flora across diverse ecosystems (coastal forests, savannas, highlands, and arid lands), has accumulated substantial GBIF records over decades of research and conservation work. However, both the data quality and the spatial, temporal, and taxonomic representativeness of these data remain poorly quantified. Understanding these aspects is critical for Kenya because:

1. The country hosts numerous endemic and threatened species requiring evidence-based conservation
2. Climate change impacts on East African biodiversity need robust baseline data
3. Conservation planning and protected area management depend on accurate species distribution information
4. National biodiversity monitoring and reporting obligations require reliable data sources

## The Documentation Gap

Most biodiversity studies using GBIF data apply quality filters but rarely document and quantify the specific issues encountered. Papers typically report only the final cleaned dataset size (e.g., "After quality filtering, 500,000 records remained"), without specifying:
- What specific quality issues were identified and in how many records
- What percentage of records had coordinate errors, missing taxonomy, or duplicates
- Which quality issues were filtered vs. flagged
- Whether different analyses used different filtering strategies
- Whether sampling biases were assessed and how they might affect conclusions

Furthermore, most studies apply a single, strict filtering strategy to all analyses, potentially:
- Removing the very biases they should be measuring (e.g., filtering urban records when studying spatial bias)
- Discarding records that would be valid for certain analyses (e.g., records without coordinates can still contribute to taxonomic assessments)
- Applying unnecessarily strict standards that reduce sample size without improving analysis quality

This lack of flexibility and transparency makes it difficult to:
- Assess whether quality standards were appropriate for each specific analysis
- Compare data quality across regions or taxonomic groups
- Understand fitness-for-use for different research questions
- Replicate filtering procedures in other studies
- Identify systemic issues that data providers could address

## Study Framework and Tools

Recent advances in biodiversity informatics have produced powerful tools for data quality assessment and bias detection. The occAssess R package [@marsh2023] provides a comprehensive framework for assessing biases in species occurrence data through spatial, temporal, and taxonomic analyses. The CoordinateCleaner package [@zizka2019] offers automated tools for identifying common coordinate quality issues such as institutional coordinates, country centroids, and statistical outliers.

By combining these tools with a flexible flagging approach that documents quality issues without automatically removing records, we can address the dual challenges of quality and bias in a systematic, reproducible way. Understanding both aspects is critical for:

1. **Assessing fitness-for-use**: Different research questions have different quality requirements. Fine-scale conservation planning needs higher coordinate precision than broad-scale macroecological studies.
2. **Informing conservation prioritization**: Biased data can lead to biased conservation decisions, potentially overlooking under-sampled regions with high conservation value.
3. **Improving species distribution models**: Models trained on biased data may produce unreliable predictions, especially when projected to under-sampled regions.
4. **Guiding future survey efforts**: Identifying sampling gaps helps target field work where it will maximize information gain.
5. **Ensuring robust biodiversity assessments**: National reporting on biodiversity trends requires understanding data limitations and potential biases.
6. **Providing transparency**: Full documentation of data processing enables replication, comparison across studies, and informed interpretation of results.

## Study Objectives

This study provides the first comprehensive, reproducible assessment of both data quality issues and systematic biases in biodiversity data for Kenya using standardized methods. Unlike previous studies that focus on either quality or bias, we integrate both perspectives to provide a complete picture of data reliability and representativeness. Critically, we employ a **flagging approach** that identifies quality issues without automatically removing records, allowing each analysis to apply appropriate filters.

Our specific objectives are to:

1. **Systematically flag all data quality issues**, reporting the number and percentage of records affected by each specific issue (missing coordinates, high uncertainty, missing taxonomy, temporal problems, duplicates, coordinate flags), while retaining all records for flexible downstream use

2. **Document filtering decisions for each analysis**, explaining which quality flags are relevant for spatial, temporal, and taxonomic bias assessments, and why certain flags should NOT be filtered (e.g., urban bias is a signal, not noise)

3. **Quantify spatial bias patterns** across Kenya using grid-based analyses, assess sampling completeness, and test for spatial autocorrelation to identify clustered vs. dispersed sampling

4. **Evaluate environmental bias** by comparing sampled locations to available environmental space, determining whether certain habitats, elevations, or climate zones are over- or under-represented

5. **Assess temporal patterns** in data collection, identify temporal gaps and trends, and quantify temporal completeness to understand how data availability varies over time

6. **Evaluate taxonomic bias** across major taxonomic groups using inequality metrics (Gini coefficients), diversity indices, and rarity analyses to identify which taxa are well-sampled vs. neglected

7. **Identify environmental and geographic predictors** of sampling effort using statistical models (GLMs and GAMs) to understand what drives sampling patterns (accessibility, urbanization, environmental conditions)

8. **Provide actionable recommendations** for targeted sampling to address identified gaps, enabling more strategic future data collection

9. **Present a reproducible flagging framework** for comprehensive data quality and bias assessment that can be readily applied to other countries, regions, or taxonomic groups, with clear documentation of which filters are appropriate for different research questions

By achieving these objectives, we provide both a thorough assessment of Kenya's biodiversity data and a methodological template for transparent, reproducible, and flexible data quality and bias assessment in biodiversity informatics.

# Methods

## Data Acquisition and Cleaning

```{r load-data}
# Load flagged data (with quality flags, not filtered)
kenya_flagged <- readRDS(file.path(data_processed, "kenya_gbif_flagged.rds"))

# Also load metadata
metadata <- readRDS(file.path(data_processed, "metadata.rds"))
summary_stats <- readRDS(file.path(data_processed, "summary_stats.rds"))

# Load flagged dataset with all quality flags
kenya_flagged <- readRDS(file.path(data_processed, "kenya_gbif_flagged.rds"))
```

### Data Download

We downloaded species occurrence data for Kenya from GBIF using the `rgbif` package [@chamberlain2023]. The download (DOI: `r metadata$gbif_doi`) was performed on `r metadata$download_date` using GBIF's API. We applied basic filters at the download stage to exclude obviously problematic records:

- **Geographic scope**: Records within Kenya's boundaries (identified by ISO country code KE)
- **Coordinate presence**: Only records with decimal latitude and longitude (excluding records with no georeferencing)
- **GBIF flags**: Excluded records flagged by GBIF as having geospatial issues at the time of download
- **Temporal scope**: Records from 1950 onwards to focus on modern biodiversity patterns and exclude historical records that may have substantial georeferencing uncertainty

These initial filters reduce download size and processing time while retaining the vast majority of usable records. However, additional quality filtering (documented below) is essential because GBIF's automated flags do not catch all quality issues.

### Quality Flagging Approach

Rather than automatically removing problematic records, we implemented a **flagging approach** that identifies quality issues while retaining all records. This approach recognizes that different analyses have different quality requirements and that some apparent "problems" (like urban bias) are actually the signals we want to measure.

Our flagging system adds boolean flag columns to each record indicating specific quality issues:

**Core Quality Flags:**
- `flag_missing_coords`: Missing decimal latitude or longitude
- `flag_high_uncertainty`: Coordinate uncertainty >10 km
- `flag_inappropriate_basis`: Fossil or living specimens
- `flag_missing_species`: No species-level identification
- `flag_invalid_date`: Date before 1950 or in future
- `flag_duplicate`: Exact duplicate record

**CoordinateCleaner Flags:**
- `flag_coord_capitals`: Within 10km of country/province capitals
- `flag_coord_centroids`: Within 5km of country/province centroids
- `flag_coord_urban`: In major urban areas
- `flag_coord_outliers`: Statistical outliers for the species
- `flag_coord_equal`: Identical latitude and longitude values
- `flag_coord_gbif`: At GBIF headquarters location
- `flag_coord_zeros`: At (0,0) or near equator/prime meridian
- `flag_any_coord_issue`: Any of the above coordinate issues

Each downstream analysis then applies filters appropriate to its specific requirements:

- **Spatial bias analysis**: Filters missing coordinates and species, obvious errors (equal/GBIF/zeros), but **retains** urban/capital records (the bias being measured)
- **Temporal bias analysis**: Filters invalid dates and missing species, but **does not require** coordinates
- **Taxonomic bias analysis**: Filters only missing species (coordinates and dates irrelevant)
- **Statistical models**: Applies strict filtering including outliers that would bias environmental relationships

This flexible approach provides several advantages:

1. **Preserves signal**: Biases we want to measure aren't filtered out
2. **Maximizes data use**: Records useful for some analyses aren't discarded
3. **Enables sensitivity analysis**: Different filter combinations can be tested
4. **Improves transparency**: Each analysis documents its filtering rationale
5. **Facilitates replication**: Other studies can apply appropriate filters

### Flagged Dataset Summary

The quality-flagged dataset contains **`r format(nrow(kenya_flagged), big.mark=",")`** occurrence records (all records from the original download). Under different filtering strategies, different numbers of records are retained:

- **Strict filtering** (all flags pass): `r format(quality_assessment$clean_counts$strict_clean, big.mark=",")` records (`r round(100 * quality_assessment$clean_counts$strict_clean / nrow(kenya_flagged), 1)`%)
- **Moderate filtering** (coordinates, species, dates, no coordinate errors): `r format(quality_assessment$clean_counts$moderate_clean, big.mark=",")` records (`r round(100 * quality_assessment$clean_counts$moderate_clean / nrow(kenya_flagged), 1)`%)
- **Minimal filtering** (coordinates and species only): `r format(quality_assessment$clean_counts$minimal_clean, big.mark=",")` records (`r round(100 * quality_assessment$clean_counts$minimal_clean / nrow(kenya_flagged), 1)`%)

The analyses in this manuscript primarily use moderate filtering, representing **`r format(summary_stats$n_species, big.mark=",")`** species from **`r format(summary_stats$n_families, big.mark=",")`** families across **`r format(summary_stats$n_classes, big.mark=",")`** classes.

```{r data-summary-table}
# Create summary table
summary_table <- data.frame(
  Metric = c("Total Records", "Number of Species", "Number of Genera",
             "Number of Families", "Number of Orders", "Number of Classes",
             "Year Range", "Median Coordinate Uncertainty (m)"),
  Value = c(
    format(summary_stats$n_records, big.mark = ","),
    format(summary_stats$n_species, big.mark = ","),
    format(summary_stats$n_genera, big.mark = ","),
    format(summary_stats$n_families, big.mark = ","),
    format(summary_stats$n_orders, big.mark = ","),
    format(summary_stats$n_classes, big.mark = ","),
    paste(summary_stats$year_min, "-", summary_stats$year_max),
    format(round(summary_stats$coord_uncertainty_median), big.mark = ",")
  )
)

kable(summary_table, caption = "Summary statistics of cleaned GBIF data for Kenya",
      booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = FALSE)
```

## Data Quality Assessment

```{r load-quality-data}
# Load comprehensive data quality assessment
quality_assessment <- readRDS(file.path(data_outputs, "data_quality_assessment.rds"))
quality_tracking <- quality_assessment$tracking
coord_issues <- quality_assessment$coord_issues
quality_metrics <- quality_assessment$metrics
```

To comprehensively document data quality issues, we implemented a systematic **flagging framework** that identifies quality issues while retaining all records. Each issue type is flagged with a boolean column, and the number of records flagged at each step was recorded along with the percentage relative to the original dataset (`r format(quality_assessment$n_original, big.mark=",")` records).

### Quality Flagging Categories

The quality flagging process identified seven major categories of issues:

1. **Missing Coordinates:** `r sum(kenya_flagged$flag_missing_coords)` records (`r round(100*sum(kenya_flagged$flag_missing_coords)/nrow(kenya_flagged), 1)`%) lacking decimal latitude or longitude values
2. **High Coordinate Uncertainty:** `r sum(kenya_flagged$flag_high_uncertainty, na.rm=TRUE)` records (`r round(100*sum(kenya_flagged$flag_high_uncertainty, na.rm=TRUE)/nrow(kenya_flagged), 1)`%) with coordinate uncertainty exceeding 10 km
3. **Inappropriate Basis of Record:** `r sum(kenya_flagged$flag_inappropriate_basis)` records (`r round(100*sum(kenya_flagged$flag_inappropriate_basis)/nrow(kenya_flagged), 1)`%) representing fossil or living specimens
4. **Missing Species Identification:** `r sum(kenya_flagged$flag_missing_species)` records (`r round(100*sum(kenya_flagged$flag_missing_species)/nrow(kenya_flagged), 1)`%) without species-level taxonomic identification
5. **Invalid Dates:** `r sum(kenya_flagged$flag_invalid_date)` records (`r round(100*sum(kenya_flagged$flag_invalid_date)/nrow(kenya_flagged), 1)`%) with dates before 1950 or in the future
6. **Duplicate Records:** `r sum(kenya_flagged$flag_duplicate)` records (`r round(100*sum(kenya_flagged$flag_duplicate)/nrow(kenya_flagged), 1)`%) identified as exact duplicates
7. **Coordinate Quality Flags:** `r sum(kenya_flagged$flag_any_coord_issue)` records (`r round(100*sum(kenya_flagged$flag_any_coord_issue)/nrow(kenya_flagged), 1)`%) flagged by one or more CoordinateCleaner tests

**Key Insight:** `r format(sum(rowSums(kenya_flagged[,grepl("^flag_", names(kenya_flagged))]) > 0), big.mark=",")` total records (`r round(100 * sum(rowSums(kenya_flagged[,grepl("^flag_", names(kenya_flagged))]) > 0) / nrow(kenya_flagged), 1)`%) have at least one quality flag, but different analyses filter different flags based on their requirements.

### CoordinateCleaner Tests

The CoordinateCleaner package [@zizka2019] implements automated tests to identify common georeferencing errors in biodiversity data. These tests are based on patterns observed in large-scale data quality assessments and address specific, known sources of coordinate errors. We applied seven independent tests:

- **Capitals Test:** Identifies coordinates within 10 km of country or province capitals. Records may be assigned to capital cities when only administrative-level locality information is available (e.g., "Kenya" geocoded to Nairobi coordinates). Such records have high uncertainty and may not represent actual occurrence locations.

- **Centroids Test:** Flags coordinates within 5 km of country or province geographic centroids. Similar to capitals, these may result from automated geocoding that assigns centroid coordinates when precise locations are unavailable. These are particularly problematic as they appear valid but don't represent real sampling locations.

- **Equal Coordinates Test:** Detects records where latitude equals longitude (e.g., -1.234, -1.234). This pattern is physically possible but statistically unlikely and usually indicates data entry errors such as accidentally copying the same value to both coordinate fields.

- **GBIF Headquarters Test:** Identifies coordinates matching GBIF's headquarters in Copenhagen, Denmark. These obvious errors occur when default coordinates are not replaced during data entry or when specimen metadata is confused with institutional location.

- **Zeros Test:** Flags coordinates at or very near (0°, 0°) in the Atlantic Ocean. True occurrences at this location are rare for terrestrial/freshwater species; most represent null or missing coordinates that were incorrectly assigned default values of zero.

- **Urban Areas Test:** Identifies coordinates in major urban centers that may represent collection storage locations (museums, herbaria, universities) rather than where specimens were actually collected. This is particularly relevant for historical collections where only institutional location was recorded.

- **Outliers Test:** Uses statistical methods to detect coordinates that are geographically distant from other records of the same species. The test employs a quantile-based approach, flagging records in the outer 5% of distance distributions for species with sufficient records (≥10). Outliers may indicate misidentifications, transcription errors (e.g., coordinate decimal place errors), or valid range extensions requiring verification.

**Important Note:** Each test is independent, so a single record can be flagged by multiple tests. The `flag_any_coord_issue` column indicates records flagged by ANY test. **Critically, records are flagged but not automatically removed.** Each downstream analysis decides which flags to filter:

- **Spatial bias analysis** RETAINS capital/urban/centroid flags (these ARE the bias!)
- **Temporal bias analysis** IGNORES coordinate flags (not relevant to temporal patterns)
- **Statistical models** FILTERS outliers (would bias environmental relationships)
- **Taxonomic analysis** IGNORES all coordinate flags (taxonomy not location-dependent)

This flexible approach ensures we don't remove the very biases we're trying to measure while still identifying problematic coordinates for analyses that need spatial precision.

### Filtering Strategies and Data Retention

Since records are flagged rather than removed, we can assess data retention under different filtering strategies:

- **No filtering (all records):** `r format(nrow(kenya_flagged), big.mark=",")` records (100%)
- **Minimal filtering** (coordinates + species only): `r format(quality_assessment$clean_counts$minimal_clean, big.mark=",")` records (`r round(100 * quality_assessment$clean_counts$minimal_clean / nrow(kenya_flagged), 1)`%)
- **Moderate filtering** (+ dates + coordinate errors): `r format(quality_assessment$clean_counts$moderate_clean, big.mark=",")` records (`r round(100 * quality_assessment$clean_counts$moderate_clean / nrow(kenya_flagged), 1)`%)
- **Strict filtering** (all flags must pass): `r format(quality_assessment$clean_counts$strict_clean, big.mark=",")` records (`r round(100 * quality_assessment$clean_counts$strict_clean / nrow(kenya_flagged), 1)`%)

**The analyses in this manuscript primarily use moderate filtering**, which retains records suitable for spatial and temporal bias assessment while removing clear errors. Different research questions may justify different filtering strategies - this is the advantage of the flagging approach.

## Spatial Bias Assessment

```{r load-spatial-data}
spatial_grid <- readRDS(file.path(data_outputs, "spatial_grid_effort.rds"))
spatial_summary <- readRDS(file.path(data_outputs, "spatial_bias_summary.rds"))
moran_results <- spatial_summary$moran_results
env_bias <- spatial_summary$env_bias_summary
```

Spatial bias occurs when sampling effort is unevenly distributed across geographic space, typically concentrated in accessible areas near roads, cities, and research stations. We assessed spatial patterns using multiple complementary approaches:

### Grid-Based Analysis

We overlaid Kenya with a hexagonal grid at ~10 km resolution (approximately 100 km² per cell). Hexagonal grids are preferred over square grids because they:
- Have equal distances between centroids of neighboring cells
- Better approximate circular neighborhoods
- Reduce edge effects in spatial analyses
- Provide better tessellation for mapping

For each grid cell, we calculated:

- **Number of occurrence records**: Raw sampling effort indicator
- **Species richness**: Number of unique species observed
- **Sampling completeness**: Estimated proportion of species actually present that have been detected, accounting for detection probability

These metrics allow us to distinguish between cells with low richness due to poor sampling (few records, low completeness) versus genuinely low biodiversity (many records, high completeness but few species).

### Spatial Autocorrelation

We tested for spatial autocorrelation using Moran's I [@moran1950], which quantifies whether nearby locations have similar sampling effort (positive autocorrelation), different sampling effort (negative autocorrelation), or independent sampling effort (no autocorrelation).

**Interpretation**: Moran's I ranges from -1 to +1:
- **I ≈ +1**: Strong positive autocorrelation (nearby cells have similar sampling effort) - indicates clustered sampling
- **I ≈ 0**: No spatial autocorrelation - indicates random or evenly distributed sampling
- **I ≈ -1**: Negative autocorrelation (nearby cells have dissimilar effort) - rare in observational data

Significant positive autocorrelation suggests non-random, clustered sampling patterns where researchers repeatedly visit the same regions while leaving other areas undersampled. This is the most common pattern in biodiversity data due to accessibility constraints and concentration of research activity near institutions.

### Environmental Bias

Beyond geographic space, we assessed whether sampling is representative of Kenya's environmental diversity. We compared the environmental conditions at sampled locations versus those available across all of Kenya using Kolmogorov-Smirnov (K-S) two-sample tests [@blonder2014].

**Environmental variables tested:**
- **Elevation**: SRTM 30-arc second digital elevation model (~1 km resolution)
- **Mean Annual Temperature**: WorldClim BIO1 (°C × 10)
- **Annual Precipitation**: WorldClim BIO12 (mm)

The K-S test compares cumulative distribution functions and returns a D-statistic (maximum difference between distributions) and p-value. Significant differences (p < 0.05) indicate environmental bias - certain environmental conditions are over- or under-sampled relative to their availability.

**Why this matters**: If sampling is biased toward certain environments (e.g., mid-elevation forests near cities), biodiversity patterns and species distribution models may not generalize well to under-sampled environments (e.g., arid lowlands or high-elevation areas).

## Temporal Bias Assessment

```{r load-temporal-data}
temporal_summary <- readRDS(file.path(data_outputs, "temporal_bias_summary.rds"))
temporal_stats <- temporal_summary$temporal_stats
trend_tests <- temporal_summary$trend_tests
gap_summary <- temporal_summary$gap_summary
```

Temporal bias refers to uneven distribution of sampling effort over time. This can confound analyses of biodiversity change, species trends, and phenology. We assessed temporal patterns using multiple approaches:

### Temporal Trends

We aggregated records by year and tested for temporal trends using the Mann-Kendall test [@mann1945; @kendall1975], a non-parametric test that:
- **Does not assume linear trends**: Detects monotonic (consistently increasing or decreasing) patterns
- **Robust to outliers**: Not sensitive to extreme values in specific years
- **Handles missing data**: Works with gaps in the time series
- **Provides tau statistic**: Ranges from -1 (perfect decreasing trend) to +1 (perfect increasing trend)

We applied the test separately to:
- **Number of records per year**: Overall sampling effort trend
- **Number of species per year**: Taxonomic coverage trend

**Interpretation**: Increasing trends are common in GBIF data due to:
- Digitization of historical museum collections (making old data accessible)
- Growth of citizen science platforms (iNaturalist, eBird)
- Improved data sharing infrastructure
- Increased research activity

Strong temporal trends can confound analyses of genuine biodiversity change (e.g., distinguishing increased sampling from genuine species range expansions).

### Temporal Completeness

We quantified temporal completeness as the proportion of years within the study period (1950-present) that contain at least one record. We also calculated:

- **Years with data**: Number of years containing records
- **Years without data**: Temporal gaps
- **Coefficient of variation (CV)**: Standard deviation / mean of annual record counts, measuring inter-annual variability

**Why this matters**:
- Temporal gaps prevent continuous time series analysis
- High CV indicates sporadic sampling that may miss important biodiversity events
- Uneven temporal coverage complicates trend detection and change attribution

### Seasonal Patterns

We analyzed the distribution of records across months to identify potential seasonal sampling biases. Many groups are preferentially sampled during specific seasons (e.g., birds during breeding season, insects during summer), which can affect phenological analyses and detectability estimates.

## Taxonomic Bias Assessment

```{r load-taxonomic-data}
taxonomic_summary <- readRDS(file.path(data_outputs, "taxonomic_bias_summary.rds"))
class_summary <- readRDS(file.path(data_outputs, "class_summary.rds"))
rarity_summary <- readRDS(file.path(data_outputs, "rarity_summary.rds"))
```

Taxonomic bias occurs when certain taxonomic groups are disproportionately sampled compared to their true diversity. This is one of the most pervasive biases in biodiversity data, driven by:
- **Human preferences**: Charismatic megafauna (birds, mammals) attract more attention than invertebrates
- **Economic importance**: Agricultural pests/beneficials, disease vectors, and commercial species are well-studied
- **Taxonomic expertise**: Groups with active expert communities and good identification resources are better sampled
- **Detection probability**: Large, conspicuous species are more likely to be observed and identified
- **Accessibility**: Some groups require specialized sampling equipment or techniques

We quantified taxonomic bias using multiple complementary metrics:

### Inequality Metrics

- **Gini Coefficient** [@gini1912]: A measure of inequality borrowed from economics, ranging from 0 (perfect equality - all taxa have equal records) to 1 (perfect inequality - one taxon has all records). Values > 0.6 indicate high inequality. The Gini coefficient summarizes the Lorenz curve, which plots cumulative proportion of records against cumulative proportion of taxa.

### Diversity Indices

- **Simpson's Diversity Index** [@simpson1949]: Probability that two randomly selected records belong to different taxa. Higher values indicate greater evenness.
- **Shannon's Diversity Index** [@shannon1948]: Entropy-based measure incorporating both richness (number of taxa) and evenness (distribution of records among taxa). Higher values indicate more diverse, even communities.

These indices help distinguish between high richness with uneven sampling (few taxa dominate records) versus balanced sampling across taxa.

### Rarity Analysis

We classified species by occurrence frequency:
- **Singletons**: Species with exactly 1 record (rarity class 1)
- **Doubletons**: Species with exactly 2 records (rarity class 2)
- **Very rare**: 3-5 records
- **Rare**: 6-10 records
- **Uncommon**: 11-50 records
- **Common**: 51+ records

High proportions of singleton/rare species may indicate:
- True rarity (genuinely rare species)
- Undersampling (species present but rarely detected)
- Identification errors (misidentifications creating spurious rare species)
- Taxonomic uncertainties (cryptic species, unresolved taxonomy)

### Species Accumulation Curves

We generated species accumulation curves showing cumulative species richness as a function of sampling effort (number of records). Curves approaching asymptotes suggest sampling is relatively complete, while steeply increasing curves indicate ongoing species discovery with additional effort.

## Statistical Modeling

```{r load-model-data}
modeling_summary <- readRDS(file.path(data_outputs, "modeling_summary.rds"))
model_summaries <- readRDS(file.path(data_outputs, "model_summaries.rds"))
```

To understand what environmental and geographic factors predict sampling effort, we used generalized linear models (GLMs). This allows us to move beyond description to explanation - identifying where and why sampling is concentrated.

### Model Structure

We fitted two complementary models:

**Count Model** (Negative Binomial GLM):
$$
\log(\lambda_i) = \beta_0 + \beta_1 \times \text{elevation}_i + \beta_2 \times \text{temperature}_i + \beta_3 \times \text{precipitation}_i + \\
\beta_4 \times \text{distance to city}_i + \beta_5 \times \text{distance from equator}_i + \beta_6 \times \text{distance from coast}_i
$$

where $\lambda_i$ is the expected number of records in grid cell $i$.

This model predicts how many records we expect in a location based on environmental and accessibility variables. The negative binomial distribution accounts for overdispersion (variance > mean), which is common in ecological count data.

**Presence Model** (Binomial GLM):
$$
\text{logit}(p_i) = \beta_0 + \beta_1 \times \text{elevation}_i + \ldots + \beta_6 \times \text{distance from coast}_i
$$

where $p_i$ is the probability that grid cell $i$ contains any records (presence/absence).

This model identifies factors determining whether a location is sampled at all, complementing the count model which focuses on intensity among sampled locations.

### Predictor Variables

- **Elevation**: Altitude in meters; affects accessibility and biodiversity
- **Temperature**: Mean annual temperature (°C); represents climatic gradient
- **Precipitation**: Annual precipitation (mm); represents moisture availability
- **Distance to nearest city**: Proxy for accessibility and human influence
- **Distance from equator**: Latitude proxy; Kenya spans equatorial to subtropical zones
- **Distance from coast**: East-west geographic gradient from Indian Ocean to interior

All continuous predictors were scaled (mean-centered and standardized to unit variance) to facilitate coefficient interpretation and comparison.

### Model Interpretation

Positive coefficients (β > 0) indicate increased sampling effort with increasing predictor values. Negative coefficients (β < 0) indicate decreased effort. Statistical significance (p < 0.05) indicates the relationship is unlikely due to chance. The model allows us to:

1. **Identify accessibility bias**: Expect strong negative coefficient for distance to cities
2. **Detect environmental preferences**: Which habitats/climates are preferred by researchers
3. **Map sampling bias**: Model residuals show over-sampled (positive) vs under-sampled (negative) areas relative to environmental/geographic expectations
4. **Guide future sampling**: Target under-sampled regions identified by model predictions

Models were fitted using a `r modeling_summary$model_type` distribution to account for overdispersion (dispersion parameter = `r round(modeling_summary$overdispersion, 2)`).

# Results

## Data Quality

### Overall Data Retention

```{r quality-overview}
# Calculate summary statistics using moderate filtering strategy
n_original <- quality_assessment$n_original
n_moderate_clean <- quality_assessment$clean_counts$moderate_clean
n_flagged <- n_original - n_moderate_clean
retention_rate <- 100 * n_moderate_clean / n_original
flag_rate <- 100 * n_flagged / n_original
```

Of the **`r format(n_original, big.mark=",")`** records originally downloaded from GBIF for Kenya, our quality flagging framework identified **`r format(n_flagged, big.mark=",")`** records (`r round(flag_rate, 1)`%) with one or more quality issues. Under a moderate filtering strategy (excluding records with missing coordinates, missing species, invalid dates, or any coordinate quality issues), **`r format(n_moderate_clean, big.mark=",")`** records (`r round(retention_rate, 1)`%) would be retained for analysis (Figure 2).

**Interpretation**: The `r round(flag_rate, 1)`% flag rate reflects the prevalence of quality issues in aggregated biodiversity data, comparable to filtering rates in other regional GBIF assessments (typically 40-95% depending on quality standards). Importantly, our flagging approach preserves all records while documenting quality issues, allowing different analyses to apply appropriate filters based on their specific requirements. For example, spatial bias assessments should retain records flagged as "near urban centers" since urban clustering is the bias being measured, while species distribution models might exclude such records. This flexible approach balances data quality with analytical needs and ensures transparency in quality control decisions.

```{r fig-quality-summary, fig.cap="Comparison of filtering strategies showing the number of records that would be retained under different quality filtering approaches (minimal, moderate, strict). This demonstrates the flexibility of the flagging framework to support different analytical requirements while preserving all records with documented quality flags."}
knitr::include_graphics(here("figures", "data_quality_filter_levels.png"))
```

### Quality Flag Summary

Table 2 presents the detailed breakdown of quality flags applied to the dataset. Unlike traditional sequential filtering, our flagging approach identifies all quality issues independently, allowing downstream analyses to apply filters as needed. The most prevalent data quality issues were:

```{r quality-flag-table}
# Create formatted quality tracking table
quality_table <- quality_tracking %>%
  filter(records_flagged > 0) %>%
  arrange(desc(records_flagged)) %>%
  mutate(
    `Quality Issue` = issue,
    `Description` = description,
    `Records Flagged` = format(records_flagged, big.mark = ","),
    `% of Original` = sprintf("%.2f%%", percent_flagged)
  ) %>%
  select(`Quality Issue`, `Description`, `Records Flagged`, `% of Original`)

kable(quality_table,
      caption = "Detailed breakdown of data quality flags. Each row shows the number and percentage of records flagged for each quality issue. Note: Flags are independent - a single record may have multiple flags.",
      booktabs = TRUE,
      align = c("l", "l", "r", "r")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = TRUE,
                font_size = 10) %>%
  column_spec(1, bold = TRUE, width = "5cm") %>%
  column_spec(2, width = "7cm")
```

Figure 3 visualizes the quality flag frequencies, showing the number of records flagged for each quality issue.

```{r fig-quality-flags, fig.cap="Data quality flag frequencies showing the number of records flagged for each quality issue. Bar height represents the number of records flagged, with color intensity indicating the percentage relative to the original dataset."}
knitr::include_graphics(here("figures", "data_quality_flags.png"))
```

### Coordinate Quality Issues

```{r coord-quality-summary}
# Get top coordinate issues
top_coord_issue <- coord_issues %>%
  filter(records_flagged > 0) %>%
  slice_max(records_flagged, n = 1)

total_coord_flagged <- quality_metrics$coordinate_quality$total_coord_flags
pct_coord_flagged <- quality_metrics$coordinate_quality$percent_coord_flags
```

The seven CoordinateCleaner tests identified a total of **`r format(total_coord_flagged, big.mark=",")`** records with potentially problematic coordinates, representing **`r round(pct_coord_flagged, 1)`%** of the original dataset. Table 3 presents the detailed breakdown of coordinate quality issues.

**Interpretation**: The `r round(pct_coord_flagged, 1)`% of records flagged for coordinate quality issues highlights a substantial problem with georeferencing accuracy in the dataset. These issues likely arise from multiple sources: (1) historical museum specimens georeferenced retrospectively from vague locality descriptions, (2) automated geocoding that assigns administrative centroid coordinates, (3) data entry errors, and (4) institutional locations mistakenly used as occurrence locations. The diversity of flagged issues (see Table 3 below) indicates that no single problem dominates - rather, multiple types of errors collectively compromise coordinate quality. Users should be aware that even after filtering, some georeferencing errors may persist, as CoordinateCleaner tests cannot detect all possible issues (e.g., coordinates that are slightly wrong but not statistical outliers).

```{r coord-issues-table}
# Create formatted coordinate issues table
coord_table <- coord_issues %>%
  filter(records_flagged > 0) %>%
  mutate(
    `Records Flagged` = format(records_flagged, big.mark = ","),
    `% of Original` = sprintf("%.2f%%", percent_of_original)
  ) %>%
  select(
    `Quality Test` = test,
    `Description` = description,
    `Records Flagged`,
    `% of Original`
  )

kable(coord_table,
      caption = "Breakdown of coordinate quality issues identified by CoordinateCleaner tests. Tests are ordered by number of records flagged.",
      booktabs = TRUE,
      align = c("l", "l", "r", "r")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = TRUE,
                font_size = 11) %>%
  column_spec(1, bold = TRUE, width = "3cm") %>%
  column_spec(2, width = "7cm")
```

The most common coordinate quality issue was **`r top_coord_issue$test`**, affecting **`r format(top_coord_issue$records_flagged, big.mark=",")`** records (`r round(top_coord_issue$percent_of_original, 2)`% of the original dataset). Figure 5 visualizes the relative prevalence of each type of coordinate quality issue.

```{r fig-coord-issues, fig.cap="Breakdown of coordinate quality issues detected by CoordinateCleaner. Bar length indicates the number of records flagged by each test, with color intensity representing the percentage of the original dataset."}
knitr::include_graphics(here("figures", "coordinate_issues_breakdown.png"))
```

### Taxonomic Completeness

```{r taxonomic-completeness}
tax_complete <- quality_metrics$taxonomic_completeness
```

Taxonomic identification completeness varied across hierarchical levels (Table 4). While nearly all records had kingdom-level classification (`r round(tax_complete$pct_n_with_kingdom, 1)`%), species-level identification was present in `r round(tax_complete$pct_n_with_species, 1)`% of records in the pre-filtered dataset.

```{r taxonomic-completeness-table}
# Create taxonomic completeness table
tax_levels <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")
tax_pcts <- c(
  tax_complete$pct_n_with_kingdom,
  tax_complete$pct_n_with_phylum,
  tax_complete$pct_n_with_class,
  tax_complete$pct_n_with_order,
  tax_complete$pct_n_with_family,
  tax_complete$pct_n_with_genus,
  tax_complete$pct_n_with_species
)

tax_complete_table <- data.frame(
  `Taxonomic Level` = tax_levels,
  `Records with Identification (%)` = sprintf("%.2f%%", tax_pcts),
  check.names = FALSE
)

kable(tax_complete_table,
      caption = "Taxonomic completeness at each hierarchical level in the original dataset (before species-level filtering).",
      booktabs = TRUE,
      align = c("l", "r")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = FALSE)
```

### Coordinate Uncertainty

```{r coord-uncertainty}
uncertainty <- quality_metrics$coordinate_quality$uncertainty_summary
```

Among records that included coordinate uncertainty information, the median uncertainty was **`r format(round(uncertainty$median_uncertainty), big.mark=",")`** meters (mean = `r format(round(uncertainty$mean_uncertainty), big.mark=",")` m). The distribution of coordinate uncertainty showed that `r round((uncertainty$n_gt_10km / uncertainty$n_with_uncertainty) * 100, 1)`% of records with uncertainty data exceeded the 10 km threshold and were flagged as high uncertainty.

### Duplicate Records

```{r duplicates}
dup_info <- quality_metrics$duplicate_info
```

The duplicate detection identified **`r format(dup_info$n_duplicate_records, big.mark=",")`** duplicate records organized into **`r format(dup_info$n_duplicate_sets, big.mark=",")`** unique sets of duplicates, representing **`r round(dup_info$percent_duplicates, 1)`%** of the original dataset. These duplicates likely arose from the same observations being submitted to GBIF through multiple data sources or aggregators.

### Basis of Record

```{r basis-of-record}
basis <- quality_metrics$basis_of_record %>%
  arrange(desc(n))
top_basis <- basis$basisOfRecord[1]
top_basis_pct <- basis$percent[1]
```

The most common basis of record was **`r top_basis`**, accounting for **`r round(top_basis_pct, 1)`%** of records. Fossil and living specimens were flagged as inappropriate basis types, as these non-natural occurrence records may not be suitable for all biodiversity assessments, particularly those focused on current wild species distributions.

## Spatial Bias

```{r spatial-bias-results}
effort_summary <- spatial_summary$effort_summary
```

### Sampling Effort Distribution

Sampling effort was highly heterogeneous across Kenya (Figure 1). Of the **`r format(effort_summary$total_cells, big.mark=",")`** grid cells covering Kenya, only **`r format(effort_summary$cells_with_records, big.mark=",")`** (`r round(effort_summary$percent_covered, 1)`%) contained occurrence records. The median number of records per cell was `r effort_summary$median_records` (mean = `r round(effort_summary$mean_records, 1)`, SD = `r round(effort_summary$sd_records, 1)`).

```{r fig-sampling-effort, fig.cap="Spatial distribution of sampling effort across Kenya. Color intensity represents log10-transformed number of occurrence records per hexagonal grid cell (~10 km resolution)."}
knitr::include_graphics(here("figures", "01_sampling_effort_map.png"))
```

### Spatial Autocorrelation

Sampling effort exhibited strong positive spatial autocorrelation (Moran's I = `r round(moran_results$morans_i, 3)`, p < 0.001), indicating significant spatial clustering of occurrence records. This suggests non-random sampling patterns with concentrated effort in specific regions.

```{r spatial-autocorrelation-table}
moran_table <- data.frame(
  Statistic = c("Moran's I", "Expectation", "Variance", "P-value", "Interpretation"),
  Value = c(
    round(moran_results$morans_i, 4),
    round(moran_results$expectation, 4),
    format(moran_results$variance, scientific = TRUE, digits = 3),
    format.pval(moran_results$p_value, eps = 0.001),
    moran_results$interpretation
  )
)

kable(moran_table, caption = "Spatial autocorrelation analysis of sampling effort",
      booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

### Environmental Bias

Sampled locations differed significantly from available environmental space for all tested variables (Table 3). This indicates systematic bias in which environments are sampled.

```{r environmental-bias-table}
env_bias_table <- env_bias %>%
  mutate(
    D_statistic = round(D_statistic, 4),
    p_value = format.pval(p_value, eps = 0.001),
    significant = ifelse(significant, "Yes", "No")
  ) %>%
  dplyr::select(Variable = variable, `D Statistic` = D_statistic,
         `P-value` = p_value, Significant = significant)

kable(env_bias_table,
      caption = "Kolmogorov-Smirnov tests for environmental bias (sampled vs available space)",
      booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

```{r fig-environmental-bias, fig.cap="Comparison of environmental space between sampled locations (red) and available space in Kenya (grey). Significant differences indicate environmental bias in sampling."}
knitr::include_graphics(here("figures", "03_environmental_bias.png"))
```

## Temporal Bias

### Temporal Trends

Data collection showed a significant increasing trend over time for both number of records (Mann-Kendall τ = `r round(trend_tests$tau[trend_tests$variable == "n_records"], 3)`, p < 0.001) and number of species (Mann-Kendall τ = `r round(trend_tests$tau[trend_tests$variable == "n_species"], 3)`, p < 0.001).

```{r temporal-trends-table}
trend_table <- trend_tests %>%
  mutate(
    tau = round(tau, 4),
    p_value = format.pval(p_value, eps = 0.001)
  ) %>%
  dplyr::select(Variable = variable, `Kendall's τ` = tau, `P-value` = p_value, Trend = trend)

kable(trend_table, caption = "Mann-Kendall trend tests for temporal patterns",
      booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

```{r fig-temporal-trends, fig.cap="Temporal trends in GBIF data for Kenya showing number of records (blue) and species (red, scaled ×10) from 1950 to present."}
knitr::include_graphics(here("figures", "05_temporal_trends.png"))
```

### Temporal Completeness

The temporal coverage spanned **`r temporal_stats$year_range`** years (`r temporal_stats$year_min`-`r temporal_stats$year_max`), with data available for **`r temporal_stats$total_years`** years. Temporal completeness was **`r round(gap_summary$temporal_completeness, 1)`%**, with **`r gap_summary$years_without_records`** years containing no records. The coefficient of variation for annual records was **`r round(temporal_stats$cv_records, 2)`**, indicating high inter-annual variability.

```{r temporal-summary-table}
temporal_sum_table <- data.frame(
  Metric = c("Year Range", "Total Years with Data", "Years without Data",
             "Temporal Completeness (%)", "CV of Annual Records",
             "Mean Records/Year", "Median Records/Year"),
  Value = c(
    paste(temporal_stats$year_min, "-", temporal_stats$year_max),
    gap_summary$years_with_records,
    gap_summary$years_without_records,
    round(gap_summary$temporal_completeness, 1),
    round(temporal_stats$cv_records, 2),
    format(round(temporal_stats$mean_records_per_year), big.mark = ","),
    format(round(temporal_stats$median_records_per_year), big.mark = ",")
  )
)

kable(temporal_sum_table, caption = "Temporal coverage summary",
      booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

## Taxonomic Bias

### Taxonomic Coverage

```{r taxonomic-overview}
tax_overview <- taxonomic_summary$overview
diversity_metrics <- taxonomic_summary$diversity_metrics
gini_records <- diversity_metrics$value[diversity_metrics$metric == "Gini_records"]
```

Taxonomic coverage was highly uneven across classes. The Gini coefficient for record distribution was **`r round(gini_records, 3)`**, indicating strong inequality in sampling effort among taxonomic groups.

```{r top-classes-table}
top_10_classes <- head(class_summary, 10) %>%
  dplyr::select(Kingdom = kingdom, Phylum = phylum, Class = class,
         Records = n_records, Species = n_species, Families = n_families) %>%
  mutate(
    Records = format(Records, big.mark = ","),
    Species = format(Species, big.mark = ","),
    Families = format(Families, big.mark = ",")
  )

kable(top_10_classes, caption = "Top 10 taxonomic classes by number of records",
      booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = FALSE, font_size = 11)
```

```{r fig-taxonomic-treemap, fig.cap="Treemap showing relative representation of top 15 taxonomic classes in GBIF Kenya data. Area represents number of records, color represents number of species."}
knitr::include_graphics(here("figures", "10_taxonomic_treemap.png"))
```

### Taxonomic Inequality

The Lorenz curve (Figure 5) illustrates the concentration of records among taxonomic classes. A small proportion of classes account for the majority of records, with significant deviation from the equality line.

```{r fig-lorenz, fig.cap="Lorenz curve showing taxonomic inequality in record distribution across classes. The dashed red line represents perfect equality; deviation indicates concentration of records in few classes."}
knitr::include_graphics(here("figures", "11_lorenz_curve.png"))
```

### Species Rarity

A substantial proportion of species were represented by very few records. **`r rarity_summary$n_species[rarity_summary$rarity_class == "Singleton (1 record)"]`** species (`r round(100*rarity_summary$prop_species[rarity_summary$rarity_class == "Singleton (1 record)"], 1)`%) were singletons (1 record), and **`r sum(rarity_summary$n_species[rarity_summary$rarity_class %in% c("Singleton (1 record)", "Doubleton (2 records)", "Very rare (3-5 records)")])`** species (`r round(100*sum(rarity_summary$prop_species[rarity_summary$rarity_class %in% c("Singleton (1 record)", "Doubleton (2 records)", "Very rare (3-5 records)")]), 1)`%) had ≤5 records.

```{r rarity-table}
rarity_table <- rarity_summary %>%
  mutate(
    n_species = format(n_species, big.mark = ","),
    total_records = format(total_records, big.mark = ","),
    prop_species = paste0(round(prop_species * 100, 1), "%"),
    prop_records = paste0(round(prop_records * 100, 1), "%")
  ) %>%
  dplyr::select(`Rarity Class` = rarity_class, `Species` = n_species,
         `Total Records` = total_records, `% Species` = prop_species,
         `% Records` = prop_records)

kable(rarity_table, caption = "Distribution of species by rarity class",
      booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

## Predictors of Sampling Effort

```{r model-results}
count_model_summary <- model_summaries %>%
  filter(model == "Count Model", term != "(Intercept)")
```

The `r modeling_summary$model_type` GLM revealed significant predictors of sampling effort (Table 7). Distance to cities was the strongest predictor, with sampling effort decreasing significantly with increasing distance from urban centers (β = `r round(count_model_summary$estimate[count_model_summary$term == "dist_to_city_scaled"], 3)`, p < 0.001).

```{r model-coefficients-table}
coef_table <- count_model_summary %>%
  mutate(
    term = str_remove(term, "_scaled"),
    term = str_replace_all(term, "_", " "),
    term = str_to_title(term)
  ) %>%
  dplyr::select(Predictor = term, Estimate = estimate, `Std. Error` = std.error,
         `Z value` = statistic, `P-value` = p.value) %>%
  mutate(
    Estimate = round(Estimate, 4),
    `Std. Error` = round(`Std. Error`, 4),
    `Z value` = round(`Z value`, 3),
    `P-value` = format.pval(`P-value`, eps = 0.001)
  )

kable(coef_table,
      caption = "Coefficients from negative binomial GLM predicting sampling effort",
      booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

```{r fig-coefficients, fig.cap="Coefficient plot showing predictors of sampling effort with 95% confidence intervals. Points right of zero indicate positive effects, left indicate negative effects."}
knitr::include_graphics(here("figures", "23_coefficient_plot.png"))
```

### Sampling Bias Map

Based on model predictions, we identified under-sampled and over-sampled regions (Figure 8). Under-sampled areas are characterized by greater distance from cities, intermediate elevations, and specific environmental conditions.

```{r fig-bias-map, fig.cap="Sampling bias map showing standardized residuals from the GLM. Blue areas are under-sampled relative to environmental/geographic predictors, red areas are over-sampled."}
knitr::include_graphics(here("figures", "22_sampling_bias_map.png"))
```

# Discussion

## Data Quality Issues

Our comprehensive quality flagging framework identified quality issues in `r round(flag_rate, 1)`% of the original GBIF records for Kenya. This substantial flag rate underscores the importance of rigorous quality control in biodiversity data workflows and highlights several key insights about our innovative flagging approach:

### Transparency in Data Quality Reporting

Most studies using GBIF data report only final cleaned dataset sizes, without quantifying the specific quality issues encountered or explaining which filters were applied. Our systematic flagging approach provides maximum transparency by documenting all quality issues while preserving all records, enabling downstream analyses to apply filters appropriate to their specific needs. This approach enables:

1. **Reproducibility:** Other researchers can apply comparable quality standards
2. **Fitness-for-use assessment:** Data users can evaluate whether quality standards meet their analytical needs
3. **Regional comparisons:** Quality metrics can be compared across countries and taxa
4. **Data improvement:** GBIF and data providers can identify systematic quality issues requiring attention

### Coordinate Quality as a Primary Issue

Coordinate-related issues represented a major source of data quality problems. Among records evaluated at the coordinate validation stage, `r format(quality_metrics$coordinate_quality$total_coord_flags, big.mark=",")` records (`r round(quality_metrics$coordinate_quality$percent_coord_flags, 1)`%) were flagged by CoordinateCleaner tests. The most prevalent coordinate issue was `r top_coord_issue$test`, affecting `r round(top_coord_issue$percent_of_original, 2)`% of original records. These findings align with previous studies [@zizka2019] showing that georeferencing errors are widespread in biodiversity databases.

Coordinate quality issues arise from multiple sources:

- **Historical collections:** Many museum specimens were georeferenced retrospectively from locality descriptions
- **Administrative centroids:** Records assigned to capital or province centroids rather than actual occurrence locations
- **GPS precision:** Early GPS units and mobile devices had limited accuracy
- **Data entry errors:** Manual transcription of coordinates introduces errors

### Duplicates and Data Aggregation

The identification of `r format(dup_info$n_duplicate_records, big.mark=",")` duplicate records (`r round(dup_info$percent_duplicates, 1)`%) reflects GBIF's role as a data aggregator. The same biological observation may be shared by multiple institutions or appear in multiple datasets, necessitating duplicate detection. However, distinguishing true duplicates from legitimate repeated observations at the same location requires careful consideration of dates, collectors, and metadata.

### Taxonomic Identification Completeness

While GBIF enforces kingdom-level classification, species-level identification varied considerably. Approximately `r round(100 - tax_complete$pct_n_with_species, 1)`% of records lacked species-level identification before filtering. This reflects:

- Identification difficulties in the field
- Specimens awaiting expert determination
- Taxonomic groups lacking identification keys
- Uncertainty in morphologically cryptic species

Requiring species-level identification, while appropriate for species distribution modeling, excludes potentially valuable data on genus- or family-level occurrences that could inform higher-level biodiversity patterns.

### Implications for Biodiversity Science

The quality issues documented here have important implications:

1. **Uncertainty quantification:** Error rates in coordinate precision and taxonomic identification propagate through downstream analyses
2. **Temporal trends in quality:** Data quality may vary over time, with older records potentially having lower coordinate precision but higher taxonomic expertise
3. **Taxonomic variation:** Quality issues may vary among taxa, affecting comparative analyses
4. **Fitness-for-use varies by application:** Appropriate quality standards depend on analytical goals (e.g., broad-scale macroecology vs. fine-scale conservation planning)

Our framework provides a template for transparent documentation of data quality that can be adopted across regions and taxa, facilitating standardization and comparability in biodiversity informatics.

## Spatial Bias

Our analysis revealed strong spatial biases in GBIF data for Kenya, consistent with global patterns [@beck2014; @meyer2016]. Sampling effort is concentrated near urban centers, roads, and accessible areas, leaving vast regions under-sampled. This accessibility bias is a well-documented phenomenon in biodiversity databases [@reddy2015] and has important implications:

1. **Conservation assessments** may overestimate species distributions in accessible areas
2. **Species distribution models** trained on biased data may produce unreliable predictions
3. **Biodiversity hotspot** identification may be confounded with sampling hotspots

The strong positive spatial autocorrelation (Moran's I = `r round(moran_results$morans_i, 3)`) indicates that nearby locations have similar sampling intensity, reflecting clustered sampling efforts rather than biological patterns.

## Temporal Bias

Temporal patterns show increasing data availability over time, particularly after 2000, coinciding with:

- Digital data mobilization efforts
- Increased citizen science participation (e.g., eBird, iNaturalist)
- Improved data sharing infrastructure

However, the high coefficient of variation (`r round(temporal_stats$cv_records, 2)`) and temporal gaps indicate inconsistent sampling effort across years. This temporal bias can:

- Confound trend analyses of biodiversity change
- Limit detection of phenological shifts
- Bias estimates of species rarity

## Taxonomic Bias

Taxonomic coverage is highly uneven, with a Gini coefficient of `r round(gini_records, 3)` indicating strong concentration of records in few classes. This reflects well-known biases favoring:

- **Charismatic megafauna** (birds, mammals)
- **Economically important groups** (agricultural pests/beneficials)
- **Groups with active hobbyist communities** (butterflies, birds)

The high proportion of singleton species (`r round(100*rarity_summary$prop_species[rarity_summary$rarity_class == "Singleton (1 record)"], 1)`%) may reflect:

- True rarity
- Identification errors
- Taxonomic uncertainties
- Sampling deficiencies

## Predictors of Sampling Effort

Distance to cities emerged as the strongest predictor of sampling effort, confirming accessibility bias. Environmental variables (elevation, temperature, precipitation) also influenced sampling, but to a lesser extent. The generalized additive model (GAM) revealed non-linear relationships, suggesting:

- Sampling peaks at intermediate elevations
- Urban-rural gradients drive sampling patterns
- Coastal areas receive differential attention

## Limitations

Our analysis has several limitations:

1. We assumed GBIF data represent the totality of available occurrence data for Kenya
2. Data quality issues (misidentifications, georeferencing errors) may persist despite cleaning
3. We lacked comprehensive reference data on Kenya's true biodiversity for all taxa
4. Temporal and spatial coverage may have improved since our download date

## Recommendations

Based on our findings, we recommend:

### For Data Users:

1. **Account for spatial bias** in species distribution models using appropriate methods [@fithian2015]
2. **Weight records** by sampling intensity when estimating diversity patterns
3. **Stratify analyses** by time period to account for temporal bias
4. **Consider taxonomic bias** when making cross-taxa comparisons

### For Data Collectors:

1. **Target under-sampled regions** identified in our bias maps (Figure 8)
2. **Prioritize systematic surveys** over opportunistic sampling
3. **Increase effort** for under-represented taxonomic groups
4. **Maintain consistent temporal coverage** to enable trend detection

### For Data Platforms:

1. **Provide bias metadata** with dataset downloads
2. **Develop tools** for bias-aware data filtering and analysis
3. **Incentivize** data collection in under-sampled areas
4. **Improve data quality** control and validation processes

# Conclusions

GBIF occurrence data for Kenya exhibit widespread quality issues and significant spatial, temporal, and taxonomic biases. Our comprehensive flagging framework identified quality issues in `r round(flag_rate, 1)`% of original records, with coordinate problems being the most prevalent. Critically, our flagging approach preserves all records while documenting issues, allowing different analyses to apply appropriate filters based on their specific requirements. Beyond quality issues, sampling effort is strongly concentrated near accessible areas, has increased over time but remains temporally inconsistent, and heavily favors certain taxonomic groups. Both data quality issues and systematic biases must be carefully considered in biodiversity assessments, species distribution modeling, and conservation planning.

This study makes three key contributions:

1. **Comprehensive Quality Quantification:** We provide detailed documentation of all data quality issues encountered during cleaning, quantifying the number and percentage of records affected by each issue. This transparent approach can be adopted as a standard for biodiversity data quality reporting.

2. **Integrated Quality and Bias Assessment:** By combining quality control with bias assessment, we provide a complete picture of data reliability and representativeness, enabling more informed decisions about data use.

3. **Reproducible Framework:** Our analytical pipeline, implemented in R using rgbif, CoordinateCleaner, and occAssess packages, can be readily applied to other countries and regions. We provide open-source code, data, and documentation to facilitate replication and extension of this work.

As GBIF continues to grow and evolve, regular quality and bias assessments are essential to:

- Track improvements in both data quality and coverage
- Guide strategic sampling efforts toward under-sampled regions and taxa
- Ensure robust biodiversity science through transparent data documentation
- Support evidence-based conservation with fitness-for-use metadata
- Enable cross-regional comparisons of data quality standards

We recommend that future biodiversity data studies adopt systematic quality tracking frameworks similar to ours, reporting not just final dataset sizes but the specific quality issues encountered and their prevalence. This will advance transparency, reproducibility, and standardization in biodiversity informatics.

# Data Availability

All data and code are openly available:

- **GBIF Data:** `r metadata$gbif_doi`
- **Analysis Code:** GitHub repository: [github.com/username/gbif-kenya-bias](https://github.com/username/gbif-kenya-bias)
- **Processed Data:** Zenodo DOI: [to be assigned upon publication]

# Acknowledgments

We thank the Global Biodiversity Information Facility (GBIF) and all data contributors for making biodiversity data openly accessible. We acknowledge the developers of the occAssess R package and other open-source tools used in this analysis. We are grateful to [funding agencies] for financial support.

# References

<div id="refs"></div>

# Session Information

```{r session-info}
sessionInfo()
```

# Appendix: Additional Figures

```{r fig-appendix-species-richness, fig.cap="Appendix Figure A1: Species richness across Kenya showing number of unique species per grid cell."}
knitr::include_graphics(here("figures", "02_species_richness_map.png"))
```

```{r fig-appendix-temporal-class, fig.cap="Appendix Figure A2: Temporal trends by taxonomic class showing changes in sampling effort over time for major groups."}
knitr::include_graphics(here("figures", "08_temporal_by_class.png"))
```

```{r fig-appendix-rarity, fig.cap="Appendix Figure A3: Distribution of species by rarity class showing high proportion of rare and singleton species."}
knitr::include_graphics(here("figures", "13_rarity_distribution.png"))
```
