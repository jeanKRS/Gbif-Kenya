---
title: "Comprehensive Data Quality Assessment and Bias Analysis of GBIF Biodiversity Data in Kenya"
author:
  - name: "Kwizera Jean"
    affiliation: "Kwiz Computing Technologies Limited"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    theme: flatly
    highlight: tango
    code_folding: hide
    fig_width: 10
    fig_height: 8
  pdf_document:
    toc: true
    number_sections: true
    fig_caption: yes
  word_document:
    toc: true
    fig_caption: yes
bibliography: references.bib
csl: ecology.csl
---

# Disclaimer {-}

<div style="background-color: #f0f0f0; padding: 15px; border-left: 5px solid #3498db; margin-bottom: 20px;">
**IMPORTANT NOTICE:** This document represents an independent analysis conducted to satisfy scientific curiosity and advance biodiversity data science methods. It has **not undergone formal peer review** and should not be cited as a peer-reviewed publication. While the methods employed follow established scientific standards and best practices in biodiversity informatics, the findings and interpretations presented here are preliminary and subject to revision. This work is shared openly to:

1. Demonstrate reproducible workflows for biodiversity data quality assessment
2. Provide a comprehensive baseline for Kenya's GBIF data quality and coverage
3. Facilitate discussion and collaboration on biodiversity data science methods
4. Support informed use of GBIF data by researchers and conservation practitioners

Users of this analysis should exercise appropriate scientific judgment and consider conducting additional validation for critical applications. Feedback, suggestions, and collaborative refinement of these methods are welcome.
</div>

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  cache = FALSE
)

# Load required packages
suppressPackageStartupMessages({
  library(tidyverse)
  library(sf)
  library(here)
  library(knitr)
  library(kableExtra)
  library(patchwork)
  library(viridis)
  library(scales)
  # Interactive visualization packages
  library(DT)           # Interactive tables
  library(plotly)       # Interactive charts
  library(leaflet)      # Interactive maps
  library(htmltools)    # HTML widgets support
})

# Source utility functions
source(here("R", "utils.R"))

# Set paths
data_processed <- here("data", "processed")
data_outputs <- here("data", "outputs")
figures_dir <- here("figures")

# Helper functions for interactive documentation
# Creates a "function flow" annotation showing which code generated a result
create_function_flow <- function(source_file, line_range = NULL, func_name = NULL,
                                  description = NULL) {
  lines <- if(!is.null(line_range)) paste0("lines ", line_range) else ""
  func <- if(!is.null(func_name)) paste0("<code>", func_name, "</code>") else ""

  html_content <- paste0(
    '<div style="background-color: #f8f9fa; border-left: 4px solid #28a745; padding: 10px; margin: 10px 0; font-size: 0.9em;">',
    '<strong>üìä Function Flow:</strong> ',
    if(!is.null(description)) paste0(description, '<br>') else '',
    '<strong>Source:</strong> <code>', source_file, '</code> ', lines, '<br>',
    if(!is.null(func_name)) paste0('<strong>Function:</strong> ', func, '<br>') else '',
    '<em>Educational note: This output was generated by the code above. You can trace back through the analysis pipeline to understand how raw data becomes final results.</em>',
    '</div>'
  )

  HTML(html_content)
}

# Creates a collapsible code block for showing source code
create_code_details <- function(code, title = "View source code", language = "r") {
  html_content <- paste0(
    '<details style="margin: 10px 0; border: 1px solid #dee2e6; border-radius: 5px; padding: 5px;">',
    '<summary style="cursor: pointer; padding: 10px; background-color: #e9ecef; font-weight: bold;">',
    'üîç ', title, ' (click to expand)',
    '</summary>',
    '<pre style="background-color: #f8f9fa; padding: 15px; margin: 10px; overflow-x: auto;"><code class="language-', language, '">',
    htmltools::htmlEscape(code),
    '</code></pre>',
    '</details>'
  )

  HTML(html_content)
}

# Creates an interactive table with DT
create_interactive_table <- function(data, caption = NULL, pageLength = 10,
                                      source_info = NULL) {
  dt <- datatable(
    data,
    caption = caption,
    options = list(
      pageLength = pageLength,
      dom = 'Bfrtip',
      buttons = c('copy', 'csv', 'excel'),
      scrollX = TRUE,
      searching = TRUE,
      ordering = TRUE
    ),
    extensions = 'Buttons',
    filter = 'top',
    rownames = FALSE,
    class = 'cell-border stripe hover'
  )

  if (!is.null(source_info)) {
    div(
      dt,
      create_function_flow(
        source_info$file,
        source_info$lines,
        source_info$func_name,
        source_info$description
      )
    )
  } else {
    dt
  }
}

# Load all data needed for abstract and inline R code
# Check if required data files exist
required_files <- c(
  file.path(data_outputs, "data_quality_assessment.rds"),
  file.path(data_processed, "kenya_gbif_flagged.rds"),
  file.path(data_outputs, "spatial_bias_summary.rds"),
  file.path(data_outputs, "temporal_bias_summary.rds"),
  file.path(data_outputs, "taxonomic_bias_summary.rds")
)

missing_files <- !sapply(required_files, file.exists)

if (any(missing_files)) {
  stop("\n\n========================================\n",
       "ERROR: Required data files are missing!\n",
       "========================================\n\n",
       "The following files do not exist:\n",
       paste("  -", required_files[missing_files], collapse = "\n"), "\n\n",
       "These files are created by running the analysis pipeline.\n\n",
       "To generate all required data files, run:\n\n",
       "  cd ", here(), "\n",
       "  Rscript run_all_analyses.R\n\n",
       "This will:\n",
       "  1. Download GBIF data for Kenya\n",
       "  2. Perform quality assessment\n",
       "  3. Conduct spatial, temporal, and taxonomic bias analyses\n",
       "  4. Fit statistical models\n",
       "  5. Generate all figures and tables\n",
       "  6. Render this R Markdown document\n\n",
       "========================================\n")
}

# Load data files
quality_assessment <- readRDS(file.path(data_outputs, "data_quality_assessment.rds"))
quality_tracking <- quality_assessment$tracking
coord_issues <- quality_assessment$coord_issues
quality_metrics <- quality_assessment$metrics

# Load flagged data to get actual filtering numbers
kenya_flagged <- readRDS(file.path(data_processed, "kenya_gbif_flagged.rds"))

spatial_summary <- readRDS(file.path(data_outputs, "spatial_bias_summary.rds"))
moran_results <- spatial_summary$moran_results

temporal_summary <- readRDS(file.path(data_outputs, "temporal_bias_summary.rds"))
temporal_stats <- temporal_summary$temporal_stats

taxonomic_summary <- readRDS(file.path(data_outputs, "taxonomic_bias_summary.rds"))
diversity_metrics <- taxonomic_summary$diversity_metrics
gini_records <- diversity_metrics$value[diversity_metrics$metric == "Gini_records"]
```

# List of Tables {-}

1. Summary statistics of GBIF data for Kenya
2. Detailed breakdown of data quality flags
3. Breakdown of coordinate quality issues identified by CoordinateCleaner
4. Taxonomic completeness at each hierarchical level
5. Sensitivity analysis comparing biodiversity metrics across filtering strategies
6. Spatial autocorrelation analysis of sampling effort
7. Kolmogorov-Smirnov tests for environmental bias
8. Mann-Kendall trend tests for temporal patterns
9. Temporal coverage summary
10. Top 10 taxonomic classes by number of records
11. Distribution of species by rarity class
12. Coefficients from negative binomial GLM predicting sampling effort

# List of Figures {-}

1. Spatial distribution of sampling effort across Kenya
2. Comparison of filtering strategies showing record retention
3. Data quality flag frequencies
4. Breakdown of coordinate quality issues
5. Comparison of environmental space (sampled vs. available)
6. Temporal trends in GBIF data for Kenya
7. Treemap of top taxonomic classes
8. Lorenz curve showing taxonomic inequality
9. Coefficient plot showing predictors of sampling effort
10. Sampling bias map showing model residuals

# Abstract

**Background:** The Global Biodiversity Information Facility (GBIF) represents the largest repository of species occurrence data globally. However, these data are known to suffer from both quality issues and systematic biases that can affect biodiversity assessments and conservation decisions. Understanding data quality and biases is critical for Kenya, a biodiversity hotspot with rich fauna and flora.

**Objectives:** We conducted a comprehensive assessment of data quality issues and systematic biases in GBIF occurrence data for Kenya, quantifying all quality filters applied during data cleaning and analyzing spatial, temporal, and taxonomic biases using reproducible R-based workflows.

**Methods:** We downloaded `r format(quality_assessment$n_original, big.mark=",")` occurrence records from GBIF for Kenya and implemented a systematic quality **flagging** framework that identified and documented seven major quality issues: missing coordinates, high coordinate uncertainty (>10 km), inappropriate basis of record, missing species identification, invalid dates, duplicates, and coordinate quality flags. Critically, our approach **flags** quality issues rather than automatically removing records, allowing each downstream analysis to apply filters appropriate to its specific requirements. We assessed spatial bias using grid-based analyses, environmental space coverage, and spatial autocorrelation. Temporal bias was evaluated through trend analyses and completeness metrics. Taxonomic bias was quantified using diversity indices and species accumulation curves. We used generalized linear models (GLMs) and generalized additive models (GAMs) to identify environmental and geographic predictors of sampling effort.

**Results:** Data quality assessment identified issues in `r format(sum(rowSums(kenya_flagged[,grepl("^flag_", names(kenya_flagged))]) > 0), big.mark=",")` records (`r round(100 * sum(rowSums(kenya_flagged[,grepl("^flag_", names(kenya_flagged))]) > 0) / nrow(kenya_flagged), 1)`%), with coordinate-related problems being most common: `r format(quality_metrics$coordinate_quality$total_coord_flags, big.mark=",")` records (`r round(quality_metrics$coordinate_quality$percent_coord_flags, 1)`%) flagged by CoordinateCleaner tests. Different analyses applied different quality filters based on their requirements: spatial bias analysis required valid coordinates and species identification but retained records near cities (the bias being measured); temporal analysis required valid dates but not coordinates; taxonomic analysis required only species identification. Under a moderate filtering strategy (coordinates, species, dates, no coordinate errors), `r format(quality_assessment$clean_counts$moderate_clean, big.mark=",")` records (`r round(100 * quality_assessment$clean_counts$moderate_clean / quality_assessment$n_original, 1)`%) were retained. Our bias analyses revealed significant spatial, temporal, and taxonomic biases. Sampling effort was concentrated near major cities and roads, with strong positive spatial autocorrelation (Moran's I = `r round(moran_results$morans_i, 3)`, p < 0.001). Temporal analysis showed increasing trends in data collection, with `r round(temporal_stats$cv_records, 2)` coefficient of variation in annual records. Taxonomic coverage was highly uneven, with a Gini coefficient of `r round(gini_records, 3)` for records across classes.

**Conclusions:** GBIF data for Kenya exhibit substantial quality issues and multiple forms of systematic bias that must be considered in biodiversity assessments. Our flagging approach allows different analyses to apply appropriate quality filters rather than using one-size-fits-all cleaning. We provide detailed quantification of all data quality flags, document which filters each analysis applies and why, provide recommendations for targeted sampling to address identified gaps, and present a reproducible framework applicable to other regions. This flexible, transparent approach to quality assessment and bias analysis provides a template for modern biodiversity data science.

**Keywords:** GBIF, biodiversity data, data quality, sampling bias, Kenya, spatial bias, temporal bias, taxonomic bias, occAssess, reproducible research

# Introduction

## The Growing Importance of Biodiversity Data

Biodiversity data from the Global Biodiversity Information Facility (GBIF) represent one of the largest openly accessible repositories of species occurrence records, with over 2 billion records globally [@gbif2023]. These data are increasingly used for critical applications including conservation planning, climate change research, species distribution modeling, biodiversity assessment, and policy decisions. However, the utility of these data for scientific research and conservation management depends fundamentally on both their quality and their representativeness [@meyer2016].

## The Dual Challenge: Quality and Bias

GBIF data face two distinct but interconnected challenges:

**Data Quality Issues** include technical errors and inconsistencies such as missing or erroneous coordinates, taxonomic misidentifications, temporal inconsistencies, and duplicate records [@zizka2019]. These issues arise from:
- Historical data digitization from museum specimens with incomplete metadata
- Georeferencing errors when converting locality descriptions to coordinates
- Taxonomic revisions that create outdated names in historical records
- Data entry mistakes during transcription
- Multiple institutions sharing the same observations, creating duplicates

**Systematic Biases** stem from non-random sampling efforts that create uneven coverage across space, time, and taxonomic groups [@beck2014; @hortal2015]. Unlike quality issues (which are errors), biases reflect real patterns in where, when, and what researchers choose to sample:
- **Spatial bias**: Sampling concentrated near roads, cities, and accessible areas
- **Temporal bias**: Increased sampling in recent decades due to digital technologies
- **Taxonomic bias**: Focus on charismatic species, economically important groups, or taxa with active hobbyist communities

Both quality issues and systematic biases can substantially affect downstream analyses if not properly addressed. Quality issues introduce noise and errors, while biases create misleading patterns that can be mistaken for biological signals. For example, high species richness near cities might reflect sampling effort rather than true biodiversity patterns.

## The Kenya Context

Kenya, as a biodiversity hotspot with rich fauna and flora across diverse ecosystems (coastal forests, savannas, highlands, and arid lands), has accumulated substantial GBIF records over decades of research and conservation work. However, both the data quality and the spatial, temporal, and taxonomic representativeness of these data remain poorly quantified. Understanding these aspects is critical for Kenya because:

1. The country hosts numerous endemic and threatened species requiring evidence-based conservation
2. Climate change impacts on East African biodiversity need robust baseline data
3. Conservation planning and protected area management depend on accurate species distribution information
4. National biodiversity monitoring and reporting obligations require reliable data sources

## The Documentation Gap

Most biodiversity studies using GBIF data apply quality filters but rarely document and quantify the specific issues encountered. Papers typically report only the final cleaned dataset size (e.g., "After quality filtering, 500,000 records remained"), without specifying:
- What specific quality issues were identified and in how many records
- What percentage of records had coordinate errors, missing taxonomy, or duplicates
- Which quality issues were filtered vs. flagged
- Whether different analyses used different filtering strategies
- Whether sampling biases were assessed and how they might affect conclusions

Furthermore, most studies apply a single, strict filtering strategy to all analyses, potentially:
- Removing the very biases they should be measuring (e.g., filtering urban records when studying spatial bias)
- Discarding records that would be valid for certain analyses (e.g., records without coordinates can still contribute to taxonomic assessments)
- Applying unnecessarily strict standards that reduce sample size without improving analysis quality

This lack of flexibility and transparency makes it difficult to:
- Assess whether quality standards were appropriate for each specific analysis
- Compare data quality across regions or taxonomic groups
- Understand fitness-for-use for different research questions
- Replicate filtering procedures in other studies
- Identify systemic issues that data providers could address

## Study Framework and Tools

Recent advances in biodiversity informatics have produced powerful tools for data quality assessment and bias detection. The occAssess R package [@marsh2023] provides a comprehensive framework for assessing biases in species occurrence data through spatial, temporal, and taxonomic analyses. The CoordinateCleaner package [@zizka2019] offers automated tools for identifying common coordinate quality issues such as institutional coordinates, country centroids, and statistical outliers.

By combining these tools with a flexible flagging approach that documents quality issues without automatically removing records, we can address the dual challenges of quality and bias in a systematic, reproducible way. Understanding both aspects is critical for:

1. **Assessing fitness-for-use**: Different research questions have different quality requirements. Fine-scale conservation planning needs higher coordinate precision than broad-scale macroecological studies.
2. **Informing conservation prioritization**: Biased data can lead to biased conservation decisions, potentially overlooking under-sampled regions with high conservation value.
3. **Improving species distribution models**: Models trained on biased data may produce unreliable predictions, especially when projected to under-sampled regions.
4. **Guiding future survey efforts**: Identifying sampling gaps helps target field work where it will maximize information gain.
5. **Ensuring robust biodiversity assessments**: National reporting on biodiversity trends requires understanding data limitations and potential biases.
6. **Providing transparency**: Full documentation of data processing enables replication, comparison across studies, and informed interpretation of results.

## Study Objectives

This study provides the first comprehensive, reproducible assessment of both data quality issues and systematic biases in biodiversity data for Kenya using standardized methods. Unlike previous studies that focus on either quality or bias, we integrate both perspectives to provide a complete picture of data reliability and representativeness. Critically, we employ a **flagging approach** that identifies quality issues without automatically removing records, allowing each analysis to apply appropriate filters.

Our specific objectives are to:

1. **Systematically flag all data quality issues**, reporting the number and percentage of records affected by each specific issue (missing coordinates, high uncertainty, missing taxonomy, temporal problems, duplicates, coordinate flags), while retaining all records for flexible downstream use

2. **Document filtering decisions for each analysis**, explaining which quality flags are relevant for spatial, temporal, and taxonomic bias assessments, and why certain flags should NOT be filtered (e.g., urban bias is a signal, not noise)

3. **Quantify spatial bias patterns** across Kenya using grid-based analyses, assess sampling completeness, and test for spatial autocorrelation to identify clustered vs. dispersed sampling

4. **Evaluate environmental bias** by comparing sampled locations to available environmental space, determining whether certain habitats, elevations, or climate zones are over- or under-represented

5. **Assess temporal patterns** in data collection, identify temporal gaps and trends, and quantify temporal completeness to understand how data availability varies over time

6. **Evaluate taxonomic bias** across major taxonomic groups using inequality metrics (Gini coefficients), diversity indices, and rarity analyses to identify which taxa are well-sampled vs. neglected

7. **Identify environmental and geographic predictors** of sampling effort using statistical models (GLMs and GAMs) to understand what drives sampling patterns (accessibility, urbanization, environmental conditions)

8. **Provide actionable recommendations** for targeted sampling to address identified gaps, enabling more strategic future data collection

9. **Present a reproducible flagging framework** for comprehensive data quality and bias assessment that can be readily applied to other countries, regions, or taxonomic groups, with clear documentation of which filters are appropriate for different research questions

By achieving these objectives, we provide both a thorough assessment of Kenya's biodiversity data and a methodological template for transparent, reproducible, and flexible data quality and bias assessment in biodiversity informatics.

# Methods

## Data Acquisition and Cleaning

```{r load-data}
# Load flagged data (with quality flags, not filtered)
kenya_flagged <- readRDS(file.path(data_processed, "kenya_gbif_flagged.rds"))

# Load metadata and summary statistics
metadata <- readRDS(file.path(data_processed, "metadata.rds"))
summary_stats <- readRDS(file.path(data_processed, "summary_stats.rds"))
```

### Data Download

We downloaded species occurrence data for Kenya from GBIF using the `rgbif` package [@chamberlain2023]. The download (DOI: `r metadata$gbif_doi`) was performed on `r metadata$download_date` using GBIF's API. We applied basic filters at the download stage to exclude obviously problematic records:

- **Geographic scope**: Records within Kenya's boundaries (identified by ISO country code KE)
- **Coordinate presence**: Only records with decimal latitude and longitude (excluding records with no georeferencing)
- **GBIF flags**: Excluded records flagged by GBIF as having geospatial issues at the time of download
- **Temporal scope**: Records from 1950 onwards to focus on modern biodiversity patterns and exclude historical records that may have substantial georeferencing uncertainty

These initial filters reduce download size and processing time while retaining the vast majority of usable records. However, additional quality filtering (documented below) is essential because GBIF's automated flags do not catch all quality issues.

### Quality Flagging Approach

Rather than automatically removing problematic records, we implemented a **flagging approach** that identifies quality issues while retaining all records. This approach recognizes that different analyses have different quality requirements and that some apparent "problems" (like urban bias) are actually the signals we want to measure.

Our flagging system adds boolean flag columns to each record indicating specific quality issues:

**Core Quality Flags:**
- `flag_missing_coords`: Missing decimal latitude or longitude
- `flag_high_uncertainty`: Coordinate uncertainty >10 km
- `flag_inappropriate_basis`: Fossil or living specimens
- `flag_missing_species`: No species-level identification
- `flag_invalid_date`: Date before 1950 or in future
- `flag_duplicate`: Exact duplicate record

**CoordinateCleaner Flags:**
- `flag_coord_capitals`: Within 10km of country/province capitals
- `flag_coord_centroids`: Within 5km of country/province centroids
- `flag_coord_urban`: In major urban areas
- `flag_coord_outliers`: Statistical outliers for the species
- `flag_coord_equal`: Identical latitude and longitude values
- `flag_coord_gbif`: At GBIF headquarters location
- `flag_coord_zeros`: At (0,0) or near equator/prime meridian
- `flag_any_coord_issue`: Any of the above coordinate issues

Each downstream analysis then applies filters appropriate to its specific requirements:

- **Spatial bias analysis**: Filters missing coordinates and species, obvious errors (equal/GBIF/zeros), but **retains** urban/capital records (the bias being measured)
- **Temporal bias analysis**: Filters invalid dates and missing species, but **does not require** coordinates
- **Taxonomic bias analysis**: Filters only missing species (coordinates and dates irrelevant)
- **Statistical models**: Applies strict filtering including outliers that would bias environmental relationships

This flexible approach provides several advantages:

1. **Preserves signal**: Biases we want to measure aren't filtered out
2. **Maximizes data use**: Records useful for some analyses aren't discarded
3. **Enables sensitivity analysis**: Different filter combinations can be tested
4. **Improves transparency**: Each analysis documents its filtering rationale
5. **Facilitates replication**: Other studies can apply appropriate filters

### Flagged Dataset Summary

The quality-flagged dataset contains **`r format(nrow(kenya_flagged), big.mark=",")`** occurrence records (all records from the original download). Under different filtering strategies, different numbers of records are retained:

- **Strict filtering** (all flags pass): `r format(quality_assessment$clean_counts$strict_clean, big.mark=",")` records (`r round(100 * quality_assessment$clean_counts$strict_clean / nrow(kenya_flagged), 1)`%)
- **Moderate filtering** (coordinates, species, dates, no coordinate errors): `r format(quality_assessment$clean_counts$moderate_clean, big.mark=",")` records (`r round(100 * quality_assessment$clean_counts$moderate_clean / nrow(kenya_flagged), 1)`%)
- **Minimal filtering** (coordinates and species only): `r format(quality_assessment$clean_counts$minimal_clean, big.mark=",")` records (`r round(100 * quality_assessment$clean_counts$minimal_clean / nrow(kenya_flagged), 1)`%)

The analyses in this manuscript primarily use moderate filtering, representing **`r format(summary_stats$n_species, big.mark=",")`** species from **`r format(summary_stats$n_families, big.mark=",")`** families across **`r format(summary_stats$n_classes, big.mark=",")`** classes.

```{r data-summary-table}
# Create summary table
summary_table <- data.frame(
  Metric = c("Analyzed Records (Moderate Filtering)", "Original Downloaded Records",
             "Number of Species", "Number of Genera",
             "Number of Families", "Number of Orders", "Number of Classes",
             "Year Range", "Median Coordinate Uncertainty (m)"),
  Value = c(
    format(summary_stats$n_records, big.mark = ","),
    format(nrow(kenya_flagged), big.mark = ","),
    format(summary_stats$n_species, big.mark = ","),
    format(summary_stats$n_genera, big.mark = ","),
    format(summary_stats$n_families, big.mark = ","),
    format(summary_stats$n_orders, big.mark = ","),
    format(summary_stats$n_classes, big.mark = ","),
    paste(summary_stats$year_min, "-", summary_stats$year_max),
    format(round(summary_stats$coord_uncertainty_median), big.mark = ",")
  )
)

# Display as interactive table with function flow annotation
create_interactive_table(
  summary_table,
  caption = "Table 1: Summary statistics of GBIF data for Kenya (Interactive). Click column headers to sort, use search box to filter.",
  pageLength = 15,
  source_info = list(
    file = "docs/kenya_gbif_bias_assessment.Rmd",
    lines = "407-432",
    func_name = "data.frame() + create_interactive_table()",
    description = "Summary statistics calculated from quality-filtered GBIF data"
  )
)

# Show the source code in a collapsible block
create_code_details("
### COMPLETE PIPELINE: GBIF Download ‚Üí Quality Filtering ‚Üí Summary Statistics ###

## Step 1: Download GBIF data for Kenya
## File: scripts/01_data_download.R, lines 40-60
kenya_download <- occ_download(
  pred('country', 'KE'),
  pred('hasCoordinate', TRUE),
  pred('hasGeospatialIssue', FALSE),
  format = 'SIMPLE_CSV'
)

## Step 2: Load and initial filtering
## File: scripts/01_data_download.R, lines 80-120
kenya_raw <- occ_download_get(kenya_download) %>%
  occ_download_import() %>%
  filter(
    !is.na(decimalLatitude),
    !is.na(decimalLongitude),
    !is.na(species)
  )

## Step 3: Apply CoordinateCleaner tests
## File: scripts/01_data_download.R, lines 150-180
kenya_clean <- clean_coordinates(
  kenya_raw,
  lon = 'decimalLongitude',
  lat = 'decimalLatitude',
  tests = c('capitals', 'centroids', 'equal', 'gbif', 'zeros', 'urban')
)

## Step 4: Calculate summary statistics
## File: scripts/01_data_download.R, lines 267-279
summary_stats <- kenya_final %>%
  summarise(
    n_records = n(),
    n_species = n_distinct(species),
    n_genera = n_distinct(genus),
    n_families = n_distinct(family),
    n_orders = n_distinct(order),
    n_classes = n_distinct(class),
    n_phyla = n_distinct(phylum),
    year_min = min(year, na.rm = TRUE),
    year_max = max(year, na.rm = TRUE),
    coord_uncertainty_median = median(coordinateUncertaintyInMeters, na.rm = TRUE)
  )

## Step 5: Quality flagging (separate script)
## File: scripts/01b_data_quality_assessment.R
kenya_flagged <- kenya_clean %>%
  mutate(
    flag_missing_coords = is.na(decimalLatitude) | is.na(decimalLongitude),
    flag_high_uncertainty = coordinateUncertaintyInMeters > 10000,
    flag_missing_species = is.na(species),
    flag_invalid_date = year < 1950 | year > year(Sys.Date()),
    # ... additional quality flags
  )

## Step 6: Create interactive summary table (in R Markdown)
## File: docs/kenya_gbif_bias_assessment.Rmd, lines 411-441
summary_table <- data.frame(
  Metric = c('Analyzed Records', 'Original Records', 'Species', 'Genera', ...),
  Value = c(summary_stats$n_records, nrow(kenya_flagged), ...)
)

create_interactive_table(summary_table, ...)

### FULL DATA FLOW SUMMARY ###
# GBIF API ‚Üí kenya_gbif_raw.rds (01_data_download.R)
#          ‚Üí kenya_gbif_clean.rds (after CoordinateCleaner)
#          ‚Üí summary_stats.rds (biodiversity metrics)
#          ‚Üí kenya_gbif_flagged.rds (01b_data_quality_assessment.R)
#          ‚Üí Interactive DT table (kenya_gbif_bias_assessment.Rmd)
",
  title = "üìã View complete code pipeline (GBIF download ‚Üí summary table)"
)
```

## Data Quality Assessment

```{r load-quality-data}
# Load comprehensive data quality assessment
quality_assessment <- readRDS(file.path(data_outputs, "data_quality_assessment.rds"))
quality_tracking <- quality_assessment$tracking
coord_issues <- quality_assessment$coord_issues
quality_metrics <- quality_assessment$metrics
```

```{r quality-assessment-pipeline, echo=FALSE, results='asis'}
# Show complete pipeline for quality assessment
create_code_details("
### COMPLETE PIPELINE: Raw GBIF Data ‚Üí Quality Flagging ‚Üí Assessment Object ###

## Step 1: Load raw GBIF data
## File: scripts/01b_data_quality_assessment.R, lines 46-56
kenya_raw <- readRDS(file.path(data_processed, 'kenya_gbif_raw.rds'))
n_original <- nrow(kenya_raw)

## Step 2: Initialize flagging framework
## File: scripts/01b_data_quality_assessment.R, lines 59-87
# Create empty tracking dataframe
quality_tracking <- tibble(
  issue = character(),
  description = character(),
  records_flagged = numeric(),
  percent_flagged = numeric()
)

# Helper function to track each flag
add_flag_tracking <- function(issue_name, description, flag_vector) {
  n_flagged <- sum(flag_vector, na.rm = TRUE)
  pct_flagged <- (n_flagged / n_original) * 100

  quality_tracking <<- bind_rows(quality_tracking, tibble(
    issue = issue_name,
    description = description,
    records_flagged = n_flagged,
    percent_flagged = pct_flagged
  ))

  return(flag_vector)
}

## Step 3: Add quality flags (one column per issue)
## File: scripts/01b_data_quality_assessment.R, lines 90-220

# Flag 1: Missing coordinates
kenya_flagged <- kenya_raw
kenya_flagged$flag_missing_coords <- add_flag_tracking(
  'Missing coordinates',
  'Records without decimal latitude or longitude',
  is.na(kenya_raw$decimalLongitude) | is.na(kenya_raw$decimalLatitude)
)

# Flag 2: High coordinate uncertainty (>10 km)
uncertainty_summary <- kenya_raw %>%
  filter(!is.na(coordinateUncertaintyInMeters)) %>%
  summarise(
    n_with_uncertainty = n(),
    median_uncertainty = median(coordinateUncertaintyInMeters),
    q75_uncertainty = quantile(coordinateUncertaintyInMeters, 0.75),
    n_gt_10km = sum(coordinateUncertaintyInMeters > 10000)
  )

kenya_flagged$flag_high_uncertainty <- add_flag_tracking(
  'High uncertainty',
  'Coordinate uncertainty > 10 km',
  !is.na(kenya_raw$coordinateUncertaintyInMeters) &
    kenya_raw$coordinateUncertaintyInMeters >= 10000
)

# Flag 3: Inappropriate basis of record
basis_summary <- kenya_raw %>%
  count(basisOfRecord, sort = TRUE) %>%
  mutate(percent = (n / n_original) * 100)

kenya_flagged$flag_inappropriate_basis <- add_flag_tracking(
  'Inappropriate basis',
  'Fossil or living specimens',
  kenya_raw$basisOfRecord %in% c('FOSSIL_SPECIMEN', 'LIVING_SPECIMEN')
)

# Flag 4: Missing species-level identification
tax_completeness <- kenya_raw %>%
  summarise(
    n_with_species = sum(!is.na(species) & species != ''),
    n_with_genus = sum(!is.na(genus) & genus != ''),
    pct_n_with_species = (n_with_species / n_original) * 100
  )

kenya_flagged$flag_missing_species <- add_flag_tracking(
  'Missing species ID',
  'Records without species-level identification',
  is.na(kenya_raw$species) | kenya_raw$species == ''
)

# Flag 5: Invalid dates (before 1950 or in future)
kenya_flagged <- kenya_flagged %>%
  mutate(
    eventDate = ymd(eventDate),
    year = year(eventDate)
  )

kenya_flagged$flag_invalid_date <- add_flag_tracking(
  'Invalid dates',
  'Dates before 1950 or in future',
  is.na(kenya_flagged$year) |
    kenya_flagged$year < 1950 |
    kenya_flagged$year > year(Sys.Date())
)

# Flag 6: Duplicate records
duplicates <- kenya_flagged %>%
  group_by(species, decimalLatitude, decimalLongitude, eventDate) %>%
  filter(n() > 1) %>%
  ungroup()

kenya_flagged$flag_duplicate <- add_flag_tracking(
  'Duplicates',
  'Exact duplicates (same species, location, date)',
  kenya_flagged$gbifID %in% duplicates$gbifID
)

# Flag 7: CoordinateCleaner tests (7 independent tests)
# - capitals, centroids, equal, gbif, zeros, urban, outliers
coord_flags <- cc_val(kenya_flagged, lon = 'decimalLongitude', lat = 'decimalLatitude')
kenya_flagged$flag_any_coord_issue <- !coord_flags

## Step 4: Calculate filtering strategies
## File: scripts/01b_data_quality_assessment.R, lines 350-380
clean_counts <- list(
  # Minimal: coordinates + species only
  minimal_clean = sum(
    !kenya_flagged$flag_missing_coords &
    !kenya_flagged$flag_missing_species
  ),

  # Moderate: + dates + no coordinate errors
  moderate_clean = sum(
    !kenya_flagged$flag_missing_coords &
    !kenya_flagged$flag_missing_species &
    !kenya_flagged$flag_invalid_date &
    !kenya_flagged$flag_any_coord_issue
  ),

  # Strict: all flags must pass
  strict_clean = sum(
    !kenya_flagged$flag_missing_coords &
    !kenya_flagged$flag_missing_species &
    !kenya_flagged$flag_invalid_date &
    !kenya_flagged$flag_any_coord_issue &
    !kenya_flagged$flag_high_uncertainty &
    !kenya_flagged$flag_duplicate &
    !kenya_flagged$flag_inappropriate_basis
  )
)

## Step 5: Compile quality assessment object
## File: scripts/01b_data_quality_assessment.R, lines 447-455
quality_results <- list(
  tracking = quality_tracking,             # Step-by-step flagging results
  coord_issues = coord_issues_df,          # CoordinateCleaner details
  metrics = quality_metrics,               # Uncertainty, completeness stats
  n_original = n_original,                 # Original record count
  clean_counts = clean_counts,             # Records under different filters
  assessment_date = Sys.Date(),            # When assessment was run
  flag_columns = names(kenya_flagged)[grepl('^flag_', names(kenya_flagged))]
)

## Step 6: Save to file
saveRDS(quality_results, 'data/outputs/data_quality_assessment.rds')
saveRDS(kenya_flagged, 'data/processed/kenya_gbif_flagged.rds')

### FULL DATA FLOW SUMMARY ###
# 1. Load raw GBIF data (kenya_gbif_raw.rds)
# 2. Count total records (n_original)
# 3. Add 14 boolean flag columns (one per quality issue)
# 4. Track how many records flagged at each step
# 5. Calculate retention under 3 filtering strategies
# 6. Compile comprehensive quality_results object
# 7. Save flagged dataset and quality assessment
# 8. Load in R Markdown for reporting

### KEY INSIGHT ###
# Records are FLAGGED but not REMOVED
# Downstream analyses choose which flags to filter based on their needs
# This preserves data for analyses where 'issues' are the signal (e.g., urban bias)
",
  title = "üìã View complete quality flagging pipeline (raw data ‚Üí flagged dataset)"
)
```

To comprehensively document data quality issues, we implemented a systematic **flagging framework** that identifies quality issues while retaining all records. Each issue type is flagged with a boolean column, and the number of records flagged at each step was recorded along with the percentage relative to the original dataset (`r format(quality_assessment$n_original, big.mark=",")` records).

### Quality Flagging Categories

The quality flagging process identified seven major categories of issues:

1. **Missing Coordinates:** `r sum(kenya_flagged$flag_missing_coords)` records (`r round(100*sum(kenya_flagged$flag_missing_coords)/nrow(kenya_flagged), 1)`%) lacking decimal latitude or longitude values
2. **High Coordinate Uncertainty:** `r sum(kenya_flagged$flag_high_uncertainty, na.rm=TRUE)` records (`r round(100*sum(kenya_flagged$flag_high_uncertainty, na.rm=TRUE)/nrow(kenya_flagged), 1)`%) with coordinate uncertainty exceeding 10 km
3. **Inappropriate Basis of Record:** `r sum(kenya_flagged$flag_inappropriate_basis)` records (`r round(100*sum(kenya_flagged$flag_inappropriate_basis)/nrow(kenya_flagged), 1)`%) representing fossil or living specimens
4. **Missing Species Identification:** `r sum(kenya_flagged$flag_missing_species)` records (`r round(100*sum(kenya_flagged$flag_missing_species)/nrow(kenya_flagged), 1)`%) without species-level taxonomic identification
5. **Invalid Dates:** `r sum(kenya_flagged$flag_invalid_date)` records (`r round(100*sum(kenya_flagged$flag_invalid_date)/nrow(kenya_flagged), 1)`%) with dates before 1950 or in the future
6. **Duplicate Records:** `r sum(kenya_flagged$flag_duplicate)` records (`r round(100*sum(kenya_flagged$flag_duplicate)/nrow(kenya_flagged), 1)`%) identified as exact duplicates
7. **Coordinate Quality Flags:** `r sum(kenya_flagged$flag_any_coord_issue)` records (`r round(100*sum(kenya_flagged$flag_any_coord_issue)/nrow(kenya_flagged), 1)`%) flagged by one or more CoordinateCleaner tests

**Key Insight:** `r format(sum(rowSums(kenya_flagged[,grepl("^flag_", names(kenya_flagged))]) > 0), big.mark=",")` total records (`r round(100 * sum(rowSums(kenya_flagged[,grepl("^flag_", names(kenya_flagged))]) > 0) / nrow(kenya_flagged), 1)`%) have at least one quality flag, but different analyses filter different flags based on their requirements.

### CoordinateCleaner Tests

The CoordinateCleaner package [@zizka2019] implements automated tests to identify common georeferencing errors in biodiversity data. These tests are based on patterns observed in large-scale data quality assessments and address specific, known sources of coordinate errors. We applied seven independent tests:

- **Capitals Test:** Identifies coordinates within 10 km of country or province capitals. Records may be assigned to capital cities when only administrative-level locality information is available (e.g., "Kenya" geocoded to Nairobi coordinates). Such records have high uncertainty and may not represent actual occurrence locations.

- **Centroids Test:** Flags coordinates within 5 km of country or province geographic centroids. Similar to capitals, these may result from automated geocoding that assigns centroid coordinates when precise locations are unavailable. These are particularly problematic as they appear valid but don't represent real sampling locations.

- **Equal Coordinates Test:** Detects records where latitude equals longitude (e.g., -1.234, -1.234). This pattern is physically possible but statistically unlikely and usually indicates data entry errors such as accidentally copying the same value to both coordinate fields.

- **GBIF Headquarters Test:** Identifies coordinates matching GBIF's headquarters in Copenhagen, Denmark. These obvious errors occur when default coordinates are not replaced during data entry or when specimen metadata is confused with institutional location.

- **Zeros Test:** Flags coordinates at or very near (0¬∞, 0¬∞) in the Atlantic Ocean. True occurrences at this location are rare for terrestrial/freshwater species; most represent null or missing coordinates that were incorrectly assigned default values of zero.

- **Urban Areas Test:** Identifies coordinates in major urban centers that may represent collection storage locations (museums, herbaria, universities) rather than where specimens were actually collected. This is particularly relevant for historical collections where only institutional location was recorded.

- **Outliers Test:** Uses statistical methods to detect coordinates that are geographically distant from other records of the same species. The test employs a quantile-based approach, flagging records in the outer 5% of distance distributions for species with sufficient records (‚â•10). Outliers may indicate misidentifications, transcription errors (e.g., coordinate decimal place errors), or valid range extensions requiring verification.

**Important Note:** Each test is independent, so a single record can be flagged by multiple tests. The `flag_any_coord_issue` column indicates records flagged by ANY test. **Critically, records are flagged but not automatically removed.** Each downstream analysis decides which flags to filter:

- **Spatial bias analysis** RETAINS capital/urban/centroid flags (these ARE the bias!)
- **Temporal bias analysis** IGNORES coordinate flags (not relevant to temporal patterns)
- **Statistical models** FILTERS outliers (would bias environmental relationships)
- **Taxonomic analysis** IGNORES all coordinate flags (taxonomy not location-dependent)

This flexible approach ensures we don't remove the very biases we're trying to measure while still identifying problematic coordinates for analyses that need spatial precision.

### Filtering Strategies and Data Retention

Since records are flagged rather than removed, we can assess data retention under different filtering strategies:

- **No filtering (all records):** `r format(nrow(kenya_flagged), big.mark=",")` records (100%)
- **Minimal filtering** (coordinates + species only): `r format(quality_assessment$clean_counts$minimal_clean, big.mark=",")` records (`r round(100 * quality_assessment$clean_counts$minimal_clean / nrow(kenya_flagged), 1)`%)
- **Moderate filtering** (+ dates + coordinate errors): `r format(quality_assessment$clean_counts$moderate_clean, big.mark=",")` records (`r round(100 * quality_assessment$clean_counts$moderate_clean / nrow(kenya_flagged), 1)`%)
- **Strict filtering** (all flags must pass): `r format(quality_assessment$clean_counts$strict_clean, big.mark=",")` records (`r round(100 * quality_assessment$clean_counts$strict_clean / nrow(kenya_flagged), 1)`%)

**The analyses in this manuscript primarily use moderate filtering**, which retains records suitable for spatial and temporal bias assessment while removing clear errors. Different research questions may justify different filtering strategies - this is the advantage of the flagging approach.

### Sensitivity Analysis Framework

To demonstrate the flexibility and robustness of our flagging approach, we implement a sensitivity analysis that compares key biodiversity metrics across different filtering strategies. This analysis serves multiple purposes:

1. **Demonstrates reproducibility**: Shows how results vary with different quality standards
2. **Informs decision-making**: Helps users choose appropriate filters for their research questions
3. **Quantifies uncertainty**: Reveals which metrics are robust vs. sensitive to filtering choices
4. **Validates approach**: Ensures that our primary conclusions are not artifacts of arbitrary filtering decisions

We compare three filtering strategies across multiple biodiversity metrics:

**Filtering Strategies:**
- **Minimal filtering**: Retains records with valid coordinates and species identification only (most permissive)
- **Moderate filtering**: Adds requirements for valid dates and no coordinate quality flags (balanced approach - used for primary analyses)
- **Strict filtering**: Requires all quality flags to pass, including uncertainty <10km and no duplicates (most conservative)

**Metrics compared:**
- Total record count and retention rate
- Species richness (total unique species)
- Taxonomic coverage (families, orders, classes)
- Spatial coverage (number of grid cells with records, geographic extent)
- Temporal coverage (year range, years with data)
- Taxonomic inequality (Gini coefficient)
- Spatial autocorrelation (Moran's I)
- Environmental bias (Kolmogorov-Smirnov D-statistics)

By comparing these metrics across filtering strategies, we demonstrate that our key findings (spatial clustering, environmental bias, taxonomic inequality) are robust to quality control decisions, while also identifying which aspects of the data are most affected by stringent filtering.

## Spatial Bias Assessment

```{r load-spatial-data}
spatial_grid <- readRDS(file.path(data_outputs, "spatial_grid_effort.rds"))
spatial_summary <- readRDS(file.path(data_outputs, "spatial_bias_summary.rds"))
moran_results <- spatial_summary$moran_results
env_bias <- spatial_summary$env_bias_summary
```

```{r complete-spatial-grid-pipeline, eval=FALSE, echo=FALSE, results='asis'}
create_code_details("
### COMPLETE PIPELINE: Spatial Grid Creation ‚Üí Sampling Effort Metrics ###

## File: scripts/02_spatial_bias.R
## Description: Creating hexagonal grid at ~10km resolution and calculating
##              sampling effort metrics per grid cell

## Step 1: Create hexagonal grid covering Kenya
## File: scripts/02_spatial_bias.R, lines 76-84
kenya_grid <- st_make_grid(
  kenya_boundary,
  cellsize = 0.1,  # ~10km at equator
  square = FALSE,   # hexagonal grid
  what = 'polygons'
) %>%
  st_sf() %>%
  mutate(grid_id = row_number()) %>%
  st_intersection(kenya_boundary)  # Clip to Kenya boundary

## Step 2: Count records per grid cell
## File: scripts/02_spatial_bias.R, lines 87-94
grid_effort <- kenya_grid %>%
  st_join(kenya_occurrences) %>%  # Spatial join: assign records to grid cells
  group_by(grid_id) %>%
  summarise(
    n_records = sum(!is.na(species)),  # Count non-NA species
    n_species = n_distinct(species, na.rm = TRUE),  # Count unique species
    .groups = 'drop'
  )

## Step 3: Add zero-record cells and calculate log-transformed effort
## File: scripts/02_spatial_bias.R, lines 97-103
kenya_grid_complete <- kenya_grid %>%
  left_join(st_drop_geometry(grid_effort), by = 'grid_id') %>%
  mutate(
    n_records = replace_na(n_records, 0),  # Grid cells with no records = 0
    n_species = replace_na(n_species, 0),
    log_records = log10(n_records + 1)  # Log transform for visualization
  )

## Step 4: Save spatial grid with effort metrics
saveRDS(kenya_grid_complete,
        file.path(data_outputs, 'spatial_grid_effort.rds'))

### Data Flow Summary:
# 1. Kenya boundary (polygon) ‚Üí st_make_grid() ‚Üí hexagonal grid
# 2. Grid cells + occurrence points ‚Üí st_join() ‚Üí records per cell
# 3. Grid with effort ‚Üí replace_na() ‚Üí complete grid (including zeros)
# Result: spatial_grid object with columns: grid_id, geometry, n_records,
#         n_species, log_records
",
title = "View Complete Spatial Grid Creation Code",
language = "r")
```

```{r complete-moran-pipeline, eval=FALSE, echo=FALSE, results='asis'}
create_code_details("
### COMPLETE PIPELINE: Moran's I Spatial Autocorrelation Test ###

## File: scripts/02_spatial_bias.R
## Description: Testing for spatial clustering in sampling effort using
##              Moran's I statistic

## Step 1: Get grid cell centroids
## File: scripts/02_spatial_bias.R, lines 139-140
coords_centroids <- st_centroid(kenya_grid_complete) %>%
  st_coordinates()  # Extract XY coordinates as matrix

## Step 2: Create distance-based neighbor network
## File: scripts/02_spatial_bias.R, lines 143-144
# Define neighbors as cells within 15km of each other
neighbors <- dnearneigh(coords_centroids, 0, 15000)  # 15km threshold
weights <- nb2listw(neighbors, style = 'W', zero.policy = TRUE)
# style='W': row-standardized weights (neighbors sum to 1)
# zero.policy=TRUE: allow cells with no neighbors

## Step 3: Calculate Moran's I for sampling effort
## File: scripts/02_spatial_bias.R, lines 147
moran_test <- moran.test(
  kenya_grid_complete$n_records,  # Variable to test
  weights,                         # Spatial weights matrix
  zero.policy = TRUE               # Handle isolated cells
)

## Step 4: Extract test results into tidy format
## File: scripts/02_spatial_bias.R, lines 149-156
moran_results <- data.frame(
  variable = 'sampling_effort',
  morans_i = moran_test$estimate['Moran I statistic'],
  expectation = moran_test$estimate['Expectation'],  # Expected I under null
  variance = moran_test$estimate['Variance'],
  p_value = moran_test$p.value,
  interpretation = ifelse(moran_test$statistic > 0, 'Clustered', 'Dispersed')
)

## Step 5: Save results
saveRDS(moran_results,
        file.path(data_outputs, 'spatial_autocorrelation.rds'))

### Interpretation:
# Moran's I ranges from -1 to +1:
#  - I > 0: Positive autocorrelation (clustering) - nearby cells similar
#  - I ‚âà 0: No autocorrelation - random spatial pattern
#  - I < 0: Negative autocorrelation (dispersion) - nearby cells dissimilar
#
# p-value tests null hypothesis of spatial randomness
# Significant positive I indicates clustered sampling (common in biodiversity data)
",
title = "View Complete Moran's I Calculation Code",
language = "r")
```

Spatial bias occurs when sampling effort is unevenly distributed across geographic space, typically concentrated in accessible areas near roads, cities, and research stations. We assessed spatial patterns using multiple complementary approaches:

### Grid-Based Analysis

We overlaid Kenya with a hexagonal grid at ~10 km resolution (approximately 100 km¬≤ per cell). Hexagonal grids are preferred over square grids because they:
- Have equal distances between centroids of neighboring cells
- Better approximate circular neighborhoods
- Reduce edge effects in spatial analyses
- Provide better tessellation for mapping

For each grid cell, we calculated:

- **Number of occurrence records**: Raw sampling effort indicator
- **Species richness**: Number of unique species observed
- **Sampling completeness**: Estimated proportion of species actually present that have been detected, accounting for detection probability

These metrics allow us to distinguish between cells with low richness due to poor sampling (few records, low completeness) versus genuinely low biodiversity (many records, high completeness but few species).

### Spatial Autocorrelation

We tested for spatial autocorrelation using Moran's I [@moran1950], which quantifies whether nearby locations have similar sampling effort (positive autocorrelation), different sampling effort (negative autocorrelation), or independent sampling effort (no autocorrelation).

**Interpretation**: Moran's I ranges from -1 to +1:
- **I ‚âà +1**: Strong positive autocorrelation (nearby cells have similar sampling effort) - indicates clustered sampling
- **I ‚âà 0**: No spatial autocorrelation - indicates random or evenly distributed sampling
- **I ‚âà -1**: Negative autocorrelation (nearby cells have dissimilar effort) - rare in observational data

Significant positive autocorrelation suggests non-random, clustered sampling patterns where researchers repeatedly visit the same regions while leaving other areas undersampled. This is the most common pattern in biodiversity data due to accessibility constraints and concentration of research activity near institutions.

### Environmental Bias

Beyond geographic space, we assessed whether sampling is representative of Kenya's environmental diversity. We compared the environmental conditions at sampled locations versus those available across all of Kenya using Kolmogorov-Smirnov (K-S) two-sample tests [@blonder2014].

**Environmental variables tested:**
- **Elevation**: SRTM 30-arc second digital elevation model (~1 km resolution)
- **Mean Annual Temperature**: WorldClim BIO1 (¬∞C √ó 10)
- **Annual Precipitation**: WorldClim BIO12 (mm)

The K-S test compares cumulative distribution functions and returns a D-statistic (maximum difference between distributions) and p-value. Significant differences (p < 0.05) indicate environmental bias - certain environmental conditions are over- or under-sampled relative to their availability.

**Why this matters**: If sampling is biased toward certain environments (e.g., mid-elevation forests near cities), biodiversity patterns and species distribution models may not generalize well to under-sampled environments (e.g., arid lowlands or high-elevation areas).

## Temporal Bias Assessment

```{r load-temporal-data}
temporal_summary <- readRDS(file.path(data_outputs, "temporal_bias_summary.rds"))
temporal_stats <- temporal_summary$temporal_stats
trend_tests <- temporal_summary$trend_tests
gap_summary <- temporal_summary$gap_summary
```

Temporal bias refers to uneven distribution of sampling effort over time. This can confound analyses of biodiversity change, species trends, and phenology. We assessed temporal patterns using multiple approaches:

### Temporal Trends

We aggregated records by year and tested for temporal trends using the Mann-Kendall test [@mann1945; @kendall1975], a non-parametric test that:
- **Does not assume linear trends**: Detects monotonic (consistently increasing or decreasing) patterns
- **Robust to outliers**: Not sensitive to extreme values in specific years
- **Handles missing data**: Works with gaps in the time series
- **Provides tau statistic**: Ranges from -1 (perfect decreasing trend) to +1 (perfect increasing trend)

We applied the test separately to:
- **Number of records per year**: Overall sampling effort trend
- **Number of species per year**: Taxonomic coverage trend

**Interpretation**: Increasing trends are common in GBIF data due to:
- Digitization of historical museum collections (making old data accessible)
- Growth of citizen science platforms (iNaturalist, eBird)
- Improved data sharing infrastructure
- Increased research activity

Strong temporal trends can confound analyses of genuine biodiversity change (e.g., distinguishing increased sampling from genuine species range expansions).

### Temporal Completeness

We quantified temporal completeness as the proportion of years within the study period (1950-present) that contain at least one record. We also calculated:

- **Years with data**: Number of years containing records
- **Years without data**: Temporal gaps
- **Coefficient of variation (CV)**: Standard deviation / mean of annual record counts, measuring inter-annual variability

**Why this matters**:
- Temporal gaps prevent continuous time series analysis
- High CV indicates sporadic sampling that may miss important biodiversity events
- Uneven temporal coverage complicates trend detection and change attribution

### Seasonal Patterns

We analyzed the distribution of records across months to identify potential seasonal sampling biases. Many groups are preferentially sampled during specific seasons (e.g., birds during breeding season, insects during summer), which can affect phenological analyses and detectability estimates.

## Taxonomic Bias Assessment

```{r load-taxonomic-data}
taxonomic_summary <- readRDS(file.path(data_outputs, "taxonomic_bias_summary.rds"))
class_summary <- readRDS(file.path(data_outputs, "class_summary.rds"))
rarity_summary <- readRDS(file.path(data_outputs, "rarity_summary.rds"))
```

```{r complete-class-summary-pipeline, eval=FALSE, echo=FALSE, results='asis'}
create_code_details("
### COMPLETE PIPELINE: Taxonomic Class Summary & Diversity Metrics ###

## File: scripts/04_taxonomic_bias.R
## Description: Aggregating records by taxonomic class and calculating
##              inequality and diversity metrics

## Step 1: Aggregate records by class
## File: scripts/04_taxonomic_bias.R, lines 82-92
class_summary <- kenya_data %>%
  filter(!is.na(class)) %>%  # Exclude records without class
  group_by(kingdom, phylum, class) %>%  # Group by hierarchical taxonomy
  summarise(
    n_records = n(),  # Total records for this class
    n_orders = n_distinct(order, na.rm = TRUE),
    n_families = n_distinct(family, na.rm = TRUE),
    n_species = n_distinct(species, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  arrange(desc(n_records))  # Sort by most recorded classes first

## Step 2: Save class-level summary
saveRDS(class_summary, file.path(data_outputs, 'class_summary.rds'))

## Step 3: Calculate Gini coefficient (inequality metric)
## File: scripts/04_taxonomic_bias.R, lines 129-130
gini_records <- ineq::Gini(class_summary$n_records)
gini_species <- ineq::Gini(class_summary$n_species)
# Gini = 0: perfect equality (all classes have equal records)
# Gini = 1: perfect inequality (one class has all records)
# Typical biodiversity data: Gini > 0.6 indicates high inequality

## Step 4: Calculate Simpson's diversity index
## File: scripts/04_taxonomic_bias.R, lines 133-134
simpson_class <- diversity(class_summary$n_records, index = 'simpson')
shannon_class <- diversity(class_summary$n_records, index = 'shannon')
# Simpson: probability two random records are different classes
# Shannon: entropy-based measure (higher = more diverse/even)

## Step 5: Calculate Pielou's evenness
## File: scripts/04_taxonomic_bias.R, lines 137
evenness_class <- shannon_class / log(nrow(class_summary))
# Evenness = 1: perfectly even distribution across classes
# Evenness < 0.5: highly uneven (few classes dominate)

## Step 6: Create diversity metrics summary
## File: scripts/04_taxonomic_bias.R, lines 139-153
diversity_metrics <- data.frame(
  metric = c('Gini_records', 'Gini_species', 'Simpson_diversity',
             'Shannon_diversity', 'Pielou_evenness'),
  value = c(gini_records, gini_species, simpson_class,
            shannon_class, evenness_class),
  interpretation = c(
    paste0('Records concentration (0=equal, 1=concentrated): ',
           round(gini_records, 3)),
    paste0('Species concentration: ', round(gini_species, 3)),
    paste0('Class diversity: ', round(simpson_class, 3)),
    paste0('Class diversity: ', round(shannon_class, 3)),
    paste0('Evenness (0=uneven, 1=even): ', round(evenness_class, 3))
  )
)

saveRDS(diversity_metrics,
        file.path(data_outputs, 'taxonomic_diversity_metrics.rds'))

### Data Flow Summary:
# 1. Raw records ‚Üí group_by(class) ‚Üí count records & species per class
# 2. Class counts ‚Üí ineq::Gini() ‚Üí inequality metric
# 3. Class counts ‚Üí vegan::diversity() ‚Üí Simpson/Shannon indices
# 4. Shannon / log(n_classes) ‚Üí Pielou's evenness
# Result: class_summary (records & species per class) + diversity_metrics
",
title = "View Complete Taxonomic Diversity Calculation Code",
language = "r")
```

```{r complete-rarity-summary-pipeline, eval=FALSE, echo=FALSE, results='asis'}
create_code_details("
### COMPLETE PIPELINE: Species Rarity Distribution Analysis ###

## File: scripts/04_taxonomic_bias.R
## Description: Classifying species by rarity (number of occurrence records)
##              and calculating rarity distribution

## Step 1: Count records per species
## File: scripts/04_taxonomic_bias.R, lines 223-235
species_freq <- kenya_data %>%
  count(species, name = 'n_records') %>%  # Count records per species
  mutate(
    rarity_class = case_when(
      n_records == 1 ~ 'Singleton (1 record)',
      n_records == 2 ~ 'Doubleton (2 records)',
      n_records <= 5 ~ 'Very rare (3-5 records)',
      n_records <= 10 ~ 'Rare (6-10 records)',
      n_records <= 50 ~ 'Uncommon (11-50 records)',
      n_records <= 100 ~ 'Common (51-100 records)',
      TRUE ~ 'Very common (>100 records)'
    )
  )

## Step 2: Summarize rarity distribution
## File: scripts/04_taxonomic_bias.R, lines 237-247
rarity_summary <- species_freq %>%
  group_by(rarity_class) %>%
  summarise(
    n_species = n(),  # Number of species in this rarity class
    total_records = sum(n_records),  # Total records from these species
    .groups = 'drop'
  ) %>%
  mutate(
    prop_species = n_species / sum(n_species),  # % of species
    prop_records = total_records / sum(total_records)  # % of records
  )

## Step 3: Save rarity summary
saveRDS(rarity_summary, file.path(data_outputs, 'rarity_summary.rds'))

### Interpretation:
# - High proportion of singletons/rare species may indicate:
#   1. True rarity (genuinely rare species)
#   2. Undersampling (species present but rarely detected)
#   3. Identification errors (misidentifications creating spurious rarities)
#   4. Taxonomic uncertainties (cryptic species, unresolved taxonomy)
#
# - Typical GBIF datasets have 30-50% singletons
# - Rarity classes help identify sampling completeness issues
# - Species with >100 records generally have reliable distribution data

### Data Flow Summary:
# 1. Raw records ‚Üí count(species) ‚Üí records per species
# 2. Records per species ‚Üí case_when() ‚Üí rarity class assignment
# 3. Rarity classes ‚Üí group_by() ‚Üí count species per class
# Result: rarity_summary showing distribution of species across rarity classes
",
title = "View Complete Rarity Distribution Calculation Code",
language = "r")
```

Taxonomic bias occurs when certain taxonomic groups are disproportionately sampled compared to their true diversity. This is one of the most pervasive biases in biodiversity data, driven by:
- **Human preferences**: Charismatic megafauna (birds, mammals) attract more attention than invertebrates
- **Economic importance**: Agricultural pests/beneficials, disease vectors, and commercial species are well-studied
- **Taxonomic expertise**: Groups with active expert communities and good identification resources are better sampled
- **Detection probability**: Large, conspicuous species are more likely to be observed and identified
- **Accessibility**: Some groups require specialized sampling equipment or techniques

We quantified taxonomic bias using multiple complementary metrics:

### Inequality Metrics

- **Gini Coefficient** [@gini1912]: A measure of inequality borrowed from economics, ranging from 0 (perfect equality - all taxa have equal records) to 1 (perfect inequality - one taxon has all records). Values > 0.6 indicate high inequality. The Gini coefficient summarizes the Lorenz curve, which plots cumulative proportion of records against cumulative proportion of taxa.

### Diversity Indices

- **Simpson's Diversity Index** [@simpson1949]: Probability that two randomly selected records belong to different taxa. Higher values indicate greater evenness.
- **Shannon's Diversity Index** [@shannon1948]: Entropy-based measure incorporating both richness (number of taxa) and evenness (distribution of records among taxa). Higher values indicate more diverse, even communities.

These indices help distinguish between high richness with uneven sampling (few taxa dominate records) versus balanced sampling across taxa.

### Rarity Analysis

We classified species by occurrence frequency:
- **Singletons**: Species with exactly 1 record (rarity class 1)
- **Doubletons**: Species with exactly 2 records (rarity class 2)
- **Very rare**: 3-5 records
- **Rare**: 6-10 records
- **Uncommon**: 11-50 records
- **Common**: 51+ records

High proportions of singleton/rare species may indicate:
- True rarity (genuinely rare species)
- Undersampling (species present but rarely detected)
- Identification errors (misidentifications creating spurious rare species)
- Taxonomic uncertainties (cryptic species, unresolved taxonomy)

### Species Accumulation Curves

We generated species accumulation curves showing cumulative species richness as a function of sampling effort (number of records). Curves approaching asymptotes suggest sampling is relatively complete, while steeply increasing curves indicate ongoing species discovery with additional effort.

## Statistical Modeling

```{r load-model-data}
modeling_summary <- readRDS(file.path(data_outputs, "modeling_summary.rds"))
model_summaries <- readRDS(file.path(data_outputs, "model_summaries.rds"))
```

```{r complete-statistical-models-pipeline, eval=FALSE, echo=FALSE, results='asis'}
create_code_details("
### COMPLETE PIPELINE: Statistical Models for Sampling Effort Predictors ###

## File: scripts/05_statistical_models.R
## Description: Fitting GLM and GAM models to identify environmental and
##              geographic predictors of sampling effort

## Step 1: Extract environmental covariates for grid cells
## File: scripts/05_statistical_models.R, lines 34-63
# Download environmental data
elevation_data <- elevation_30s(country = 'KEN', path = tempdir())
elevation <- if (is.character(elevation_data)) terra::rast(elevation_data)
            else elevation_data

climate_data <- worldclim_country(country = 'KEN', var = 'bio', path = tempdir())
climate <- if (is.character(climate_data)) terra::rast(climate_data)
           else climate_data

# Extract bio1 (temperature) and bio12 (precipitation)
temperature <- climate[[1]]
precipitation <- climate[[12]]

# Get grid centroids for extraction
grid_centroids <- st_centroid(spatial_grid)
grid_coords <- st_coordinates(grid_centroids)

# Extract environmental values
grid_env <- spatial_grid %>%
  mutate(
    elevation = terra::extract(elevation, grid_coords)[, 1],
    temperature = terra::extract(temperature, grid_coords)[, 1],
    precipitation = terra::extract(precipitation, grid_coords)[, 1],
    lon = grid_coords[, 1],
    lat = grid_coords[, 2]
  )

## Step 2: Calculate geographic covariates
## File: scripts/05_statistical_models.R, lines 66-103
# Major cities (Nairobi, Mombasa, Kisumu, Nakuru, Eldoret)
major_cities <- data.frame(
  city = c('Nairobi', 'Mombasa', 'Kisumu', 'Nakuru', 'Eldoret'),
  lon = c(36.8219, 39.6682, 34.7617, 36.0667, 35.2698),
  lat = c(-1.2921, -4.0435, -0.1022, -0.3031, 0.5143)
) %>%
  st_as_sf(coords = c('lon', 'lat'), crs = 4326)

# Calculate minimum distance to cities
dist_to_city <- st_distance(grid_centroids, major_cities)
min_dist_to_city <- apply(dist_to_city, 1, min) / 1000  # Convert to km

# Add to grid data and scale predictors
grid_model <- st_drop_geometry(grid_env) %>%
  mutate(
    dist_to_city_km = as.numeric(min_dist_to_city),
    dist_from_equator = abs(lat),
    dist_from_coast_km = abs(lon - 40.5) * 111,  # Rough km conversion
    # Scale environmental variables (mean-center, unit variance)
    elevation_scaled = scale(elevation)[, 1],
    temperature_scaled = scale(temperature)[, 1],
    precipitation_scaled = scale(precipitation)[, 1],
    dist_to_city_scaled = scale(dist_to_city_km)[, 1],
    # Create presence/absence for logistic model
    presence = as.numeric(n_records > 0)
  ) %>%
  # Remove NA/NaN/Inf values
  filter(!is.na(elevation), !is.na(temperature), !is.na(precipitation),
         !is.na(dist_to_city_km), is.finite(elevation_scaled),
         is.finite(temperature_scaled), is.finite(precipitation_scaled))

## Step 3: Fit Poisson GLM and check for overdispersion
## File: scripts/05_statistical_models.R, lines 143-154
glm_poisson <- glm(
  n_records ~ elevation_scaled + temperature_scaled + precipitation_scaled +
    dist_to_city_scaled + dist_from_equator + dist_from_coast_km,
  data = grid_model,
  family = poisson(link = 'log'),
  control = glm.control(epsilon = 1e-10, maxit = 200, trace = FALSE)
)

# Check for overdispersion
overdispersion <- sum(residuals(glm_poisson, type = 'pearson')^2) /
                  glm_poisson$df.residual
# overdispersion > 2 indicates need for negative binomial model

## Step 4: Fit Negative Binomial GLM (if overdispersed)
## File: scripts/05_statistical_models.R, lines 157-180
if (overdispersion > 2) {
  glm_nb <- glm.nb(
    n_records ~ elevation_scaled + temperature_scaled + precipitation_scaled +
      dist_to_city_scaled + dist_from_equator + dist_from_coast_km,
    data = grid_model,
    control = glm.control(epsilon = 1e-10, maxit = 200, trace = FALSE)
  )
  primary_model <- glm_nb
  model_type <- 'Negative Binomial'
} else {
  primary_model <- glm_poisson
  model_type <- 'Poisson'
}

## Step 5: Fit Binomial GLM for presence/absence
## File: scripts/05_statistical_models.R, lines 261-268
glm_binomial <- glm(
  presence ~ elevation_scaled + temperature_scaled + precipitation_scaled +
    dist_to_city_scaled + dist_from_equator + dist_from_coast_km,
  data = grid_model,
  family = binomial(link = 'logit'),
  control = glm.control(epsilon = 1e-10, maxit = 200, trace = FALSE)
)

## Step 6: Extract model summaries with confidence intervals
## File: scripts/05_statistical_models.R, lines 302-342
# Tidy model output
summary_count <- broom::tidy(primary_model) %>%
  mutate(
    conf.low = estimate - 1.96 * std.error,
    conf.high = estimate + 1.96 * std.error,
    model = 'Count Model',
    significant = p.value < 0.05,
    effect_direction = ifelse(estimate > 0, 'Positive', 'Negative')
  )

summary_presence <- broom::tidy(glm_binomial) %>%
  mutate(
    conf.low = estimate - 1.96 * std.error,
    conf.high = estimate + 1.96 * std.error,
    model = 'Presence Model',
    significant = p.value < 0.05,
    effect_direction = ifelse(estimate > 0, 'Positive', 'Negative')
  )

# Combine summaries
model_summaries <- bind_rows(summary_count, summary_presence)
saveRDS(model_summaries, file.path(data_outputs, 'model_summaries.rds'))

## Step 7: Save final models
saveRDS(primary_model, file.path(data_outputs, 'final_count_model.rds'))
saveRDS(glm_binomial, file.path(data_outputs, 'final_presence_model.rds'))

### Data Flow Summary:
# 1. Spatial grid ‚Üí extract environmental rasters ‚Üí grid_env
# 2. Grid + cities ‚Üí calculate distances ‚Üí grid_model with predictors
# 3. Predictors + n_records ‚Üí glm() ‚Üí Poisson/NB model
# 4. Predictors + presence ‚Üí glm(binomial) ‚Üí presence/absence model
# 5. Models ‚Üí broom::tidy() ‚Üí coefficient summaries with CIs
# Result: fitted GLMs showing which factors predict sampling effort

### Model Interpretation:
# - Negative coefficients: sampling decreases with increasing predictor
# - Positive coefficients: sampling increases with increasing predictor
# - Scaled predictors allow direct comparison of effect sizes
# - Distance to city typically has strongest (negative) effect
# - Environmental effects show habitat preferences of researchers
",
title = "View Complete Statistical Modeling Pipeline Code",
language = "r")
```

To understand what environmental and geographic factors predict sampling effort, we used generalized linear models (GLMs). This allows us to move beyond description to explanation - identifying where and why sampling is concentrated.

### Model Structure

We fitted two complementary models:

**Count Model** (Negative Binomial GLM):
$$
\log(\lambda_i) = \beta_0 + \beta_1 \times \text{elevation}_i + \beta_2 \times \text{temperature}_i + \beta_3 \times \text{precipitation}_i + \\
\beta_4 \times \text{distance to city}_i + \beta_5 \times \text{distance from equator}_i + \beta_6 \times \text{distance from coast}_i
$$

where $\lambda_i$ is the expected number of records in grid cell $i$.

This model predicts how many records we expect in a location based on environmental and accessibility variables. The negative binomial distribution accounts for overdispersion (variance > mean), which is common in ecological count data.

**Presence Model** (Binomial GLM):
$$
\text{logit}(p_i) = \beta_0 + \beta_1 \times \text{elevation}_i + \ldots + \beta_6 \times \text{distance from coast}_i
$$

where $p_i$ is the probability that grid cell $i$ contains any records (presence/absence).

This model identifies factors determining whether a location is sampled at all, complementing the count model which focuses on intensity among sampled locations.

### Predictor Variables

- **Elevation**: Altitude in meters; affects accessibility and biodiversity
- **Temperature**: Mean annual temperature (¬∞C); represents climatic gradient
- **Precipitation**: Annual precipitation (mm); represents moisture availability
- **Distance to nearest city**: Proxy for accessibility and human influence
- **Distance from equator**: Latitude proxy; Kenya spans equatorial to subtropical zones
- **Distance from coast**: East-west geographic gradient from Indian Ocean to interior

All continuous predictors were scaled (mean-centered and standardized to unit variance) to facilitate coefficient interpretation and comparison.

### Model Interpretation

Positive coefficients (Œ≤ > 0) indicate increased sampling effort with increasing predictor values. Negative coefficients (Œ≤ < 0) indicate decreased effort. Statistical significance (p < 0.05) indicates the relationship is unlikely due to chance. The model allows us to:

1. **Identify accessibility bias**: Expect strong negative coefficient for distance to cities
2. **Detect environmental preferences**: Which habitats/climates are preferred by researchers
3. **Map sampling bias**: Model residuals show over-sampled (positive) vs under-sampled (negative) areas relative to environmental/geographic expectations
4. **Guide future sampling**: Target under-sampled regions identified by model predictions

Models were fitted using a `r modeling_summary$model_type` distribution to account for overdispersion (dispersion parameter = `r round(modeling_summary$overdispersion, 2)`).

# Results

## Data Quality

### Overall Data Retention

```{r quality-overview}
# Calculate summary statistics using moderate filtering strategy
n_original <- quality_assessment$n_original
n_moderate_clean <- quality_assessment$clean_counts$moderate_clean
n_flagged <- n_original - n_moderate_clean
retention_rate <- 100 * n_moderate_clean / n_original
flag_rate <- 100 * n_flagged / n_original
```

Of the **`r format(n_original, big.mark=",")`** records originally downloaded from GBIF for Kenya, our quality flagging framework identified **`r format(n_flagged, big.mark=",")`** records (`r round(flag_rate, 1)`%) with one or more quality issues. Under a moderate filtering strategy (excluding records with missing coordinates, missing species, invalid dates, or any coordinate quality issues), **`r format(n_moderate_clean, big.mark=",")`** records (`r round(retention_rate, 1)`%) would be retained for analysis (Figure 2).

**Interpretation**: The `r round(flag_rate, 1)`% flag rate reflects the prevalence of quality issues in aggregated biodiversity data, comparable to filtering rates in other regional GBIF assessments (typically 40-95% depending on quality standards). Importantly, our flagging approach preserves all records while documenting quality issues, allowing different analyses to apply appropriate filters based on their specific requirements. For example, spatial bias assessments should retain records flagged as "near urban centers" since urban clustering is the bias being measured, while species distribution models might exclude such records. This flexible approach balances data quality with analytical needs and ensures transparency in quality control decisions.

```{r fig-quality-summary, fig.cap="Comparison of filtering strategies showing the number of records that would be retained under different quality filtering approaches (minimal, moderate, strict). This demonstrates the flexibility of the flagging framework to support different analytical requirements while preserving all records with documented quality flags."}
knitr::include_graphics(here("figures", "data_quality_filter_levels.png"))
```

### Quality Flag Summary

Table 2 presents the detailed breakdown of quality flags applied to the dataset. Unlike traditional sequential filtering, our flagging approach identifies all quality issues independently, allowing downstream analyses to apply filters as needed. The most prevalent data quality issues were:

```{r quality-flag-table}
# Create formatted quality tracking table
quality_table <- quality_tracking %>%
  filter(records_flagged > 0) %>%
  arrange(desc(records_flagged)) %>%
  mutate(
    `Quality Issue` = issue,
    `Description` = description,
    `Records Flagged` = format(records_flagged, big.mark = ","),
    `% of Original` = sprintf("%.2f%%", percent_flagged)
  ) %>%
  select(`Quality Issue`, `Description`, `Records Flagged`, `% of Original`)

kable(quality_table,
      caption = "Table 2: Detailed breakdown of data quality flags. Each row shows the number and percentage of records flagged for each quality issue. Note: Flags are independent - a single record may have multiple flags.",
      booktabs = TRUE,
      align = c("l", "l", "r", "r")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = TRUE,
                font_size = 10) %>%
  column_spec(1, bold = TRUE, width = "5cm") %>%
  column_spec(2, width = "7cm")
```

Figure 3 visualizes the quality flag frequencies, showing the number of records flagged for each quality issue.

```{r fig-quality-flags, fig.cap="Data quality flag frequencies showing the number of records flagged for each quality issue. Bar height represents the number of records flagged, with color intensity indicating the percentage relative to the original dataset."}
knitr::include_graphics(here("figures", "data_quality_flags.png"))
```

### Coordinate Quality Issues

```{r coord-quality-summary}
# Get top coordinate issues
top_coord_issue <- coord_issues %>%
  filter(records_flagged > 0) %>%
  slice_max(records_flagged, n = 1)

total_coord_flagged <- quality_metrics$coordinate_quality$total_coord_flags
pct_coord_flagged <- quality_metrics$coordinate_quality$percent_coord_flags
```

The seven CoordinateCleaner tests identified a total of **`r format(total_coord_flagged, big.mark=",")`** records with potentially problematic coordinates, representing **`r round(pct_coord_flagged, 1)`%** of the original dataset. Table 3 presents the detailed breakdown of coordinate quality issues.

**Interpretation**: The `r round(pct_coord_flagged, 1)`% of records flagged for coordinate quality issues highlights a substantial problem with georeferencing accuracy in the dataset. These issues likely arise from multiple sources: (1) historical museum specimens georeferenced retrospectively from vague locality descriptions, (2) automated geocoding that assigns administrative centroid coordinates, (3) data entry errors, and (4) institutional locations mistakenly used as occurrence locations. The diversity of flagged issues (see Table 3 below) indicates that no single problem dominates - rather, multiple types of errors collectively compromise coordinate quality. Users should be aware that even after filtering, some georeferencing errors may persist, as CoordinateCleaner tests cannot detect all possible issues (e.g., coordinates that are slightly wrong but not statistical outliers).

```{r coord-issues-table}
# Create formatted coordinate issues table
coord_table <- coord_issues %>%
  filter(records_flagged > 0) %>%
  mutate(
    `Records Flagged` = format(records_flagged, big.mark = ","),
    `% of Original` = sprintf("%.2f%%", percent_of_original)
  ) %>%
  select(
    `Quality Test` = test,
    `Description` = description,
    `Records Flagged`,
    `% of Original`
  )

kable(coord_table,
      caption = "Table 3: Breakdown of coordinate quality issues identified by CoordinateCleaner tests. Tests are ordered by number of records flagged.",
      booktabs = TRUE,
      align = c("l", "l", "r", "r")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = TRUE,
                font_size = 11) %>%
  column_spec(1, bold = TRUE, width = "3cm") %>%
  column_spec(2, width = "7cm")
```

The most common coordinate quality issue was **`r top_coord_issue$test`**, affecting **`r format(top_coord_issue$records_flagged, big.mark=",")`** records (`r round(top_coord_issue$percent_of_original, 2)`% of the original dataset). Figure 5 visualizes the relative prevalence of each type of coordinate quality issue.

```{r fig-coord-issues, fig.cap="Breakdown of coordinate quality issues detected by CoordinateCleaner. Bar length indicates the number of records flagged by each test, with color intensity representing the percentage of the original dataset."}
knitr::include_graphics(here("figures", "coordinate_issues_breakdown.png"))
```

### Taxonomic Completeness

```{r taxonomic-completeness}
tax_complete <- quality_metrics$taxonomic_completeness
```

Taxonomic identification completeness varied across hierarchical levels (Table 4). While nearly all records had kingdom-level classification (`r round(tax_complete$pct_n_with_kingdom, 1)`%), species-level identification was present in `r round(tax_complete$pct_n_with_species, 1)`% of records in the original dataset.

```{r taxonomic-completeness-table}
# Create taxonomic completeness table
tax_levels <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")
tax_pcts <- c(
  tax_complete$pct_n_with_kingdom,
  tax_complete$pct_n_with_phylum,
  tax_complete$pct_n_with_class,
  tax_complete$pct_n_with_order,
  tax_complete$pct_n_with_family,
  tax_complete$pct_n_with_genus,
  tax_complete$pct_n_with_species
)

tax_complete_table <- data.frame(
  `Taxonomic Level` = tax_levels,
  `Records with Identification (%)` = sprintf("%.2f%%", tax_pcts),
  check.names = FALSE
)

kable(tax_complete_table,
      caption = "Table 4: Taxonomic completeness at each hierarchical level in the original dataset (before species-level filtering).",
      booktabs = TRUE,
      align = c("l", "r")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = FALSE)
```

### Coordinate Uncertainty

```{r coord-uncertainty}
uncertainty <- quality_metrics$coordinate_quality$uncertainty_summary
```

Among records that included coordinate uncertainty information, the median uncertainty was **`r format(round(uncertainty$median_uncertainty), big.mark=",")`** meters (mean = `r format(round(uncertainty$mean_uncertainty), big.mark=",")` m). The distribution of coordinate uncertainty showed that `r round((uncertainty$n_gt_10km / uncertainty$n_with_uncertainty) * 100, 1)`% of records with uncertainty data exceeded the 10 km threshold and were flagged as high uncertainty.

### Duplicate Records

```{r duplicates}
dup_info <- quality_metrics$duplicate_info
pct_duplicates <- 100 * dup_info$n_duplicate_records / n_original
```

The duplicate detection identified **`r format(dup_info$n_duplicate_records, big.mark=",")`** duplicate records organized into **`r format(dup_info$n_duplicate_sets, big.mark=",")`** unique sets of duplicates, representing **`r round(pct_duplicates, 1)`%** of the original dataset. These duplicates likely arose from the same observations being submitted to GBIF through multiple data sources or aggregators.

### Basis of Record

```{r basis-of-record}
basis <- quality_metrics$basis_of_record %>%
  arrange(desc(n))
top_basis <- basis$basisOfRecord[1]
top_basis_pct <- basis$percent[1]
```

The most common basis of record was **`r top_basis`**, accounting for **`r round(top_basis_pct, 1)`%** of records. Fossil and living specimens were flagged as inappropriate basis types, as these non-natural occurrence records may not be suitable for all biodiversity assessments, particularly those focused on current wild species distributions.

## Sensitivity Analysis: Impact of Filtering Strategies

```{r sensitivity-analysis-data}
# Define three filtering strategies and calculate metrics for each
# Minimal: coordinates + species only
minimal_data <- kenya_flagged %>%
  filter(!flag_missing_coords, !flag_missing_species)

# Moderate: + dates + no coordinate errors (PRIMARY ANALYSIS)
moderate_data <- kenya_flagged %>%
  filter(!flag_missing_coords, !flag_missing_species,
         !flag_invalid_date, !flag_any_coord_issue)

# Strict: all flags must pass
strict_data <- kenya_flagged %>%
  filter(!flag_missing_coords, !flag_missing_species,
         !flag_invalid_date, !flag_any_coord_issue,
         !flag_high_uncertainty, !flag_duplicate,
         !flag_inappropriate_basis)

# Calculate metrics for each strategy
sensitivity_metrics <- data.frame(
  Strategy = c("Minimal", "Moderate (Primary)", "Strict"),
  Records = c(nrow(minimal_data), nrow(moderate_data), nrow(strict_data)),
  Retention_Pct = c(
    100 * nrow(minimal_data) / nrow(kenya_flagged),
    100 * nrow(moderate_data) / nrow(kenya_flagged),
    100 * nrow(strict_data) / nrow(kenya_flagged)
  ),
  Species = c(
    n_distinct(minimal_data$species),
    n_distinct(moderate_data$species),
    n_distinct(strict_data$species)
  ),
  Families = c(
    n_distinct(minimal_data$family),
    n_distinct(moderate_data$family),
    n_distinct(strict_data$family)
  ),
  Classes = c(
    n_distinct(minimal_data$class),
    n_distinct(moderate_data$class),
    n_distinct(strict_data$class)
  )
)
```

To demonstrate the robustness and flexibility of our flagging approach, we compared key biodiversity metrics across three filtering strategies (Table X). This sensitivity analysis reveals which metrics are robust to quality control decisions and which are sensitive to filtering stringency.

```{r sensitivity-comparison-table}
# Format the sensitivity analysis table (keep numeric for interactive sorting)
sensitivity_table_interactive <- sensitivity_metrics %>%
  mutate(
    `Retention (%)` = round(Retention_Pct, 1)
  ) %>%
  select(Strategy, Records, `Retention (%)`, Species, Families, Classes)

# Create interactive table with custom formatting
datatable(
  sensitivity_table_interactive,
  caption = "Table 5: Sensitivity Analysis - Compare filtering strategies interactively. Click headers to sort, search to filter.",
  options = list(
    pageLength = 5,
    dom = 't',  # Simple table (no pagination needed for 3 rows)
    searching = FALSE,
    ordering = TRUE,
    columnDefs = list(
      list(className = 'dt-center', targets = 1:5)
    )
  ),
  rownames = FALSE,
  class = 'cell-border stripe hover'
) %>%
  formatStyle(
    'Strategy',
    target = 'row',
    backgroundColor = styleEqual('Moderate (Primary)', '#E8F4F8'),
    fontWeight = styleEqual('Moderate (Primary)', 'bold')
  ) %>%
  formatCurrency('Records', currency = '', digits = 0, mark = ',') %>%
  formatCurrency('Species', currency = '', digits = 0, mark = ',') %>%
  formatCurrency('Families', currency = '', digits = 0, mark = ',') %>%
  formatCurrency('Classes', currency = '', digits = 0, mark = ',')

# Add function flow annotation
create_function_flow(
  source_file = "docs/kenya_gbif_bias_assessment.Rmd",
  line_range = "920-964",
  func_name = "filter() + n_distinct()",
  description = "Applies three filtering strategies to flagged data and calculates biodiversity metrics for each"
)
```

### Key Findings from Sensitivity Analysis

1. **Record Retention**: Filtering stringency substantially affects sample size, with retention rates ranging from `r sprintf("%.1f%%", sensitivity_metrics$Retention_Pct[1])` (minimal) to `r sprintf("%.1f%%", sensitivity_metrics$Retention_Pct[3])` (strict). The moderate strategy retains `r sprintf("%.1f%%", sensitivity_metrics$Retention_Pct[2])` of records.

2. **Taxonomic Richness**: Species richness decreases with filtering stringency, from `r format(sensitivity_metrics$Species[1], big.mark=",")` species (minimal) to `r format(sensitivity_metrics$Species[3], big.mark=",")` species (strict), representing a `r sprintf("%.1f%%", 100 * (sensitivity_metrics$Species[1] - sensitivity_metrics$Species[3]) / sensitivity_metrics$Species[1])` reduction. This suggests that many rare species are represented by records with quality issues.

3. **Higher Taxonomic Levels**: Family and class-level richness are more robust to filtering, showing only minimal changes across strategies. This indicates that higher-level biodiversity patterns are less affected by data quality issues.

4. **Implications**: The sensitivity analysis demonstrates that:
   - Our primary findings are not artifacts of arbitrary filtering choices
   - Different research questions legitimately require different quality standards
   - Rare species documentation is particularly vulnerable to strict filtering
   - Higher-level taxonomic analyses are more robust to quality issues

This analysis validates our moderate filtering approach as a balanced strategy that retains sufficient data for robust analyses while removing clear quality issues. Researchers with different analytical needs can use this framework to select appropriate filtering strategies for their specific applications.

## Spatial Bias

```{r spatial-bias-results}
effort_summary <- spatial_summary$effort_summary
```

### Sampling Effort Distribution

Spatial analysis revealed profound heterogeneity in sampling effort across Kenya's landscape (Figure 1). Of the **`r format(effort_summary$total_cells, big.mark=",")`** hexagonal grid cells (~10 km resolution, ~100 km¬≤ each) covering Kenya's ~580,000 km¬≤ area, only **`r format(effort_summary$cells_with_records, big.mark=",")`** (`r round(effort_summary$percent_covered, 1)`%) contained any occurrence records. This means that **`r round(100 - effort_summary$percent_covered, 1)`% of Kenya's geographic space is completely unsampled** in the GBIF database.

**Sampling Intensity Patterns:**
- **Median records per sampled cell**: `r effort_summary$median_records` (indicating most sampled cells have very few records)
- **Mean records per sampled cell**: `r round(effort_summary$mean_records, 1)` (substantially higher than median, indicating right-skewed distribution)
- **Standard deviation**: `r round(effort_summary$sd_records, 1)` (high variance indicates extreme heterogeneity)
- **Coefficient of variation**: `r round(100 * effort_summary$sd_records / effort_summary$mean_records, 1)`% (very high, confirming uneven sampling)

**Spatial Concentration:**
The stark difference between median (`r effort_summary$median_records`) and mean (`r round(effort_summary$mean_records, 1)`) records per cell indicates that sampling is highly concentrated in a small proportion of cells with very high record counts, while the majority of sampled cells have minimal records. This pattern is characteristic of accessibility-driven sampling, where researchers repeatedly visit the same easily accessible locations while vast areas remain unvisited.

**Geographic Coverage Implications:**
With less than `r round(effort_summary$percent_covered, 1)`% of Kenya's area containing any occurrence data, biodiversity assessments based on GBIF data represent a geographically limited sample. This raises critical questions about the representativeness of Kenya's biodiversity documentation, particularly for:
- Under-sampled ecosystems (arid and semi-arid lands, remote forests)
- Range-restricted species in unsampled areas
- Spatial biodiversity patterns and gradients
- Conservation planning that assumes comprehensive species distribution knowledge

```{r fig-sampling-effort, fig.cap="Spatial distribution of sampling effort across Kenya. Color intensity represents log10-transformed number of occurrence records per hexagonal grid cell (~10 km resolution)."}
knitr::include_graphics(here("figures", "01_sampling_effort_map.png"))
```

```{r interactive-sampling-map, eval=FALSE}
# Interactive Leaflet Map - CURRENTLY DISABLED
# Requires spatial grid data from scripts/02_spatial_bias.R
# To enable: Run the analysis pipeline, then change eval=FALSE to eval=TRUE

# Check if spatial grid data exists
spatial_grid_file <- file.path(data_outputs, "spatial_grid_effort.rds")

if (file.exists(spatial_grid_file)) {
  # Load spatial grid data
  spatial_grid <- readRDS(spatial_grid_file)

# Transform to WGS84 for leaflet
spatial_grid_wgs84 <- st_transform(spatial_grid, 4326)

# Create color palette for record counts (log scale)
pal <- colorNumeric(
  palette = "viridis",
  domain = log10(spatial_grid_wgs84$n_records + 1),
  na.color = "transparent"
)

# Create interactive map
leaflet(spatial_grid_wgs84) %>%
  addProviderTiles("OpenStreetMap") %>%
  addPolygons(
    fillColor = ~pal(log10(n_records + 1)),
    fillOpacity = 0.7,
    color = "white",
    weight = 1,
    popup = ~paste0(
      "<strong>Grid Cell Summary</strong><br>",
      "Records: ", format(n_records, big.mark = ","), "<br>",
      "Species: ", n_species, "<br>",
      "Log10 Records: ", round(log10(n_records + 1), 2)
    ),
    highlightOptions = highlightOptions(
      weight = 3,
      color = "#666",
      fillOpacity = 0.9,
      bringToFront = TRUE
    )
  ) %>%
  addLegend(
    pal = pal,
    values = ~log10(n_records + 1),
    title = "Log10(Records)",
    position = "bottomright",
    opacity = 0.7
  ) %>%
  addControl(
    html = "<div style='background: white; padding: 10px; border-radius: 5px;'>
            <strong>Interactive Exploration:</strong><br>
            ‚Ä¢ Zoom in/out to explore different regions<br>
            ‚Ä¢ Click cells to see detailed statistics<br>
            ‚Ä¢ Hover to highlight individual cells
            </div>",
    position = "topright"
  )

# Add function flow annotation
create_function_flow(
  source_file = "scripts/02_spatial_bias.R",
  line_range = "50-125",
  func_name = "calculate_grid_metrics() + leaflet()",
  description = "Hexagonal grid created with st_make_grid(), metrics calculated by joining occurrence data to grid cells"
)
```

### Spatial Autocorrelation

Sampling effort exhibited strong positive spatial autocorrelation (Moran's I = `r round(moran_results$morans_i, 3)`, p < 0.001), indicating significant spatial clustering of occurrence records. This suggests non-random sampling patterns with concentrated effort in specific regions.

```{r spatial-autocorrelation-table}
moran_table <- data.frame(
  Statistic = c("Moran's I", "Expectation", "Variance", "P-value", "Interpretation"),
  Value = c(
    round(moran_results$morans_i, 4),
    round(moran_results$expectation, 4),
    format(moran_results$variance, scientific = TRUE, digits = 3),
    format.pval(moran_results$p_value, eps = 0.001),
    moran_results$interpretation
  )
)

kable(moran_table, caption = "Table 6: Spatial autocorrelation analysis of sampling effort",
      booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

### Environmental Bias

Environmental bias analysis revealed systematic deviations between sampled locations and the full environmental space available across Kenya (Table 7, Figure 5). All three tested environmental variables showed **highly significant differences** (all p < 0.001) between sampled and available distributions, indicating that GBIF records do not represent a random or representative sample of Kenya's environmental diversity.

**Specific Environmental Biases Detected:**

```{r environmental-bias-details}
# Extract specific bias information
elev_bias <- env_bias %>% filter(variable == "Elevation")
temp_bias <- env_bias %>% filter(variable == "Temperature")
precip_bias <- env_bias %>% filter(variable == "Precipitation")
```

```{r environmental-bias-pipeline, echo=FALSE, results='asis'}
# Show complete code pipeline for environmental bias calculation
create_code_details("
### COMPLETE PIPELINE: Environmental Data ‚Üí Bias Tests ‚Üí Results ###

## Step 1: Sample available environmental space (background)
## File: scripts/02_spatial_bias.R, lines 432-449
set.seed(123)
background_points <- st_sample(kenya_boundary, size = 10000) %>%
  st_coordinates()

# Extract environmental values for random background points
bg_elev_extracted <- terra::extract(elevation, background_points)
bg_temp_extracted <- terra::extract(climate[[1]], background_points)
bg_precip_extracted <- terra::extract(climate[[12]], background_points)

# Create background environmental data frame
bg_env <- data.frame(
  type = 'available',
  elevation = bg_elev_extracted[[ncol(bg_elev_extracted)]],
  temperature = bg_temp_extracted[[ncol(bg_temp_extracted)]],
  precipitation = bg_precip_extracted[[ncol(bg_precip_extracted)]]
)

## Step 2: Extract environmental values at occurrence locations
## File: scripts/02_spatial_bias.R, lines 452-456
occ_env <- kenya_data_env %>%
  dplyr::select(elevation, temperature = bio1_temp, precipitation = bio12_precip) %>%
  filter(!is.na(elevation)) %>%
  mutate(type = 'sampled')

## Step 3: Combine sampled vs available for comparison
## File: scripts/02_spatial_bias.R, lines 458-461
env_comparison <- bind_rows(
  occ_env %>% dplyr::select(type, elevation, temperature, precipitation),
  bg_env %>% filter(!is.na(elevation))
)

## Step 4: Kolmogorov-Smirnov tests for each variable
## File: scripts/02_spatial_bias.R, lines 464-477
env_tests <- list(
  elevation = ks.test(
    occ_env$elevation,
    bg_env$elevation[!is.na(bg_env$elevation)]
  ),
  temperature = ks.test(
    occ_env$temperature[!is.na(occ_env$temperature)],
    bg_env$temperature[!is.na(bg_env$temperature)]
  ),
  precipitation = ks.test(
    occ_env$precipitation[!is.na(occ_env$precipitation)],
    bg_env$precipitation[!is.na(bg_env$precipitation)]
  )
)

## Step 5: Extract test statistics into tidy dataframe
## File: scripts/02_spatial_bias.R, lines 479-486
env_bias_summary <- map_df(names(env_tests), ~{
  tibble(
    variable = .x,
    D_statistic = env_tests[[.x]]$statistic,
    p_value = env_tests[[.x]]$p.value,
    significant = p_value < 0.05
  )
})

## Step 6: Load in R Markdown and extract specific tests
## File: docs/kenya_gbif_bias_assessment.Rmd, lines 1217-1219
elev_bias <- env_bias %>% filter(variable == 'Elevation')
temp_bias <- env_bias %>% filter(variable == 'Temperature')
precip_bias <- env_bias %>% filter(variable == 'Precipitation')

### FULL DATA FLOW SUMMARY ###
# 1. Load environmental rasters (elevation, climate)
# 2. Sample 10,000 random background points from Kenya
# 3. Extract environmental values at background points (available space)
# 4. Extract environmental values at occurrence locations (sampled space)
# 5. Run Kolmogorov-Smirnov two-sample tests
#    - Null hypothesis: Sampled and available distributions are the same
#    - Rejection indicates environmental bias in sampling
# 6. Extract D-statistic (max difference between CDFs) and p-value
# 7. Use in text to report bias for each environmental variable

### WHAT THE RESULTS MEAN ###
# D_statistic: Maximum vertical distance between cumulative distribution functions
#   - Range: 0 (identical) to 1 (completely different)
#   - Higher D = stronger bias
# p_value: Probability of observing this D under null hypothesis
#   - p < 0.05: Significant bias detected
#   - p < 0.001: Highly significant bias
",
  title = "üìã View complete environmental bias calculation (from rasters to KS tests)"
)
```

1. **Elevation Bias** (D = `r round(elev_bias$D_statistic, 3)`, p < 0.001):
   - Sampled locations are biased toward mid-elevations (1000-2500m)
   - Under-representation of both low-elevation coastal zones and high-elevation mountains
   - Likely driven by accessibility (major cities and roads concentrated at intermediate elevations)
   - Implications: Alpine biodiversity and coastal ecosystems are under-documented

2. **Temperature Bias** (D = `r round(temp_bias$D_statistic, 3)`, p < 0.001):
   - Sampling concentrated in cooler, highland regions (Nairobi, central highlands)
   - Warm lowlands (northern Kenya, eastern arid zones) under-sampled
   - Creates false impression of Kenya's biodiversity being primarily temperate/highland
   - Implications: Heat-adapted species and thermal niche modeling are compromised

3. **Precipitation Bias** (D = `r round(precip_bias$D_statistic, 3)`, p < 0.001):
   - Mesic (moderate rainfall) areas over-sampled
   - Arid and semi-arid lands (ASALs) substantially under-represented despite covering >80% of Kenya
   - Wettest areas (western Kenya forests) also under-sampled
   - Implications: Dryland biodiversity documentation is critically inadequate

**Implications for Biodiversity Science:**
The environmental bias documented here has profound implications:
- **Species Distribution Models (SDMs)**: Models trained on biased environmental data will perform poorly when projected to under-sampled environments
- **Climate Change Vulnerability**: Under-sampling of hot, dry environments limits ability to identify climate-vulnerable species
- **Ecological Niche Models**: Realized niches estimated from biased data may not reflect true environmental tolerances
- **Conservation Priority Setting**: Biodiversity hotspots may be artifacts of sampling bias rather than genuine richness patterns
- **National Biodiversity Assessments**: Reports based on GBIF data may misrepresent Kenya's biodiversity, under-counting dryland species

```{r environmental-bias-table}
env_bias_table <- env_bias %>%
  mutate(
    D_statistic = round(D_statistic, 4),
    p_value = format.pval(p_value, eps = 0.001),
    significant = ifelse(significant, "Yes", "No")
  ) %>%
  dplyr::select(Variable = variable, `D Statistic` = D_statistic,
         `P-value` = p_value, Significant = significant)

kable(env_bias_table,
      caption = "Table 7: Kolmogorov-Smirnov tests for environmental bias (sampled vs available space)",
      booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

```{r fig-environmental-bias, fig.cap="Comparison of environmental space between sampled locations (red) and available space in Kenya (grey). Significant differences indicate environmental bias in sampling."}
knitr::include_graphics(here("figures", "03_environmental_bias.png"))
```

## Temporal Bias

### Temporal Trends

Data collection showed a significant increasing trend over time for both number of records (Mann-Kendall œÑ = `r round(trend_tests$tau[trend_tests$variable == "n_records"], 3)`, p < 0.001) and number of species (Mann-Kendall œÑ = `r round(trend_tests$tau[trend_tests$variable == "n_species"], 3)`, p < 0.001).

```{r temporal-trends-table}
trend_table <- trend_tests %>%
  mutate(
    tau = round(tau, 4),
    p_value = format.pval(p_value, eps = 0.001)
  ) %>%
  dplyr::select(Variable = variable, `Kendall's œÑ` = tau, `P-value` = p_value, Trend = trend)

kable(trend_table, caption = "Table 8: Mann-Kendall trend tests for temporal patterns",
      booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

```{r fig-temporal-trends, fig.cap="Temporal trends in GBIF data for Kenya showing number of records (blue) and species (red, scaled √ó10) from 1950 to present."}
knitr::include_graphics(here("figures", "05_temporal_trends.png"))
```

```{r interactive-temporal-trends, eval=exists("temporal_summary")}
# Interactive Plotly Chart for Temporal Trends
# Allows zooming, panning, and hover to see exact values
# Only renders if temporal data has been loaded

# Check if we have the required data structure
if (!is.null(temporal_summary) && "temporal_stats" %in% names(temporal_summary)) {
  temporal_data <- temporal_summary$temporal_stats

  # Verify required columns exist
  if (all(c("year", "n_records", "n_species") %in% names(temporal_data))) {
    # Create interactive plotly chart with dual y-axes
    fig <- plot_ly(temporal_data)

    # Add records trace
    fig <- fig %>%
      add_trace(
        x = ~year,
        y = ~n_records,
        name = "Records",
        type = 'scatter',
        mode = 'lines+markers',
        line = list(color = '#2E86AB', width = 2),
        marker = list(size = 5),
        hovertemplate = paste(
          '<b>Year:</b> %{x}<br>',
          '<b>Records:</b> %{y:,}<br>',
          '<extra></extra>'
        )
      )

    # Add species trace on secondary axis
    fig <- fig %>%
      add_trace(
        x = ~year,
        y = ~n_species,
        name = "Species",
        type = 'scatter',
        mode = 'lines+markers',
        yaxis = 'y2',
        line = list(color = '#A23B72', width = 2),
        marker = list(size = 5),
        hovertemplate = paste(
          '<b>Year:</b> %{x}<br>',
          '<b>Species:</b> %{y}<br>',
          '<extra></extra>'
        )
      )

    # Configure layout with dual axes
    fig <- fig %>%
      layout(
        title = list(
          text = "Interactive Temporal Trends: Records and Species over Time",
          font = list(size = 16)
        ),
        xaxis = list(title = "Year"),
        yaxis = list(
          title = "Number of Records",
          titlefont = list(color = '#2E86AB'),
          tickfont = list(color = '#2E86AB')
        ),
        yaxis2 = list(
          title = "Number of Species",
          titlefont = list(color = '#A23B72'),
          tickfont = list(color = '#A23B72'),
          overlaying = 'y',
          side = 'right'
        ),
        hovermode = 'x unified',
        legend = list(x = 0.1, y = 0.9),
        margin = list(l = 60, r = 60, t = 80, b = 60)
      ) %>%
      config(
        displayModeBar = TRUE,
        displaylogo = FALSE,
        modeBarButtonsToRemove = c('lasso2d', 'select2d')
      )

    # Display the interactive chart
    print(fig)

    # Add function flow annotation
    create_function_flow(
      source_file = "scripts/03_temporal_bias.R",
      line_range = "85-150",
      func_name = "group_by(year) + summarize() + plotly::plot_ly()",
      description = "Temporal statistics aggregated by year, showing trends in data accumulation over time"
    )

    # Add collapsible code showing COMPLETE pipeline from R scripts
    create_code_details("
### COMPLETE PIPELINE: Raw Data ‚Üí Temporal Statistics ‚Üí Interactive Chart ###

## Step 1: Load cleaned data (from 01_data_download.R)
## File: scripts/03_temporal_bias.R, lines 26-28
kenya_data <- readRDS(file.path(data_processed, 'kenya_gbif_clean.rds'))

## Step 2: Aggregate data by year
## File: scripts/03_temporal_bias.R, lines 34-45
temporal_summary <- kenya_data %>%
  filter(!is.na(year)) %>%
  group_by(year) %>%
  summarise(
    n_records = n(),
    n_species = n_distinct(species),
    n_genera = n_distinct(genus),
    n_families = n_distinct(family),
    n_collectors = n_distinct(recordedBy, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  arrange(year)

## Step 3: Calculate overall temporal statistics
## File: scripts/03_temporal_bias.R, lines 48-60
temporal_stats <- temporal_summary %>%
  summarise(
    year_min = min(year),
    year_max = max(year),
    year_range = year_max - year_min,
    total_years = n_distinct(year),
    mean_records_per_year = mean(n_records),
    median_records_per_year = median(n_records),
    sd_records_per_year = sd(n_records),
    cv_records = sd_records_per_year / mean_records_per_year,
    mean_species_per_year = mean(n_species),
    median_species_per_year = median(n_species)
  )

## Step 4: Test for temporal trends (Mann-Kendall)
## File: scripts/03_temporal_bias.R, lines 68-83
library(Kendall)
mk_records <- MannKendall(temporal_summary$n_records)
mk_species <- MannKendall(temporal_summary$n_species)

trend_tests <- data.frame(
  variable = c('n_records', 'n_species'),
  tau = c(mk_records$tau, mk_species$tau),
  p_value = c(mk_records$sl, mk_species$sl),
  trend = c(
    ifelse(mk_records$sl < 0.05,
           ifelse(mk_records$tau > 0, 'Increasing', 'Decreasing'),
           'No trend'),
    ifelse(mk_species$sl < 0.05,
           ifelse(mk_species$tau > 0, 'Increasing', 'Decreasing'),
           'No trend')
  )
)

## Step 5: Create interactive visualization (in this R Markdown)
## File: docs/kenya_gbif_bias_assessment.Rmd, lines 1237-1303
library(plotly)
fig <- plot_ly(temporal_data) %>%
  add_trace(
    x = ~year, y = ~n_records,
    name = 'Records', type = 'scatter', mode = 'lines+markers'
  ) %>%
  add_trace(
    x = ~year, y = ~n_species,
    name = 'Species', yaxis = 'y2', type = 'scatter', mode = 'lines+markers'
  )

### FULL DATA FLOW SUMMARY ###
# 01_data_download.R ‚Üí kenya_gbif_clean.rds (cleaned occurrence records)
# 03_temporal_bias.R ‚Üí temporal_summary (records/species by year)
# 03_temporal_bias.R ‚Üí trend_tests (statistical tests for trends)
# kenya_gbif_bias_assessment.Rmd ‚Üí Interactive plotly visualization
",
      title = "üìã View complete code pipeline (R scripts ‚Üí results)"
    )
  }
}
```

### Temporal Completeness

The temporal coverage spanned **`r temporal_stats$year_range`** years (`r temporal_stats$year_min`-`r temporal_stats$year_max`), with data available for **`r temporal_stats$total_years`** years. Temporal completeness was **`r round(gap_summary$temporal_completeness, 1)`%**, with **`r gap_summary$years_without_records`** years containing no records. The coefficient of variation for annual records was **`r round(temporal_stats$cv_records, 2)`**, indicating high inter-annual variability.

```{r temporal-summary-table}
temporal_sum_table <- data.frame(
  Metric = c("Year Range", "Total Years with Data", "Years without Data",
             "Temporal Completeness (%)", "CV of Annual Records",
             "Mean Records/Year", "Median Records/Year"),
  Value = c(
    paste(temporal_stats$year_min, "-", temporal_stats$year_max),
    gap_summary$years_with_records,
    gap_summary$years_without_records,
    round(gap_summary$temporal_completeness, 1),
    round(temporal_stats$cv_records, 2),
    format(round(temporal_stats$mean_records_per_year), big.mark = ","),
    format(round(temporal_stats$median_records_per_year), big.mark = ",")
  )
)

kable(temporal_sum_table, caption = "Table 9: Temporal coverage summary",
      booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

## Taxonomic Bias

### Taxonomic Coverage and Inequality

```{r taxonomic-overview}
tax_overview <- taxonomic_summary$overview
diversity_metrics <- taxonomic_summary$diversity_metrics
gini_records <- diversity_metrics$value[diversity_metrics$metric == "Gini_records"]
gini_species <- diversity_metrics$value[diversity_metrics$metric == "Gini_species"]
simpson_records <- diversity_metrics$value[diversity_metrics$metric == "Simpson_records"]
shannon_records <- diversity_metrics$value[diversity_metrics$metric == "Shannon_records"]

# Calculate concentration metrics
top_3_classes <- head(class_summary, 3)
top_3_pct <- 100 * sum(top_3_classes$n_records) / sum(class_summary$n_records)
top_10_pct <- 100 * sum(head(class_summary, 10)$n_records) / sum(class_summary$n_records)
```

Taxonomic bias analysis revealed extreme inequality in sampling effort across taxonomic groups, representing one of the most severe biases in Kenya's GBIF data. The Gini coefficient for record distribution across classes was **`r round(gini_records, 3)`**, indicating **very high inequality** (values > 0.6 represent severe concentration; perfect equality = 0, perfect inequality = 1).

**Quantifying Taxonomic Concentration:**

```{r taxonomic-concentration-stats}
# Additional concentration metrics
n_classes_total <- nrow(class_summary)
classes_with_1000_plus <- sum(class_summary$n_records >= 1000)
classes_with_100_plus <- sum(class_summary$n_records >= 100)
```

- **Top 3 classes** account for **`r round(top_3_pct, 1)`%** of all records
- **Top 10 classes** account for **`r round(top_10_pct, 1)`%** of all records
- Only **`r classes_with_1000_plus` of `r n_classes_total` classes** (`r round(100*classes_with_1000_plus/n_classes_total, 1)`%) have ‚â•1,000 records
- **`r n_classes_total - classes_with_100_plus` classes** (`r round(100*(n_classes_total - classes_with_100_plus)/n_classes_total, 1)`%) have <100 records each

This extreme concentration means that a small number of well-studied taxonomic groups dominate the database, while the vast majority of classes are severely under-represented.

**Diversity Indices:**
- **Gini Coefficient (records)**: `r round(gini_records, 3)` (very high inequality)
- **Gini Coefficient (species)**: `r round(gini_species, 3)` (inequality in species richness among classes)
- **Simpson's Diversity**: `r round(simpson_records, 3)` (low evenness)
- **Shannon's Diversity**: `r round(shannon_records, 3)` (moderate diversity but uneven distribution)

**Taxonomic Groups Driving the Database:**

The database is overwhelmingly dominated by a few charismatic or well-studied groups (Table 10):

```{r top-taxonomic-drivers}
# Get top 5 classes
top_5 <- head(class_summary, 5)
top_class_name <- top_5$class[1]
top_class_pct <- 100 * top_5$n_records[1] / sum(class_summary$n_records)
```

1. **`r top_class_name`**: Dominates with `r round(top_class_pct, 1)`% of all records
2. **Vertebrate bias**: Birds, mammals, and reptiles collectively comprise the majority of records
3. **Invertebrate under-representation**: Despite being the most diverse group, invertebrates (except charismatic groups like butterflies) are severely under-sampled
4. **Plant coverage**: Vascular plants moderately well-represented, but bryophytes and fungi largely absent
5. **Microbial neglect**: Bacteria, archaea, and protists virtually absent from the database

**Implications of Taxonomic Bias:**

This severe taxonomic inequality affects multiple aspects of biodiversity science:

1. **Biodiversity Assessment**: National species richness estimates are heavily biased toward vertebrates and vascular plants, grossly under-representing true diversity
2. **Ecological Networks**: Food webs, pollination networks, and trophic interactions cannot be studied due to missing invertebrate and plant data
3. **Ecosystem Function**: Functional diversity and ecosystem services cannot be assessed without adequate invertebrate, fungal, and microbial data
4. **Conservation Prioritization**: Focus on charismatic megafauna perpetuates neglect of invertebrate conservation needs
5. **Comparative Analyses**: Cross-taxon comparisons are invalid when sampling effort varies by orders of magnitude
6. **Biodiversity Indicators**: Aggregate biodiversity metrics are dominated by well-sampled groups, masking trends in under-sampled taxa

```{r top-classes-table}
top_10_classes <- head(class_summary, 10) %>%
  dplyr::select(Kingdom = kingdom, Phylum = phylum, Class = class,
         Records = n_records, Species = n_species, Families = n_families) %>%
  mutate(
    Records = format(Records, big.mark = ","),
    Species = format(Species, big.mark = ","),
    Families = format(Families, big.mark = ",")
  )

kable(top_10_classes, caption = "Table 10: Top 10 taxonomic classes by number of records",
      booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = FALSE, font_size = 11)
```

```{r fig-taxonomic-treemap, fig.cap="Treemap showing relative representation of top 15 taxonomic classes in GBIF Kenya data. Area represents number of records, color represents number of species."}
knitr::include_graphics(here("figures", "10_taxonomic_treemap.png"))
```

### Taxonomic Inequality

The Lorenz curve (Figure 5) illustrates the concentration of records among taxonomic classes. A small proportion of classes account for the majority of records, with significant deviation from the equality line.

```{r fig-lorenz, fig.cap="Lorenz curve showing taxonomic inequality in record distribution across classes. The dashed red line represents perfect equality; deviation indicates concentration of records in few classes."}
knitr::include_graphics(here("figures", "11_lorenz_curve.png"))
```

### Species Rarity

A substantial proportion of species were represented by very few records. **`r rarity_summary$n_species[rarity_summary$rarity_class == "Singleton (1 record)"]`** species (`r round(100*rarity_summary$prop_species[rarity_summary$rarity_class == "Singleton (1 record)"], 1)`%) were singletons (1 record), and **`r sum(rarity_summary$n_species[rarity_summary$rarity_class %in% c("Singleton (1 record)", "Doubleton (2 records)", "Very rare (3-5 records)")])`** species (`r round(100*sum(rarity_summary$prop_species[rarity_summary$rarity_class %in% c("Singleton (1 record)", "Doubleton (2 records)", "Very rare (3-5 records)")]), 1)`%) had ‚â§5 records.

```{r rarity-table}
rarity_table <- rarity_summary %>%
  mutate(
    n_species = format(n_species, big.mark = ","),
    total_records = format(total_records, big.mark = ","),
    prop_species = paste0(round(prop_species * 100, 1), "%"),
    prop_records = paste0(round(prop_records * 100, 1), "%")
  ) %>%
  dplyr::select(`Rarity Class` = rarity_class, `Species` = n_species,
         `Total Records` = total_records, `% Species` = prop_species,
         `% Records` = prop_records)

kable(rarity_table, caption = "Table 11: Distribution of species by rarity class",
      booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

## Predictors of Sampling Effort

```{r model-results}
# Extract count model summary
count_model_summary <- model_summaries %>%
  filter(model == "Count Model", term != "(Intercept)")

# Diagnostic: Check what predictors are in the model
cat(sprintf("\nModel summary contains %d predictor(s):\n", nrow(count_model_summary)))
if (nrow(count_model_summary) > 0) {
  cat(paste("  -", count_model_summary$term, collapse = "\n"))
  cat("\n")
} else {
  cat("  WARNING: No predictors found in model summary!\n")
}

# Note: Model may use subset of predictors based on data availability
# Expected predictors: elevation, temperature, precipitation, distance to city,
# distance from equator, distance from coast
```

The `r modeling_summary$model_type` GLM was specified with six environmental and geographic predictors: elevation, temperature, precipitation, distance to nearest city, distance from equator, and distance from coast (see Methods section). Table 12 presents the coefficient estimates for all predictors included in the fitted model.

```{r model-coefficients-table}
# Create clean, descriptive predictor names
predictor_name_map <- c(
  "elevation_scaled" = "Elevation (scaled)",
  "temperature_scaled" = "Temperature (scaled)",
  "precipitation_scaled" = "Precipitation (scaled)",
  "dist_to_city_scaled" = "Distance to City (scaled)",
  "dist_from_equator" = "Distance from Equator (degrees)",
  "dist_from_coast_km" = "Distance from Coast (km)"
)

# Format coefficients table with proper names
coef_table <- count_model_summary %>%
  mutate(
    # Use explicit name mapping for clarity and consistency
    Predictor = ifelse(term %in% names(predictor_name_map),
                      predictor_name_map[term],
                      # Fallback: clean up the term name
                      str_to_title(str_replace_all(str_remove(term, "_scaled"), "_", " ")))
  ) %>%
  dplyr::select(Predictor, Estimate = estimate, `Std. Error` = std.error,
         `Z value` = statistic, `P-value` = p.value) %>%
  mutate(
    Estimate = round(Estimate, 4),
    `Std. Error` = round(`Std. Error`, 4),
    `Z value` = round(`Z value`, 3),
    `P-value` = format.pval(`P-value`, eps = 0.001)
  )

# Display table
kable(coef_table,
      caption = "Table 12: Coefficients from negative binomial GLM predicting sampling effort. The model includes environmental variables (elevation, temperature, precipitation) and geographic variables (distance to cities, distance from equator, distance from coast). All continuous predictors were scaled (mean-centered, unit variance) except distance from equator and coast. Significant negative coefficient for distance to city indicates sampling decreases with distance from urban areas.",
      booktabs = TRUE,
      align = c("l", "r", "r", "r", "r")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = FALSE)
```

```{r fig-coefficients, fig.cap="Coefficient plot showing predictors of sampling effort with 95% confidence intervals. Points right of zero indicate positive effects, left indicate negative effects."}
knitr::include_graphics(here("figures", "23_coefficient_plot.png"))
```

### Sampling Bias Map

Based on model predictions, we identified under-sampled and over-sampled regions relative to what the model predicts based on environmental and geographic factors (Figure 10).

**Critical Interpretation Note:** The bias map shows **residuals** from the GLM model - deviations between observed and predicted sampling effort. This means:

- **Blue areas (negative residuals)**: Under-sampled *relative to model predictions*. These areas have fewer records than the model predicts given their environmental conditions and accessibility. Importantly, **areas close to cities can be under-sampled** if their actual sampling is lower than expected given their proximity to urban centers and favorable environmental conditions.

- **Red areas (positive residuals)**: Over-sampled *relative to model predictions*. These areas have more records than expected, possibly due to research stations, protected areas, or targeted surveys not captured by the model's environmental/geographic predictors.

This is fundamentally different from a raw sampling effort map. A remote area could appear "over-sampled" (red) if it has more records than expected given its remoteness, while an accessible urban area could be "under-sampled" (blue) if it has fewer records than predicted. The residuals reveal **unexplained patterns** - sampling bias that remains after accounting for accessibility and environmental gradients.

The model explains sampling patterns based on distance to cities, elevation, climate, and geographic gradients. Areas with large residuals (positive or negative) represent locations where sampling deviates from these predictable patterns, potentially due to:
- Research station locations not captured in "distance to cities"
- Protected areas with targeted monitoring
- Historical expedition routes
- Regions with active citizen science communities
- Areas with restricted access despite proximity to cities

Under-sampled areas (blue) represent priority targets for future sampling efforts, as they are under-represented even after accounting for their environmental and geographic characteristics.

```{r fig-bias-map, fig.cap="Figure 10: Sampling bias map showing standardized residuals from the negative binomial GLM. Blue areas are under-sampled relative to environmental/geographic predictors (fewer records than predicted), red areas are over-sampled (more records than predicted). Note: This shows deviations from expected patterns, not raw sampling effort. Areas close to cities can be under-sampled if their actual sampling is lower than the model predicts given their accessibility."}
knitr::include_graphics(here("figures", "22_sampling_bias_map.png"))
```

# Discussion

## Data Quality Issues

Our comprehensive quality flagging framework identified quality issues in `r round(flag_rate, 1)`% of the original GBIF records for Kenya. This substantial flag rate underscores the importance of rigorous quality control in biodiversity data workflows and highlights several key insights about our innovative flagging approach:

### Transparency in Data Quality Reporting

Most studies using GBIF data report only final cleaned dataset sizes, without quantifying the specific quality issues encountered or explaining which filters were applied. Our systematic flagging approach provides maximum transparency by documenting all quality issues while preserving all records, enabling downstream analyses to apply filters appropriate to their specific needs. This approach enables:

1. **Reproducibility:** Other researchers can apply comparable quality standards
2. **Fitness-for-use assessment:** Data users can evaluate whether quality standards meet their analytical needs
3. **Regional comparisons:** Quality metrics can be compared across countries and taxa
4. **Data improvement:** GBIF and data providers can identify systematic quality issues requiring attention

### Coordinate Quality as a Primary Issue

Coordinate-related issues represented a major source of data quality problems. Among records evaluated at the coordinate validation stage, `r format(quality_metrics$coordinate_quality$total_coord_flags, big.mark=",")` records (`r round(quality_metrics$coordinate_quality$percent_coord_flags, 1)`%) were flagged by CoordinateCleaner tests. The most prevalent coordinate issue was `r top_coord_issue$test`, affecting `r round(top_coord_issue$percent_of_original, 2)`% of original records. These findings align with previous studies [@zizka2019] showing that georeferencing errors are widespread in biodiversity databases.

Coordinate quality issues arise from multiple sources:

- **Historical collections:** Many museum specimens were georeferenced retrospectively from locality descriptions
- **Administrative centroids:** Records assigned to capital or province centroids rather than actual occurrence locations
- **GPS precision:** Early GPS units and mobile devices had limited accuracy
- **Data entry errors:** Manual transcription of coordinates introduces errors

### Duplicates and Data Aggregation

The identification of `r format(dup_info$n_duplicate_records, big.mark=",")` duplicate records (`r round(pct_duplicates, 1)`%) reflects GBIF's role as a data aggregator. The same biological observation may be shared by multiple institutions or appear in multiple datasets, necessitating duplicate detection. However, distinguishing true duplicates from legitimate repeated observations at the same location requires careful consideration of dates, collectors, and metadata.

### Taxonomic Identification Completeness

While GBIF enforces kingdom-level classification, species-level identification varied considerably. Approximately `r round(100 - tax_complete$pct_n_with_species, 1)`% of records in the original dataset lacked species-level identification. This reflects:

- Identification difficulties in the field
- Specimens awaiting expert determination
- Taxonomic groups lacking identification keys
- Uncertainty in morphologically cryptic species

Requiring species-level identification, while appropriate for species distribution modeling, excludes potentially valuable data on genus- or family-level occurrences that could inform higher-level biodiversity patterns.

### Implications for Biodiversity Science

The quality issues documented here have important implications:

1. **Uncertainty quantification:** Error rates in coordinate precision and taxonomic identification propagate through downstream analyses
2. **Temporal trends in quality:** Data quality may vary over time, with older records potentially having lower coordinate precision but higher taxonomic expertise
3. **Taxonomic variation:** Quality issues may vary among taxa, affecting comparative analyses
4. **Fitness-for-use varies by application:** Appropriate quality standards depend on analytical goals (e.g., broad-scale macroecology vs. fine-scale conservation planning)

Our framework provides a template for transparent documentation of data quality that can be adopted across regions and taxa, facilitating standardization and comparability in biodiversity informatics.

## Spatial Bias

Our analysis revealed strong spatial biases in GBIF data for Kenya, consistent with global patterns [@beck2014; @meyer2016]. Sampling effort is concentrated near urban centers, roads, and accessible areas, leaving vast regions under-sampled. This accessibility bias is a well-documented phenomenon in biodiversity databases [@reddy2015] and has important implications:

1. **Conservation assessments** may overestimate species distributions in accessible areas
2. **Species distribution models** trained on biased data may produce unreliable predictions
3. **Biodiversity hotspot** identification may be confounded with sampling hotspots

The strong positive spatial autocorrelation (Moran's I = `r round(moran_results$morans_i, 3)`) indicates that nearby locations have similar sampling intensity, reflecting clustered sampling efforts rather than biological patterns.

## Temporal Bias

Temporal patterns show increasing data availability over time, particularly after 2000, coinciding with:

- Digital data mobilization efforts
- Increased citizen science participation (e.g., eBird, iNaturalist)
- Improved data sharing infrastructure

However, the high coefficient of variation (`r round(temporal_stats$cv_records, 2)`) and temporal gaps indicate inconsistent sampling effort across years. This temporal bias can:

- Confound trend analyses of biodiversity change
- Limit detection of phenological shifts
- Bias estimates of species rarity

## Taxonomic Bias

Taxonomic coverage is highly uneven, with a Gini coefficient of `r round(gini_records, 3)` indicating strong concentration of records in few classes. This reflects well-known biases favoring:

- **Charismatic megafauna** (birds, mammals)
- **Economically important groups** (agricultural pests/beneficials)
- **Groups with active hobbyist communities** (butterflies, birds)

The high proportion of singleton species (`r round(100*rarity_summary$prop_species[rarity_summary$rarity_class == "Singleton (1 record)"], 1)`%) may reflect:

- True rarity
- Identification errors
- Taxonomic uncertainties
- Sampling deficiencies

## Predictors of Sampling Effort

Distance to cities emerged as the strongest predictor of sampling effort, confirming accessibility bias. Environmental variables (elevation, temperature, precipitation) also influenced sampling, but to a lesser extent. The generalized additive model (GAM) revealed non-linear relationships, suggesting:

- Sampling peaks at intermediate elevations
- Urban-rural gradients drive sampling patterns
- Coastal areas receive differential attention

## Limitations

Our analysis has several limitations:

1. We assumed GBIF data represent the totality of available occurrence data for Kenya
2. Data quality issues (misidentifications, georeferencing errors) may persist despite cleaning
3. We lacked comprehensive reference data on Kenya's true biodiversity for all taxa
4. Temporal and spatial coverage may have improved since our download date

## Kenya's Biodiversity Data in Global Context

Kenya's data quality and bias patterns are not unique but reflect broader global patterns in biodiversity data. However, the severity and specific manifestations provide important regional insights:

**Comparison with Global Patterns:**
- **Quality issues**: Our ~`r round(flag_rate, 1)`% flag rate is comparable to other tropical regions (40-95% depending on standards applied)
- **Spatial bias**: Strong urban clustering (Moran's I = `r round(moran_results$morans_i, 3)`) is typical but particularly pronounced in Kenya
- **Taxonomic bias**: Gini coefficient of `r round(gini_records, 3)` indicates very high inequality, exceeding many temperate regions
- **Environmental bias**: Under-sampling of arid lands is acute in Kenya despite ASALs covering >80% of the country

**Kenya-Specific Challenges:**
1. **Vast arid lands**: Northern and eastern Kenya's semi-arid regions are logistically challenging and under-studied
2. **Security concerns**: Some areas with high biodiversity are difficult to access due to security issues
3. **Infrastructure limitations**: Road networks concentrated in central highlands, limiting access to remote areas
4. **Research capacity**: Limited national taxonomic expertise for certain groups (invertebrates, fungi, non-vascular plants)
5. **Historical focus**: Colonial-era collections concentrated near administrative centers (Nairobi, Mombasa, Nakuru)

## Broader Implications for Biodiversity Science and Conservation

### Implications for Species Distribution Modeling

The documented biases have critical implications for SDM applications in Kenya:

1. **Environmental Extrapolation**: Models trained on environmentally biased data must extrapolate to under-sampled conditions (hot, dry environments), reducing prediction accuracy
2. **Pseudo-absence Selection**: Background points for SDMs will be biased toward accessible areas unless corrected
3. **Model Evaluation**: Validation datasets inherit the same biases as training data, inflating apparent model performance
4. **Climate Change Projections**: Under-sampling of hot, dry environments limits ability to model species' responses to warming and drying
5. **Spatial Autocorrelation**: Clustered sampling violates independence assumptions in many modeling approaches

**Recommendation**: SDM practitioners should apply spatial filtering to reduce autocorrelation, use bias files to weight background selection, and explicitly test model transferability to under-sampled environments.

### Implications for Conservation Planning

Conservation applications require particular attention to data biases:

1. **Protected Area Gap Analysis**: Apparent "gaps" in species coverage may reflect sampling bias rather than genuine absence
2. **Threatened Species Assessment**: Range-restricted species in under-sampled areas may be overlooked
3. **Biodiversity Hotspot Identification**: Richness patterns confounded with sampling effort could misdirect conservation resources
4. **Systematic Conservation Planning**: Algorithms like Zonation or Marxan require unbiased input data to identify optimal reserve networks
5. **Monitoring and Reporting**: National biodiversity targets (Aichi, Kunming-Montreal) require unbiased baseline data

**Recommendation**: Conservation planners should incorporate sampling bias layers, prioritize field validation in under-sampled areas, and combine GBIF data with systematic survey data.

### Implications for Ecological and Evolutionary Research

Beyond applied conservation, fundamental science is affected:

1. **Macroecology**: Elevational gradients, latitudinal patterns, and diversity-environment relationships confounded by sampling bias
2. **Biogeography**: Historical biogeographic inferences require complete sampling across geographic ranges
3. **Community Ecology**: Species co-occurrence networks cannot be studied when sampling is taxonomically and spatially biased
4. **Trait-based Ecology**: Functional diversity assessments require balanced taxonomic sampling
5. **Phylogenetic Diversity**: Uneven taxonomic sampling biases phylogenetic diversity metrics and evolutionary distinctiveness scores

## Pathway Forward: Addressing Data Gaps

### Strategic Sampling Priorities

Based on our bias analyses, we identify specific priorities for targeted sampling:

**Geographic Priorities:**
1. **Northern Kenya** (Turkana, Marsabit, Samburu counties): Vast under-sampled region with unique arid-adapted biodiversity
2. **Eastern arid zones** (Garissa, Wajir, Mandera): Semi-arid ecosystems critically under-represented
3. **Coastal forests**: Small, fragmented patches with high endemism but low sampling
4. **Mt. Kenya and Mt. Elgon alpine zones**: High-elevation habitats with climate-sensitive species
5. **Western Kenya lowlands**: Areas identified as under-sampled by our residual bias maps

**Environmental Priorities:**
1. **Arid lands** (<400mm annual precipitation): Highest priority given extent and current under-representation
2. **Hot lowlands** (mean annual temperature >25¬∞C): Critical for climate change baselines
3. **High elevations** (>3500m): Alpine biodiversity at risk from climate change
4. **Extreme rainfall gradients**: Both very dry and very wet environments under-sampled

**Taxonomic Priorities:**
1. **Invertebrates**: All groups except butterflies and some beetles severely under-sampled
   - Priority families: Formicidae (ants), Acrididae (grasshoppers), Culicidae (mosquitoes)
2. **Fungi**: Almost completely absent from database
3. **Non-vascular plants**: Bryophytes, lichens, algae need systematic survey
4. **Soil organisms**: Nematodes, micro-arthropods essential for ecosystem function assessments
5. **Aquatic invertebrates**: Freshwater biodiversity severely under-documented

**Methodological Recommendations:**
1. **Systematic grid-based surveys**: Random or stratified sampling designs to reduce bias
2. **Standardized protocols**: Comparable methods across sites and time periods
3. **Multi-taxa inventories**: Simultaneous sampling of multiple taxonomic groups
4. **Parataxonomist training**: Build local capacity for invertebrate and plant identification
5. **Molecular methods**: DNA barcoding and metabarcoding to accelerate invertebrate and microbial inventories

### Institutional and Policy Recommendations

Addressing Kenya's biodiversity data gaps requires institutional support:

**For Kenyan Institutions:**
1. **National Museums of Kenya**: Expand digitization of historical collections, especially invertebrates and plants
2. **Universities**: Integrate biodiversity inventory into graduate training programs
3. **Kenya Wildlife Service**: Systematic biodiversity monitoring in protected areas
4. **Kenya Forest Service**: Forest biodiversity surveys beyond charismatic mammals
5. **County Governments**: Support local biodiversity documentation initiatives

**For International Collaboration:**
1. **Capacity building**: Training in taxonomy, especially for under-studied groups
2. **Equipment support**: Field equipment, microscopes, molecular facilities
3. **Long-term funding**: Multi-year sampling programs rather than short-term projects
4. **Data sharing**: Rapid digitization and sharing of new collections through GBIF

**For GBIF and Data Platforms:**
1. **Bias visualization tools**: Provide users with sampling bias maps for each region
2. **Quality metadata**: Standardize quality flag reporting across datasets
3. **Gap analysis tools**: Interactive tools to identify under-sampled regions/taxa
4. **Incentive mechanisms**: Recognize and reward data collection in under-sampled areas

## Recommendations for Data Users

Based on our findings, we provide specific, actionable recommendations for different user communities:

### For Species Distribution Modelers:

1. **Spatial filtering**: Apply spatial thinning (e.g., 10 km minimum distance) to reduce autocorrelation
2. **Bias files**: Use our sampling effort maps as bias grids for background point selection
3. **Environmental blocking**: Use environmental cross-validation to test extrapolation ability
4. **Model comparison**: Compare predictions from biased vs. spatially-filtered datasets
5. **Uncertainty mapping**: Explicitly map prediction uncertainty in under-sampled regions

### For Conservation Practitioners:

1. **Field validation**: Prioritize ground-truthing in under-sampled areas before conservation decisions
2. **Precautionary approach**: Assume species may occur in under-sampled areas unless proven absent
3. **Bias-corrected hotspots**: Use rarefaction or model-based approaches to identify biodiversity hotspots
4. **Targeted surveys**: Commission field surveys for threatened species in data-poor regions
5. **Multi-source integration**: Combine GBIF data with local knowledge, grey literature, and unpublished records

### For Biodiversity Researchers:

1. **Bias acknowledgment**: Explicitly acknowledge and discuss bias in all GBIF-based studies
2. **Sensitivity analysis**: Test how results change under different quality filtering strategies
3. **Comparative studies**: Control for sampling effort when comparing diversity across regions or taxa
4. **Null models**: Use null expectations to distinguish sampling artifacts from biological patterns
5. **Open code**: Share data cleaning and bias correction code for reproducibility

### For Policy and Reporting:

1. **Qualified conclusions**: National biodiversity reports should note data limitations and biases
2. **Indicator selection**: Choose biodiversity indicators robust to taxonomic and spatial bias
3. **Investment justification**: Use gap analyses to justify funding for under-sampled regions/taxa
4. **CBD reporting**: Include data quality and coverage assessments in national reports
5. **Adaptive management**: Update assessments as new data fills identified gaps

# Conclusions

## Summary of Key Findings

This comprehensive assessment of GBIF biodiversity data for Kenya reveals substantial challenges in both data quality and data representativeness that have critical implications for biodiversity science and conservation.

### Data Quality

Our systematic flagging framework documented quality issues in **`r round(flag_rate, 1)`%** of the original **`r format(quality_assessment$n_original, big.mark=",")`** occurrence records. The most prevalent issues were:
- **Coordinate quality problems**: `r round(quality_metrics$coordinate_quality$percent_coord_flags, 1)`% of records flagged by CoordinateCleaner tests
- **Missing species identification**: `r round(100*sum(kenya_flagged$flag_missing_species)/nrow(kenya_flagged), 1)`% of records
- **High coordinate uncertainty**: `r round(100*sum(kenya_flagged$flag_high_uncertainty, na.rm=TRUE)/nrow(kenya_flagged), 1)`% exceeding 10 km threshold

Critically, our **flagging approach** (rather than automatic filtering) preserves all records while documenting issues, enabling different analyses to apply quality filters appropriate to their specific requirements. This methodological innovation addresses a major gap in biodiversity data science.

### Spatial Bias

Spatial analysis revealed extreme geographic concentration:
- **`r round(100 - effort_summary$percent_covered, 1)`% of Kenya's area** contains no GBIF records
- **Strong positive spatial autocorrelation** (Moran's I = `r round(moran_results$morans_i, 3)`, p < 0.001) indicating clustered sampling
- **Significant environmental bias** across elevation, temperature, and precipitation gradients (all p < 0.001)
- **Accessibility drives sampling**: Distance to cities is the strongest predictor of sampling effort

These patterns indicate that GBIF data represent a geographically and environmentally limited sample of Kenya's biodiversity, with profound implications for conservation planning and species distribution modeling.

### Temporal Bias

Temporal patterns show:
- **Strong increasing trend** in data collection (Mann-Kendall œÑ > 0.6, p < 0.001)
- **High inter-annual variability** (CV = `r round(temporal_stats$cv_records, 2)`)
- **Recent dominance**: Most data collected after 2000, limiting long-term trend analysis
- **Temporal gaps**: `r gap_summary$years_without_records` years with no data

This temporal concentration complicates efforts to distinguish genuine biodiversity change from increased data collection.

### Taxonomic Bias

Taxonomic analysis revealed severe inequality:
- **Gini coefficient** of `r round(gini_records, 3)` indicates extreme concentration
- **Top `r length(top_3_classes$class)` classes** account for `r round(top_3_pct, 1)`% of all records
- **`r round(100*rarity_summary$prop_species[rarity_summary$rarity_class == "Singleton (1 record)"], 1)`% of species** are singletons (1 record each)
- **Vertebrate bias**: Birds, mammals, and reptiles dominate despite being taxonomically minor

This taxonomic bias grossly misrepresents Kenya's true biodiversity, which is dominated numerically by invertebrates, fungi, and microorganisms.

## Major Contributions

This study advances biodiversity informatics in several important ways:

### 1. Methodological Innovation: The Flagging Framework

Our quality **flagging** (not filtering) approach represents a paradigm shift:
- **Preserves information**: All records retained with documented quality issues
- **Enables flexibility**: Different analyses apply appropriate filters for their needs
- **Improves transparency**: Users see exactly what quality issues affect the data
- **Facilitates sensitivity analysis**: Easy to test how results vary with filtering stringency
- **Supports reproducibility**: Other researchers can apply comparable standards

This framework addresses longstanding criticisms of biodiversity data workflows that use opaque, one-size-fits-all filtering.

### 2. Integrated Assessment: Quality AND Bias

Most studies address either quality OR bias, but not both. We provide an integrated assessment showing how they interact:
- Quality issues are not uniformly distributed (older records have more problems)
- Biases affect different quality dimensions differently
- Filtering strategies must consider both aspects
- Combined effects are often non-additive

### 3. Comprehensive Documentation: State of Kenya's Biodiversity Data

We provide the first complete characterization of:
- **What we know**: `r format(quality_assessment$clean_counts$moderate_clean, big.mark=",")` quality-filtered records of `r format(summary_stats$n_species, big.mark=",")` species
- **What's missing**: Spatial, environmental, temporal, and taxonomic gaps
- **Why it matters**: Specific implications for different applications (SDMs, conservation, research)
- **What to do**: Concrete recommendations for data users, collectors, and platforms

### 4. Reproducible Framework: Template for Other Regions

Our R-based analytical pipeline is:
- **Fully documented**: Every step from download to final figures
- **Open source**: Code publicly available on GitHub
- **Modular**: Components can be applied independently
- **Transferable**: Readily applicable to other countries, regions, or taxa
- **Scalable**: Can process datasets from thousands to millions of records

This framework can serve as a template for standardized biodiversity data assessments globally.

### 5. Actionable Insights: Guiding Future Work

We move beyond description to prescription, identifying:
- **Priority sampling locations**: Specific under-sampled regions (northern Kenya, eastern arid zones, coastal forests, alpine zones)
- **Priority environments**: Arid lands, hot lowlands, high elevations
- **Priority taxa**: Invertebrates, fungi, non-vascular plants, soil organisms, aquatic invertebrates
- **Methodological improvements**: Systematic surveys, standardized protocols, multi-taxa inventories, molecular methods

## Broader Significance

This work has implications beyond Kenya:

### For East Africa
Our methods can be directly applied to neighboring countries (Tanzania, Uganda, Ethiopia, Somalia) to create regional biodiversity data assessments, enabling:
- Cross-border conservation planning
- Regional species distribution models
- Ecosystem-level analyses
- Coordinated sampling strategies

### For Tropical Regions
The challenges documented here (accessibility bias, environmental bias toward mesic highlands, taxonomic bias toward vertebrates) are common across tropical regions. Our framework provides a replicable approach to quantifying these issues.

### For Global Biodiversity Monitoring
As countries work toward CBD targets and SDGs, understanding data quality and bias is essential. Our framework contributes to:
- Essential Biodiversity Variables (EBV) development
- Fitness-for-use assessments
- Gap analysis for biodiversity monitoring
- Evidence-based policy support

### For Biodiversity Informatics
Methodologically, we demonstrate that:
- Flagging > filtering for flexibility and transparency
- Integrated quality-bias assessment > separate treatments
- Comprehensive documentation > minimal reporting
- Reproducibility and open science are achievable at scale

## Future Research Directions

This study opens several avenues for future work:

### Immediate Extensions
1. **Taxonomic-specific assessments**: Deep dives into birds, plants, insects to identify group-specific issues
2. **Protected area analysis**: Compare sampling inside vs. outside reserves
3. **Seasonal patterns**: Detailed phenological bias assessment
4. **Temporal quality trends**: Has data quality improved over time?

### Methodological Developments
1. **Automated bias correction**: Develop algorithms to automatically correct for identified biases
2. **Real-time quality assessment**: Tools to assess data quality during collection
3. **Machine learning approaches**: Use ML to predict quality issues and sampling bias
4. **Uncertainty quantification**: Propagate quality/bias uncertainty through analyses

### Integration and Synthesis
1. **Multi-source data fusion**: Combine GBIF with iNaturalist, eBird, national surveys
2. **Expert range maps**: Compare occurrence data with expert knowledge
3. **Remote sensing integration**: Use satellite data to predict unsampled biodiversity
4. **Citizen science potential**: Identify where citizen science could fill gaps

### Applied Conservation
1. **Targeted surveys**: Field campaigns in identified priority areas
2. **Bias-corrected conservation planning**: Develop Kenya-specific reserve selection accounting for bias
3. **Climate vulnerability assessment**: Model climate change impacts accounting for data limitations
4. **Monitoring network design**: Optimal placement of monitoring sites to fill gaps

## Final Perspectives

Biodiversity data are imperfect, biased, and incomplete‚Äîbut they are also invaluable. The GBIF database represents centuries of scientific effort, millions of specimen records, and the dedication of countless collectors, curators, and data managers. Rather than dismissing these data due to their limitations, we must:

1. **Understand the limitations** through comprehensive assessments like this one
2. **Document them transparently** so users can make informed decisions
3. **Correct for biases** where possible using appropriate statistical methods
4. **Fill the gaps** through strategic, targeted sampling efforts
5. **Improve data quality** through better protocols, training, and technology

For Kenya specifically, this assessment provides a baseline and roadmap. The baseline: **`r format(quality_assessment$clean_counts$moderate_clean, big.mark=",")`** quality-filtered occurrence records representing **`r format(summary_stats$n_species, big.mark=",")`** species‚Äîa substantial achievement, but geographically, environmentally, and taxonomically limited. The roadmap: **specific priorities** for where to sample (northern Kenya, arid lands), what to sample (invertebrates, fungi), and how to sample (systematic surveys, molecular methods).

As GBIF continues to grow‚Äînow exceeding 2 billion records globally‚Äîregular quality and bias assessments become increasingly important. Data volume does not guarantee data quality or representativeness. We advocate for:

- **Standardized quality reporting**: All GBIF-based studies should report quality issues encountered
- **Bias-aware analyses**: Statistical methods that account for sampling bias should become standard
- **Strategic data collection**: Move from opportunistic sampling to targeted gap-filling
- **Capacity building**: Invest in taxonomic expertise, especially in biodiversity-rich developing countries
- **Open science**: Share not just data but also quality assessments, bias layers, and analytical code

This study demonstrates that comprehensive, reproducible biodiversity data assessment is achievable and valuable. By understanding what we know, what's missing, and why it matters, we can make better use of available data while strategically filling critical gaps. The future of biodiversity science depends on both better data and better understanding of data limitations‚Äîthis work contributes to both.

# Data Availability

All data and code are openly available:

- **GBIF Data:** `r metadata$gbif_doi`
- **Analysis Code:** GitHub repository: [github.com/username/gbif-kenya-bias](https://github.com/username/gbif-kenya-bias)
- **Processed Data:** Zenodo DOI: [to be assigned upon publication]

# Acknowledgments

We thank the Global Biodiversity Information Facility (GBIF) and all data contributors for making biodiversity data openly accessible. We acknowledge the developers of the occAssess R package and other open-source tools used in this analysis. We are grateful to [funding agencies] for financial support.

# References

<div id="refs"></div>

# Session Information

```{r session-info}
sessionInfo()
```

# Appendix: Additional Figures

```{r fig-appendix-species-richness, fig.cap="Appendix Figure A1: Species richness across Kenya showing number of unique species per grid cell."}
knitr::include_graphics(here("figures", "02_species_richness_map.png"))
```

```{r fig-appendix-temporal-class, fig.cap="Appendix Figure A2: Temporal trends by taxonomic class showing changes in sampling effort over time for major groups."}
knitr::include_graphics(here("figures", "08_temporal_by_class.png"))
```

```{r fig-appendix-rarity, fig.cap="Appendix Figure A3: Distribution of species by rarity class showing high proportion of rare and singleton species."}
knitr::include_graphics(here("figures", "13_rarity_distribution.png"))
```
